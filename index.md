---
title: Front Page
---

<head>
    <link rel="stylesheet" href="index.css">
</head>

<div class="topnav">
  <a class="active" href="#home">Home</a>
  <a href="#contact">Contact</a>
</div>

<p align="center"><h1><b>Michael Lan</b></h1></p>

<!---
--->
<span><b>H</b></span>ello. My projects can be found below. Click on the title link to learn more:

---
<p style="font-size:20px"><b>
<a href="https://github.com/wlg1/analogous_neuron_circuit_expms">Analogous Neural Circuit Experiments</a>
</b></p>

Analogous Neural Circuit Experiments aim to find analogous patterns in neural network activations based on inputs with varying similarities. Why study Analogous Neural Circuit Experiments?
<ul>
<li><b>Improve Debugging:</b> By uncovering the black box that obscures how neural networks make decisions, practitioners will have more control over debugging neural networks to do what they want. </li>
<li><b>Improve Transfer Learning:</b> By understanding which parts of a neural network perform what function, practitioners will be able to dissect neural networks to retrieve the part they need, and apply transfer learning to fine tune that part to suit their own, specific goals.</li>
</ul>

These experiments are housed in a Github repo with Colab notebooks that walk a reader through how the experiments were conducted. An example of a notebook demonstrating simple experiments is given below:

<a href="https://colab.research.google.com/drive/12hQolN9TLXsakkG96nYUgU30_6YL74bf#scrollTo=IAJjuRTDBnOr">Tutorial 0: Compare Neuron Activations</a>

[image of face and face w/o mouth, showing toy circuits of each and what differs]

[in repo- state underlying assumptions and hypotheses to test]

---
<p style="font-size:20px"><b>
<a href="eduBlogHome.html">Making the Math of Neural Networks Intuitive</a>
</b></p>

Techniques such as eigendecomposition, orthogonal projection, and more are often applied to neural networks to further new improvements. Knowing why they work requires understanding not just linear algebra, but the intuition of <b>why</b> linear algebra is used. Why use an eigenvector to change the style of a generated image?

Thus, when studying new improvements to neural networks, many people run into the following problem:

<p align="center">
<b>How do these unfamiliar mathematical concepts relate to neural networks?</b></p>

This blog solves this problem by explaining many required prerequisites from scratch, and relating them to neural network applications.

![2mod_vecs](/cob/2mod_out.PNG)

---
<p style="font-size:20px"><b>
<a href="">Latent Space Experiments</a>
</b></p>

Another way to study how neural networks make decisions is to study the geometry of their latent space. This leads to powerful methods such as Style Editing, in which one can turn the face of a generated person from old to young.

---
<p style="font-size:20px"><b>
<a href="https://mikelan300.wixsite.com/portfolio">Data Science Projects</a>
</b></p>

Regression Analysis on Video Game Sales using Unstructured Data from Reddit 
<ul>
<li>Employed web APIs to scrap Reddit posts and semi-structured Wikipedia infoboxes. Cleaned and
explored data, then ﬁt models applicable for business decisions in marketing and ﬁnance. </li>
</ul>
Facebook and YouTube Social Network Friend Prediction via SVMs 
<ul>
<li>Trained Support Vector Machines that achieved high accuracy and AUC scores on testing data.
Utilized graph analytics to compare the network statistics of diﬀerent communities.</li>
</ul>

<div id="datasci_images">
  <div class="inline-block">
  <img src="/datasci/final_model.PNG" width="350" height="300" alt="">
  <img src="/datasci/datasci_networks.png" width="350" height="300" alt="">
  </div>
</div>

<!---
![final_model](/datasci/final_model.PNG)
![datasci_networks](/datasci/datasci_networks.png)
--->

---




