# Long sequences in RNNs lead to vanishing gradients

PROB:

Long sequences in RNNs lead to vanishing gradients (long term dependencies).
Its memory is not that strong when it comes to remembering the old connection.

---

SOLN:

LSTM- have a branch that allows to pass information and to skip the long processing of the current cell, this allows the memory to be retained for a longer period of time

REF:

[https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f](https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f)

\