# Regularization

[https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms](https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms)

Norm: Map vector elements to a scalar (usually represents a size or magnitude)

L1: Manhattan/Taxicab 

L2: Euclidean

---

[https://towardsdatascience.com/visualizing-regularization-and-the-l1-and-l2-norms-d962aa769932](https://towardsdatascience.com/visualizing-regularization-and-the-l1-and-l2-norms-d962aa769932)

---

[https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models](https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models)

L1 regularization on the weights tends to induce sparsity 

![Untitled](Regularization%2012c65aa5e118436fb60a4c8e0b0fde6e/Untitled.png)

![Untitled](Regularization%2012c65aa5e118436fb60a4c8e0b0fde6e/Untitled%201.png)

Regularize: Change x1 from 1 to ${1- \delta}$

After changing it,