# GPT mechanisms

[https://huggingface.co/course/chapter1/6?fw=pt](https://huggingface.co/course/chapter1/6?fw=pt)

Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. (encoders can access both directions). These models are often called *auto-regressive models*.

The pretraining of decoder models usually revolves around predicting the next word in the sentence. GPT is one.

[https://e2eml.school/transformers.html#decoder](https://e2eml.school/transformers.html#decoder)

It is a transformer with the encoder stack and all its connections surgically removed. What remains is a 12 layer decoder stack.

![Untitled](GPT%20mechanisms%20108c635e0f0849ac8e556c91c892dec2/Untitled.png)

---

- Why are GPT outputs random if it always chooses the token with the highest probability? Wouldn't it always just choose the same word with the highest logit?
    
    [https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612](https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612)
    
    *Sampling* means randomly picking the next word according to the conditional word probability distribution extracted from the language model. As a consequence, with this decoding method text generation is not deterministic.
    
- is this true? GPT, presumably by top-k sampling, chooses a random next word instead of just the token with the highest probability.
    
    Yes, it is true that GPT (Generative Pre-trained Transformer) models, including GPT-3, can use top-k sampling to generate text. Top-k sampling is a technique used in language generation where the model selects the top-k most probable next words from the probability distribution generated by the model, and then samples from those k words according to their probabilities. This means that the model does not always choose the token with the highest probability, but instead chooses from a set of the most probable words. The value of k is typically chosen to balance between diversity and coherence in the generated text.
    
    Top-k sampling is one of several strategies that can be used to generate text with a GPT model, and there are other techniques such as top-p sampling (also known as nucleus sampling) and beam search that can be used instead. The choice of sampling strategy can have a significant impact on the quality and diversity of the generated text.
    
- is this true? The calculation of these logits is deterministic.
    
    Yes, it is true that the calculation of logits in a neural network, including GPT models, is deterministic.
    
    The logits are the output of the last layer of the neural network before applying the softmax function to generate the final probability distribution over the output vocabulary. The logits are computed by taking a weighted sum of the activations of the neurons in the last layer, where the weights are the parameters of the neural network that are learned during training.
    
    Since the weights of the neural network are fixed during inference, the calculation of the logits for a given input sequence will always be the same. This means that if you provide the same input sequence to a trained GPT model, you will always get the same logits as output.
    
    However, the generation of text from the logits is not deterministic, as the sampling process (such as top-k or top-p sampling) introduces a degree of randomness in selecting the next word. This means that even if the same input sequence is given to the model and the same logits are generated, the output text can still be different depending on the sampling strategy used.
    

---

- How can GPT have cross attention if it's decoder only?
    
    In a typical encoder-decoder model, the encoder processes the input sequence, and the decoder generates the output sequence. Cross attention is used in the decoder to attend to the encoder's output, allowing the decoder to consider information from the input sequence when generating the output.
    
    In GPT, the model is designed for language modeling and text generation tasks. The input and output sequences are essentially the same text, shifted by one position, so there's no need for an encoder in the traditional sense. GPT still employs self-attention layers, which are a part of the decoder architecture, and these layers allow the model to attend to its own past tokens.
    
    Consequently, GPT doesn't have "cross attention" as in the attention between an encoder and a decoder. Instead, it uses self-attention mechanisms to capture relationships between tokens in the input sequence.
    

---

https://www.reddit.com/r/ChatGPT/comments/13ucoev/if_chatgpt_cant_access_the_internet_then_how_is/jm090r5?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=2&utm_content=share_button