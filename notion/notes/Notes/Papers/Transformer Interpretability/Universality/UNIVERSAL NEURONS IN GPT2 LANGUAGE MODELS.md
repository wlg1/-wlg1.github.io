# UNIVERSAL NEURONS IN GPT2 LANGUAGE MODELS

(p4) Excess correlation: the difference between the highest correlated vs highest baseline. This is how much the maximums between the two differ

(p5)

> Universal neurons often exhibit large weight norms (signifying importance due to weight decay training) and substantial negative input biases, leading to predominantly negative pre-activations and infrequent activations, alongside pronounced pre-activation skew and kurtosis, indicating typical negative activations with sporadic positive spikes, characteristics anticipated in monosemantic neurons.
> 

> non-universal neurons usually have skew approximately 0 and kurtosis approximately 3, identical to a Gaussian distribution
> 

Section 6: Future Work you can try

> these functional neurons often form antipodal pairs, potentially enabling collections of neurons to ensemble to improve robustness and calibration
> 

This means we can use a concept as not just on or off, but -1 or 1, allowing for more logical computations