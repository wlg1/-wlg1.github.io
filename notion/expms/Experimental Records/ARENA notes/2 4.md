# 2.4

[https://www.youtube.com/watch?v=zcxRUyRw2dk&ab_channel=ARENA3.0](https://www.youtube.com/watch?v=zcxRUyRw2dk&ab_channel=ARENA3.0)

KL div makes prob distr match. But RL does “mode

If RLHF goal is to use more periods, aim is for behavior to use shorter sentences. But don’t want it to go overboard and only use periods and nothing else. So fine tune

- KL div is high is the samples from P are unlikely to be found (would surprise- high entropy) in Q
    
    Yes. However, KL divergence is not a direct measure of the entropy of either distribution. It's more about the relative difference in the expected information content between the two distributions. While a distribution with high entropy has more unpredictability, KL divergence is specifically about the difference in predictability between two distributions, not the absolute unpredictability of one. Entropy is a measure of the unpredictability or randomness of a distribution. 
    
    A high KL divergence in this context would indicate that the new policy (P) is deviating significantly from the old policy (Q), potentially leading to unstable learning. Therefore, PPO uses techniques to limit this divergence, ensuring more stable updates.
    

Updating (finetuning) model to new: We don’t want data obtained from PPO (new model) to be too surprising (too different) from that of what would be obtained from old model

Eg) if new model has many output samples of 20 periods in a row, that’s way too different from old model, which doesn’t expect that to happen on average

We are okay with the old model often having output samples which we don’t expect to get under the distribution of the new model. So if a sample from the old model (eg. hate speech) has low probability in the new model’s distribution, that’s okay, we don’t care about penalizing it.

So would penalizing “how hate speech has low probability” would give hate speech higher probability?

RLHF concentrates probability mass on a small number of outputs (fine tune towards them)

- Lecture questions
    
    To rephrase the end of the lecture, "We are okay with the old model often having output samples which we don’t expect to get under the distribution of the new model. So if a sample from the old model (eg. hate speech) has low probability in the new model’s distribution, that’s okay, we don’t care about penalizing it."
    If the above rephrasing is sound, what about the inferred statement: "So would penalizing “how hate speech has low probability” would give hate speech higher probability?"
    
    A: actually did train the model with those terms the wrong way around at first (i.e. by mistake), and it still kinda went fine
    
    But yeah in theory, doing KL the wrong way around would mean that **if the reference model has some behaviour X, then the new model is incentivised to not totally lose behaviour X, since this would lead to a very high KL div.** So this is kinda fighting against the "natural forces of RL" which want you to mode collapse onto a narrow set of behaviours.
    
- Q- model editing effect on distribution
    
    It was mentioned how fine tuning and RL using KL divergence change probability distributions. Is there a way to study/measure how model editing like steering would do this? (I'm not really sure how to phrase this more rigorously, it's more of a general question on intuition). Intuitively I think the change depends on what components are altered, but a specific later layer would make a very small change?
    

---

[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)

Base model doesn’t change, only fine tuend one does. Base model is used to make sure doesn’t get too far away

![Untitled](2%204%20827534c7916642a39cca8df23031fa9f/Untitled.png)

Reward preference model: is this the critic? And/or is this the human?

- It’s not the critic (that’s value head in the tuned model). RPM models human feedback to generate a reward signal. This acts like the human, but is more “stable” than the human as it’s trained on average human responses to sift the noise. So it still has human preferences and biases but is more reliable than a single human as it aggregates multiple humans.
    
    Actually i read the article on hf again and it seems like it is acc. As it says : "The differing values of humans cause these scores to be uncalibrated and noisy.”
    
    The AI replaces the human on a very human job to make it more efficient:
    
    1. It'd be super time-expensive to use human evaluators in the middle of the PPO process. On the other hand if you already have an RPM then you can run the RLHF process very fast, without human feedback being a bottleneck.
    2. Your RPM can output a float value (since it's already learned from a very large number of human ratings), so it provides a more sensitive reward signal. Human evaluators only give a "yes" or "no", which is a way lower-information signal.

rpm is supervised learning and not RL

- chatgpt answer: Does the reward pref model change during rlhf training? If so how?
    
    Yes, the reward preference model can change during Reinforcement Learning from Human Feedback (RLHF) training. The extent and nature of these changes depend on the specific RLHF approach and how the training process is structured. Here’s an overview of how and why the reward preference model might change during training:
    
    1. **Iterative Feedback and Model Updating**:
        - **Initial Training**: The reward preference model is initially trained on a dataset of human preferences. These preferences could be expressed in various forms, such as pairwise comparisons of actions or trajectories, ratings, or direct annotations.
        - **Iterative Updates**: As the reinforcement learning agent interacts with the environment and generates new data (new actions, decisions, or trajectories), these can be presented back to humans for additional feedback. The reward preference model is then updated or retrained with this new feedback, refining its understanding of what constitutes desirable behavior.
    2. **Refining Preferences Over Time**:
        - As the RL agent explores and learns, it might encounter new situations or produce new behaviors that were not covered in the initial training data of the reward preference model.
        - Continuous or periodic updating of the reward preference model allows it to adapt to these new situations and provides a more accurate and encompassing understanding of human preferences.
    3. **Adapting to Complex or Evolving Tasks**:
        - In complex environments or tasks with evolving requirements, the human understanding of what constitutes optimal behavior might change over time. Consequently, the reward preference model needs to be updated to reflect these changing preferences or understandings.
    4. **Balancing Exploration and Exploitation**:
        - In some RLHF setups, the reward preference model's updates might also be influenced by the need to balance exploration (trying new or less certain actions) and exploitation (using known, high-reward actions). Updating the model can help in guiding the agent towards effective exploration while still aligning with human preferences.
    5. **Feedback Efficiency and Model Stability**:
        - The frequency and extent of updates to the reward preference model are often carefully managed to maintain a balance between feedback efficiency (not requiring excessive human input) and model stability (avoiding too frequent or drastic changes that could destabilize the learning process).
    
    In summary, the reward preference model in RLHF is not static; it is typically updated throughout the training process to better capture and adapt to human preferences, especially as new behaviors are explored and new situations are encountered by the RL agent. This dynamic updating is crucial for ensuring that the agent's learning remains aligned with human values and goals, particularly in complex and evolving environments.
    
- Callum answer: Does the reward pref model change during rlhf training? If so how? Would there be any instabilities if it changes during training since the rewards would differ during training, or somehow it won't be unstable when it's updated with the fine tuned model?
    
    Actually also found an answer in the hf article: "Optionally, RLHF can continue from this point by iteratively updating the reward model and the policy together.”… Anthropic discusses this option as *Iterated Online RLHF* (see the original **[paper](https://arxiv.org/abs/2204.05862)**), where iterations of the policy are included in the ELO ranking system across models. This introduces complex dynamics of the policy and reward model evolving, which represents a complex and open research question.
    

- is observation space same as state space?
    
    In reinforcement learning, the terms "observation space" and "state space" are closely related but can have slightly different meanings depending on the context and the specific environment or problem being addressed. Let's clarify each term:
    
    1. **State Space**:
        - The state space represents all possible configurations or statuses of the environment.
        - In a fully observable environment, the state is a complete description of the environment at a given time.
        - The state space is a theoretical construct that includes every possible state the environment can be in.
    2. **Observation Space**:
        - The observation space refers to the information that the agent actually perceives or observes about the environment at each step.
        - In fully observable environments, the observation space is equivalent to the state space, as the agent observes the entire state of the environment.
        - In partially observable environments, the observation space might be a subset of the state space. The agent doesn't have access to the full state but rather some aspects of it. This is common in many real-world scenarios where an agent can't see everything or where some state information is hidden.
    
    In summary, while in fully observable environments the observation and state spaces are the same, in partially observable environments, the observation space is usually a subset of the state space. The distinction is important in the design and implementation of reinforcement learning algorithms, as it affects how the agent perceives and interacts with its environment.
    

per-token probability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them

- what are per-token probability distributions?
    
    These are just like the d_vocab logit outputs for each token you see in experiments
    
    They refer to the probabilities assigned to each possible token (like a word or character) in a given position in a sequence, based on the context provided by the preceding tokens. 
    
    1. **Probability Distributions**: A probability distribution, in this context, assigns a likelihood to each possible token that could appear next in a sequence. This likelihood is based on the context of the sequence - the tokens that have already appeared.
    2. **Per-Token Basis**: The term "per-token" means that these probability distributions are calculated for each position (token) in a sequence. The model predicts the next token in a sequence by sampling from this distribution, then updates the distributions [for all tokens] based on the new token, and repeats this process to generate text sequentially.

![Untitled](2%204%20827534c7916642a39cca8df23031fa9f/Untitled%201.png)

This is the RPM output (is MORE likely to be human aligned) but the second term (a penalty) ensures the new model doesn’t diverge THAT far from the old model (eg. generate nonsense)

[http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)

---

2.4 Exercises

GPT-2 small only has 117M and 2.4 uses 80M but the memory related to the GPU depends on the input token sequence sizes. So A100 is still used even for GPT-2 small. Only toy models with 2 layers would not need as big if it has seq token length ~>20.

2.4.1: base_model: str = "gpt2-medium”

Are they inserting the value head using hook fn, or just using hook fn to get actvs? 

> We can attach a hook function to the residual stream at the final layer, and have it apply our value head to the residual stream values & store the output externally. Then we can use `model.run_with_hooks` to get our logits like normal, and fetch our value estimate from the external storage object.
> 

Also is using hook fn to get residual stream output, then use that output to train value head.

[https://stackoverflow.com/questions/61598771/pytorch-squeeze-and-unsqueeze](https://stackoverflow.com/questions/61598771/pytorch-squeeze-and-unsqueeze)

- if logits is # minibatch_size, seq_len, d_vocab dim, what dim to take log_softmax on? what does it mean to take it on a dim?
    
    In most cases for such data, you'll want to apply `log_softmax` to the vocabulary dimension. This is because you typically want to convert the raw logits to log probabilities across the vocabulary for each element in the sequence of each batch. Thus, you would use `dim=2`.
    
    Here's what it means to apply `log_softmax` on a particular dimension:
    
    - When you apply `log_softmax` along a certain dimension, you are computing the softmax function across the elements of that dimension, independently for each slice along that dimension.
    - For example, if you apply `log_softmax` on `dim=2` in your case, the softmax function is computed across the vocabulary for each sequence element in each batch. This is like asking, for each position in each sequence of the batch, what is the probability distribution over the vocabulary?
    
    In code, it would look like this:
    
    ```python
    log_probabilities = torch.log_softmax(logits, dim=2)
    
    ```
    
    This operation will return a tensor of the same shape as `logits`, where each `[minibatch, sequence_element]` slice is a log probability distribution over the vocabulary.
    
- What happens if we use dim=1? Does log_softmax calculation depend on the aggregate across a dim?
    1. **Effect of Applying `log_softmax` on `dim=1`**: For each vocabulary term in each example in your minibatch, the softmax will aggregate over the entire sequence length. This is somewhat unusual for typical sequence processing tasks (like language modeling or sequence classification) because it implies that you're treating the sequence positions as interchangeable or competing with each other, which is rarely the case.
    2. **Dependency on Aggregate Across a Dimension**: Yes, the `log_softmax` calculation depends on the aggregate across the specified dimension. The softmax function exponentiates each element (making all values non-negative) and then normalizes them so that they sum up to 1 along the specified dimension. When applying `log_softmax`, it's effectively computing the logarithm of these softmax probabilities. Therefore, changing the dimension over which it is applied changes how the elements interact with each other during this normalization process.
        - For `dim=2`, each element in the sequence (for each item in the minibatch) is treated as a separate distribution over the vocabulary.
        - For `dim=1`, each vocabulary term's scores across all sequence positions are treated as a distribution.
    
    In summary, using `dim=1` would make each position in the sequence compete against every other position for each term in the vocabulary, which is a different interpretation compared to the standard use case (like in language models) where you usually compute a probability distribution over the vocabulary for each position in the sequence independently (`dim=2`).
    

<<<

**Questions**

- What's the advantage of recursively using reward_fn_char_count for each str in the list over just using generated_sample.count(char) again in the list comprehension?
    
    A: none
    
- 

---

Memory

GPU issue (prevent slow down) for CUDA processing: plug GPU into motherboard input instead of the discrete GPU

- after del do you always need gc.collect()
    
    No, you do not always need to call `gc.collect()` after using `del` in Python. The `del` statement in Python is used to delete objects, which means it simply removes the reference to the object and marks it as available for garbage collection. In most cases, Python's garbage collector will automatically handle the cleanup of unreferenced objects in a timely and efficient manner.
    
    However, there are some situations where you might want to use `gc.collect()`:
    
    1. **Handling Large Objects**: If you're deleting large objects or a large number of objects and want to ensure that the memory is freed up immediately, calling `gc.collect()` can be helpful. This is particularly relevant in memory-constrained environments or when working with large datasets.
    2. **Circular References**: Python's garbage collector can automatically detect and clean up circular references (where two objects refer to each other, preventing them from being garbage collected). However, there might be complex scenarios where manually invoking `gc.collect()` helps in ensuring that such objects are cleaned up.
    3. **Explicit Resource Management**: In cases where you have tight control over resource management or need to ensure deterministic cleanup of resources, manually invoking the garbage collector can be a part of your resource management strategy.
    
    In typical usage, especially in environments like Google Colab, it's not common to need to manually invoke the garbage collector. Python's automatic memory management is usually sufficient for most applications. The `gc.collect()` call is more of a tool for specific situations where you have particular memory management requirements.
    

---

RLHF still don’t know what it’s doing because still don’t know if it’s aligned (this is inner alignment). So this is why you need mech interp. RLHF is outer alignment. Constritution is just guidelines, so seems to also be outer alignment so instead of human feedback it’s a preset constitution. 

Its base axioms reducing suffering without eradicating all life (but even this can be misunderstood)

[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)

[In the copy]. The exact dynamics of how many parameters to freeze, or not, is considered an open research problem

related to allow RLHF to scale: “The relative maturity of this method [PPO] made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).”

LoRA FT vs MEMIT

Model editing on certain components or features is easier to prove to reviewers than circuit abaltion (since reviewers may be skeptical on “is that circuit truly associated with that behavior”? or “what if it’s something else?” even if behavior is destroyed by ablation whereas editing shows behavior that is obvious to reviewers as long as the behavior experiments cover all impt and edge cases)

reach out to anthropic ppl if you have conn and act as co-author