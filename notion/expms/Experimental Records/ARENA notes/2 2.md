# 2.2

Monte Carlo: episodic

Temportal Diff keeps running avg. Update diff in estimate. Estimates reward. use this to update before getting actual reward. Helpful b/c not everything is episodic

TD builds a model of actual data generating process. No one proves why better than MC, just better in practice. 

MC and TD converge to optimal, but opt to diff criteria. MC min eror on train data (not great all the time). TD generalzes better and runs faster b/c updates at end of every time step rather than at end.

Q learning combines MC and TD

---

SARSA (on poilcy): in algo, not every combo, just until S is terminal (each episode is from starting to terminal)

The update rule is Q += error (weighted). The error is the difference (TD) between Q* (est future rewards after taking a_t+1 in state s_t+1) vs the current Q(S, A) value.

Since the error may be off, we only update it SLIGHTLY in the error. This is done using a small learning rate weight.

Q-learning: Off-policy uses argmax_a of Q(s_{t+1}, a) because this is what policy SHOULD HAVE taken, rather than p(s)=a which is what the policy actually took. This argmax does not follow policy.

In SARSA algo, the update uses Q(S, A) which A comes from policy. But in Q-learning, the update to Q(S,A)’s error term that estimate future rewards disregards A in the error except for the old value. That’s the only difference: it’s off-policy for estimated future rewards.

Q learning seems to do better with a slightly higher optimism value.

Q learning seems to do better with a slightly higher optimism value. Increase epsilon to give more exploration and faster learning rate makes Q learning as good as SARSA.

In Q learning, you need to find every action from that state; this tabulation is costly. SARSA is also tabular since Q is a table.

The methods used here are called tabular methods, because they create a lookup table from `(state, action)` to the Q value.

Deep Q-learning can generalize learning across similar states, reducing the need to learn the value of every possible state-action pair explicitly.

Questions:

- This argmax does not follow policy. it tries all the Q then takes the max. Whereas in off-policy, the Q depends on policy?
- Is Q(S, A) the actual value or the old value?
    
    In the error term of the SARSA update rule, \( Q(s_t, a_t) \) represents the old value (or the current estimate before the update) of the action-value function for the state-action pair \((s_t, a_t)\).
    
    The error term, which is $r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$, is the difference between the estimated value of future rewards (after taking action \( a_t \) in state \( s_t \) and ending up in state \( s_{t+1} \) taking action \( a_{t+1} \)) and the old value of \( Q(s_t, a_t) \). This difference is used to adjust the value of \( Q(s_t, a_t) \) to better approximate the true action-value function based on the new experience.
    
- 5.2.2 : Use argmax or max? Argmax returns the index, whereas max is the value. We take the max Q value, not max action, because we're using Q in the eqn, not a ?
- When is Q learning worse than SARSA?
    
    SARSA is an on-policy algorithm, meaning it learns about the policy it is currently following, including the exploration steps. Q-learning, on the other hand, is an off-policy algorithm that learns about the optimal policy, assuming perfect knowledge of the environment (greedy policy), even while it is exploring. If exploration is not managed carefully, Q-learning can overestimate Q-values, leading to suboptimal policies.
    

---

- what's diff between np.stack, np.append, np.concatenate
    
    `numpy.stack`, `numpy.append`, and `numpy.concatenate` are all functions in NumPy used to combine arrays, but they do so in different ways and are used for different purposes:
    
    1. **`numpy.stack`**:
        - Purpose: `np.stack` is used to join a sequence of arrays along a new axis.
        - Usage: It stacks the arrays in sequence depth-wise along a new axis. All arrays must have the same shape.
        - Example: If you have two arrays of shape `(3, 4)` and stack them, you can get an array of shape `(2, 3, 4)` if you stack along a new first axis.
        - Code: `np.stack([array1, array2], axis=0)`
    2. **`numpy.append`**:
        - Purpose: `np.append` is used to add values to the end of an array.
        - Usage: It flattens the input array unless an axis is specified. It then adds the values at the end of the array. The input arrays do not need to have the same dimensions.
        - Example: If you append an array of shape `(3,)` to another array of shape `(3,)`, you will get an array of shape `(6,)`.
        - Code: `np.append(array1, array2)`
    3. **`numpy.concatenate`**:
        - Purpose: `np.concatenate` is used to join a sequence of arrays along an existing axis.
        - Usage: It concatenates the input arrays along the specified axis. All arrays must have the same shape except in the dimension corresponding to `axis`.
        - Example: If you have two arrays of shape `(3, 4)` and concatenate them along axis 0, you will get an array of shape `(6, 4)`.
        - Code: `np.concatenate([array1, array2], axis=0)`
    
    Here is a summary of the differences:
    
    - `stack` adds a new axis and stacks the arrays along it. The arrays must have the same shape.
    - `append` adds values to the end of an array, and it can flatten arrays if no axis is specified.
    - `concatenate` joins arrays along an existing axis, and the arrays must have the same shape except for the dimension along the specified axis.
    
    Each of these functions is used based on the specific requirements of array manipulation in your context.
    
- can you rewrite this with append instead of concatenate: