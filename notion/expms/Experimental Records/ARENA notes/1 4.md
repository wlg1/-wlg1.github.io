# 1.4

[https://colab.research.google.com/drive/1hoD36nsHp6K0E-YeFgPzxazlstb7HPyh#scrollTo=utS2IIVc5DSy](https://colab.research.google.com/drive/1hoD36nsHp6K0E-YeFgPzxazlstb7HPyh#scrollTo=utS2IIVc5DSy)

1.4, Section 1: This is a toy model with a bottleneck. To make the bottleneck, there’s a W matrix in to the low dim (bottleneck), and a W matrix out back into input space. WW is “correlation” between input vectors, or their cosine sim (via dot product).

1.4, Section 6: 

![Untitled](1%204%200186fae1fa2d41e49f64642d2ea523eb/Untitled.png)

Get rid of the bias (here, b_dec should approximate the bias of the model, so use that) and only use the weights?

SAE inherits W from Model (toy), which has model.W. This W is updated during model.forward() as losses are used in optimizing W. Then, `plot_features_in_2d`from utils is used on model.W

> we found that over the course of training some neurons cease to activate, even across a large number of datapoints. We found that “resampling” these dead neurons during training gave better results by allowing the model to represent more features for a given autoencoder hidden layer dimension.
> 

---

**Section 7, SAE on LLMs:**

How to run autoencoder 

1. Cache model activations
2. Put them through autoencoder to get post_reconstruct and loss

Exercise - find the sparsity

NOTE: autoencoder.cfg.n_hidden_ae is the number of features, and each feature has a probability of firing for every (batch, pos) combo (take the mean over every (batch, pos) of model dataset).

However, the plot is not showing features on the x-axis. It's showing the frequency of probability of firing (after log). This is confusing because it's a frequency of frequencies.

Eg) If 2 features have logprob = 0.4 of firing, and 3 features have logprob = 0.6 of firing, the logprob of 0.4 will be 2 / 5 as a percentage.

NOTE: This is done not to interpret features, but to see that most features barely fire, while there are two groups (b/c bimodal distr) which likely "fire together" given they have the same %.  

- cosine sim of features plot
    
    ![Untitled](1%204%200186fae1fa2d41e49f64642d2ea523eb/Untitled%201.png)
    

There are TWO plots. The highest percentage is around 55%. The plot above it is something else

[https://docs.google.com/document/u/0/d/187jfZSbhRjjQaazjYlThBsKp3Q0Pw3VdIHVST9H2dvw/mobilebasic](https://docs.google.com/document/u/0/d/187jfZSbhRjjQaazjYlThBsKp3Q0Pw3VdIHVST9H2dvw/mobilebasic)

Exercise - find the reconstruction loss

- why is L2 norm calculatd this way? what is it?
    
    The L2 norm, also known as the Euclidean norm or the \(L^2\) distance, is a measure of the magnitude (or length) of vectors. It is calculated as the square root of the sum of the squares of the vector components. In the context of machine learning and specifically in your function `get_reconstruction_loss`, the L2 norm is used to quantify the difference or "loss" between the original neuron activations (`post`) and their reconstructions by the autoencoder (`post_reconstructed`). This measure helps in evaluating how well the autoencoder is able to reconstruct the original input after compression and decompression steps.
    
    In your function, however, instead of directly calculating the Euclidean distance, you calculate the squared differences between the original activations and their reconstructions (which is part of computing the L2 norm) and then average those squared differences. This approach is often used in optimization problems because it simplifies the gradient computation, and the square function amplifies larger errors, making the model more sensitive to them, which is often desirable during training.
    
    To break down the relevant part of your function:
    
    1. **`post_reshaped`** is reshaped to `[batch_size * seq_len, 2, model.cfg.d_mlp]` to prepare the data for processing by the autoencoder. Each `[batch seq]` from the original tensor is expanded into two instances, effectively doubling the dimension to represent two separate instances for each position in the sequence for each batch item. This is necessary for processing through the autoencoder that expects this shape.
    2. **`l2_loss`** is computed as the difference between the autoencoder's reconstructions and the original activations. Specifically, it's calculated inside the autoencoder (`autoencoder.forward(post_reshaped)`), where you get the squared differences (as part of the L2 loss calculation). This squared difference is what you're averaging when you calculate `l2_loss.mean(0).tolist()`. This operation calculates the mean squared error (MSE) for each instance, which is a common way to compute the L2 loss in machine learning tasks. The MSE is preferred because it emphasizes larger errors over smaller ones, which can be beneficial for learning.
    3. The reason for calculating the L2 norm this way (via squared differences) is both for computational efficiency and because the square of the L2 norm (or the squared Euclidean distance) is a smooth, convex function. This convexity is desirable because it guarantees that gradient descent methods will converge to a global minimum (assuming the entire loss landscape is convex).
    4. The outputs you mentioned (`"Avg L2 norm of acts:"` and `"Avg cos sim of neuron reconstructions:"`) serve two purposes:
        - The first prints the average L2 norm of the activations before reconstruction. This gives an idea of the magnitude of the original activations.
        - The second prints the average cosine similarity between the original activations and their reconstructions. This provides a different perspective on the reconstruction quality, focusing on the directionality of the vectors rather than their magnitude.
    
    The combination of L2 loss for magnitude and cosine similarity for directional accuracy offers a comprehensive view of the autoencoder's performance in terms of both how well it captures the magnitude of the original data and how accurately it preserves the directionality of the original feature vectors.
    

The line **`einops.reduce(post_reshaped.pow(2), "batch inst d_mlp -> inst", "mean")`** computes the average squared magnitude of the activations, which is a step towards calculating an L2 norm, but it does not calculate the L2 loss related to the difference between original and reconstructed activations. That part is handled internally by the autoencoder's forward pass, which computes **`l2_loss`** based on the squared differences between the inputs and their reconstructions.

You should find that the reconstruction loss is around 0.016 (and about the same for encoder-B). You can compare this to the average squared  L2  norm of the activations to get an idea of how good this is - this value is around 0.11, so we're doing a pretty good job (albeit not perfect) of reconstructing the activations.

Exercise- Substitution Loss

Loss after ablating.

1. Make hook fn to replace model actvs with autoencoder outputs or 0
2. Run original model by `run_with_hooks(comp, partial(hook_fns) )`to get loss

**Interpreting individual features**

Exercise - find highest-activating tokens

- more efficient by calculating the activations explicitly, rather than by extracting `acts` from the 5-tuple returned by `autoencoder.forward`
- Get the top k rows of `acts` (this is z)
    - k and i refer to ranks of the (batch,seq) tokens that activate the highest for this feature_idx
        - Eg) batch 0 is “bob is big” and batch 1 is “rock”. top k=3 ranks are:
            1. (batch 0, seqpos 2): big
            2. (batch 1, seqpos 0): rock
            3. (batch 0, seqpos 0): bob

`highest_activating_tokens` takes in a single `feature_idx: int,` (eg. autoencoder neuron 7; each neuron is considered a feature) and returns the **Top-k tokens**

This has dim `[k, 2]`:

`t.stack([top_acts_batch, top_acts_seq], dim=-1)`

the i-th element of this tensor are the `(batch, seq)` indices of the i-th highest-activating token (i.e. the token on which the feature_idx-th neuron in the autoencoder has the largest activations)

i refers to token, whle `feature_idx-th`  refers to SAE neuron (modle feature)

- The highlighted orange token is the one with the `(batch, seq)`  indices of row ith highest
    
    ![Untitled](1%204%200186fae1fa2d41e49f64642d2ea523eb/Untitled%202.png)
    

 `autoencoder.W_enc[instance_idx, :, feature_idx]` : This tensor slice encodes the model activations `h_cent` , which contains activations for multiple token sequences (batches), and outputs a single vector for each sample. The indices of this SAE hidden layer vector are the tokens, so the highest value are the tokens which activate highest on this `feature_idx`

Finally, `display_top_sequences` takes the indices with the highest values and decodes them just by using the “look up table” of index to vocab word.

Questions

- Why use  `- autoencoder.b_dec[instance_idx]` on the model activations before embedding in SAE?

Exercise - find a feature's logit effect

when you multiply the SAE decoder weight with the original model's MLP output matrix and unembedding matrix, which tokens are most affected

This is just weights, no activations

Exercise - examine this feature in the neuron basis

The neuron basis is the ORIGINAL model’s neurons, so the output space is the neurons. This is the decoder matrix of SAE, since it goes from SAE features to orig model neurons.

Howevr, the code descr seems contradictory since it then talks about a feature having neurons (feature is output). But if it’s neuron space, that’s a neuron having features (neuron is output)

This code finds the neurons for a SINGLE feature (id 7)

W_dec has shape `n_instances n_hidden_ae n_input_ae`, so `W_dec[instance, feature_idx, :]` is a vector of shape `n_input_ae`, which is the number of orig model MLP neurons. It’s used by:

```jsx
 h_reconstructed = einops.einsum(
        acts, self.normalize_and_return_W_dec(),
        "batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae 
        -> batch_size n_instances n_input_ae"
    ) + self.b_dec
```

- How is the multiplication matrix for this done? I thought the rows of a matrix are the output and cols are input?
    
    `acts, self.normalize_and_return_W_dec(),
    "batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae
    -> batch_size n_instances n_input_ae"`
    
- how can weight matrix be shape n_instances n_hidden_ae n_input_ae ? Shouldn't n_input_ae be in the row (second col) since n_input_ae  is the output space?
    
    transpose of W?
    
- How does this tensor mulitplicaiton work? for matrices, it's simple to see that left side row dim = right side col dim. But it's not clear what are rows and cols here and what should be equal:
batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae
    
    In deep learning frameworks (like TensorFlow or PyTorch), operations such as **`torch.matmul`** or **`tf.matmul`** can handle these higher-dimensional multiplications by aligning the last dimension of the first tensor with the second-to-last dimension of the second tensor, effectively performing matrix multiplication across the batch and instance dimensions.
    
- why does torch.matmul or tf.matmul handle these higher-dimensional multiplications by aligning the last dimension of the first tensor with the second-to-last dimension of the second tensor? is this a legal math operation? explain in terms of math
    
    ### Matrix Multiplication Basics:
    
    In standard matrix multiplication, if you have two matrices \(A\) and \(B\) where \(A\) is of size \(m \times n\) and \(B\) is of size \(n \times p\), the result \(C = AB\) is a matrix of size \(m \times p\). Each element \(c_{ij}\) of \(C\) is calculated as the dot product of the \(i\)-th row of \(A\) and the \(j\)-th column of \(B\). This operation requires the inner dimensions to match (\(n\) in this case).
    
    ### Extension to Tensors:
    
    When we move to tensors (which you can think of as multi-dimensional matrices), the principle of "inner dimensions must match" remains, but we get an additional feature: the operation can be applied across multiple batches or sets of matrices simultaneously. This is especially useful in applications like deep learning, where working with batches of data is common.
    
    ### How `torch.matmul` and `tf.matmul` Work:
    
    These functions extend matrix multiplication to tensors with more than two dimensions by performing the operation on the trailing dimensions while preserving the outer dimensions. Specifically:
    
    - For two tensors \(A\) and \(B\), where the dimensions of \(A\) are $[..., m, n]$ and the dimensions of \(B\) are $[..., n, p]$, the functions treat these tensors as collections of matrices spread out over the additional dimensions.
    - The $m \times n$ matrices from \(A\) are multiplied by the $n \times p$ matrices from \(B\) in a standard matrix multiplication fashion, resulting in matrices of dimension \(m \times p\).
    - The outer dimensions (all dimensions in \(A\) and \(B\) except for the last two) dictate how these matrices are batched together. These dimensions must either match or be compatible according to broadcasting rules (in cases where one tensor has more dimensions than the other, or if they have dimensions of size 1 that can be expanded).
    
    ### Mathematical Legitimacy:
    
    This operation is mathematically valid and can be seen as a direct generalization of matrix multiplication. It allows for the efficient processing of batches of data or the application of the same transformation across a set of matrices simultaneously. This is particularly useful in deep learning for operations like batch processing or applying the same neural network weights across multiple data points simultaneously.
    
    In essence, the operation performed by `torch.matmul` or `tf.matmul` on higher-dimensional tensors is a series of matrix multiplications across the last two dimensions of the tensors, applied in parallel across all other dimensions. This is both a legal and highly practical extension of the matrix multiplication concept to accommodate the needs of modern computational tasks.
    

W enc: n_input_ae n_hidden_ae

W_dec: n_hidden_ae n_input_ae

The z activations are shape `n_hidden_ae`  

![Untitled](1%204%200186fae1fa2d41e49f64642d2ea523eb/Untitled%203.png)

Dim orders still don’t make sense uless we take transpose of W_dec

- will einsum automatically transpose if not the right shape?
    
    the notation explicitly states that the operation should treat *B* as if it were transposed, by how you've ordered its dimensions in the operation string.
    

I think that if we look at a col of the decoder, we can still get 

- slack question
    
    Hi, in 1.4 on sparse autoencoders part 6, the equation to get h' (reconstructed hidden state of model) is given in the attached image. h' has dimension `n_input_ae`   while the dim of decoder is W_dec: Float[Tensor, "n_instances n_hidden_ae n_input_ae"] . In matrix multiplication, if M has dim A x B and v has dim B x 1, the output Mv is dim Ax1. In this case, W_dec acting as M would be like having dim A as n_hidden_ae  ? When taking the einsum of W_dec and z, **how does the tensor multiplication work**? is it taking a transpose of W_dec?
    

SAE paper:

If we use tied weights, the decoder $M^T$ has dim $d_{in} \times d_{hid}$ , which is unlike what tensor implies

In any case, this `W_dec[0, 7, :]` is size MLP dims, so it’s the neurons that make up that col 7. This looks like “row 7” but based on SAE paper, think of it as col 7. So it’s every row of col 7, which is equivalent to the linear combo of neurons equation that makes up feature 7. 

We find that there’s no single neuron that makes up feature 7; it’s a linear combination of MANY neurons, each one contributing less than 1% to the total length of feature 7 (in contrast, if a single neuron did contribute a lot to feature 7, it would take much a larger %). So with so many neurons, this feature is dense in neuron space.

are the rows of encoder and cols of decoder of un-tied weights the same or similar? they should be. in tied, we can just use transpose.

ch 6, __init: 

In our `Model` implementations, we used a weight and its transpose. You might think it also makes sense to have the encoder and decoder weights be transposed copies of each other, since intuitively both the encoder and decoder weights for a particular feature are meant to represent that feature's "direction in the original model's hidden dimension".

The reason we might not want to tie weights is pretty subtle. The job of the encoder is in some sense to recover features from superposition, whereas the job of the decoder is just to represent that feature faithfully if present. The diagram below might help illustrate this concept. **This is why, when we have untied weights, we generally consider `W_dec` to be the "true direction" of the feature.**

![Untitled](1%204%200186fae1fa2d41e49f64642d2ea523eb/Untitled%204.png)

---

How to train own SAE using own model:

- Get model intermediate actv dims b/c SAE input is (batchsize * seqlen, actv dim)
    - z dim is (factor * actv dim)

How to ablate features:

See 4.1 of SAE paper

1. Get model intermediate actv and pass them to SAE
2. Ablate the feature (neuron) in z and run it again
3. Get the output of SAE and replace into orig model using hook fn; run again

Questions

- calculate loss: why sum over instances? why not mean?

---

If an activation has neurons, we can subtistute the neuron equation in terms of SAE features to get the features that fire for that activation, if we want to formally write out the equations . then apply set of equation algorithms

- In [1.4] Superposition & SAEs, in section 6 in the implementation of the forward pass of the SAE we normalize the decoder weights. What is the reason for that? I thought we usually normalize activations, not weights? And why only the decoder weights and not the encoder?
    
    If you don’t normalize the decoder weights the model can “cheat” on the L1 metric by making **the encoder weights extremely small and making the decoder weights extremely large**. If
    
    ```
    x' = W_d * f(W_e * x)
    ```
    
    , where features are
    
    ```
    f(W_e * x)
    ```
    
    .
    
    We can implement an equivalent function
    
    ```
    x' = W_d * 1000 * f((W_e / 1000) * x)
    ```
    
    , where the features are 1000x smaller but the reconstruction is the same.This is not what we want to achieve with the L1. So we fix the magnitude of the decoder weights, so that the SAE is forced to be sparse to achieve a low L1 loss.