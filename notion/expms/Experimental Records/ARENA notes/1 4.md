# 1.4

[https://colab.research.google.com/drive/1hoD36nsHp6K0E-YeFgPzxazlstb7HPyh#scrollTo=utS2IIVc5DSy](https://colab.research.google.com/drive/1hoD36nsHp6K0E-YeFgPzxazlstb7HPyh#scrollTo=utS2IIVc5DSy)

1.4, Section 1: This is a toy model with a bottleneck. To make the bottleneck, there’s a W matrix in to the low dim (bottleneck), and a W matrix out back into input space. WW is “correlation” between input vectors, or their cosine sim (via dot product).

1.4, Section 6: 

SAE inherits W from Model (toy), which has model.W. This W is updated during model.forward() as losses are used in optimizing W. Then, `plot_features_in_2d`from utils is used on model.W

> we found that over the course of training some neurons cease to activate, even across a large number of datapoints. We found that “resampling” these dead neurons during training gave better results by allowing the model to represent more features for a given autoencoder hidden layer dimension.
>