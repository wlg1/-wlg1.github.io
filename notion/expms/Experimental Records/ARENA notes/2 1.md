# 2.1

Sutton Ch2.1 to 2.7:

RL is one poss way to get AI to learn own morals. Because it must eval them by itself, associating long term reward with actions, rather than being purely instructed on what is right. it can change its views on how good actions are over time.

Multi-armed bandit:

- Number of Arms refers to number of actions
- The “bandit” is a name for a slot machine with k arms. Which sequence of arms to play to maximize long term reward?

First select mean q*(a) from normal distribution, then use this mean to construct a probabiilty distribution from which to select actual Reward R(a)

---

Render() used for plotting

First set up env, then pass to Agent class

[https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)

env.step() returns 5 values (truncated) in newer vers, but colab uses older vers

---

UCB: epsi greedy  not best bc you already have explroed some of them. so find “waht’s the best to explore if exploring”, and as more epsidoes go on then get more confident what action to explore

Markov assmp works in tic-tac-toe, but not Poker

Policy is a fn (either determ a = P(s), or stoch in which actions are sampled from a distribution using state s as conditional)

Value function: the expected discounted return from a state s (the sum of rewards it would expect to obtain by following it's currently chosen **policy)**

***not discounting would lead to a bunch of problems when we try to sum an infinite sequence of rewards, since that sum could be unbounded (or more generally non-converging).***

***geometric discounting is "time consistent", since the nature of the problem looks the same from each timestep.***

**Bellman equation: value function as a recursive function**

Finite number of states, know all transi probs, know all rewards (most practical don’t have all this), then compute this Bellman eqn with linear algebra

- why Q? is it estimate?
    
    The use of "Q" in "Q-value" in reinforcement learning has its origins in the word "quality." In this context, Q-values represent the quality of a particular action taken in a given state.
    
    In reinforcement learning, the goal is often to learn a policy that tells an agent what action to take under each possible state of the environment. The Q-value function quantifies the expected utility of taking a given action in a given state, following a certain policy. It incorporates not only the immediate reward received from the current action but also the expected rewards from the future states that this action leads to.
    
    Therefore, the "Q" in "Q-value" essentially stands for the "quality" of an action given a state, reflecting how good it is to take that action when in that state. This concept is central to many reinforcement learning algorithms, including Q-learning, where the agent learns the optimal policy by learning the Q-values associated with the action-state pairs.
    

---