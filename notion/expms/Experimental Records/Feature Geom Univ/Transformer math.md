# Transformer math

[https://www.lesswrong.com/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs](https://www.lesswrong.com/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs)

https://transformer-circuits.pub/2021/framework/index.html

read/write residual streams, so edges between components

[https://medium.com/@zepingyu/123-cb62513f5d50](https://medium.com/@zepingyu/123-cb62513f5d50)