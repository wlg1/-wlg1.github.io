# Multimodal interp notes

multimodal models in pretrained text

froze vision and text model, and just trained linear projection between them. found that even without adjusting weights within each model, that features ALREADY corresponded to each other