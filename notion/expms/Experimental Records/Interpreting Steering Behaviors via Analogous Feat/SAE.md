# SAE

[Training at scale](SAE%206b08b4ad57a342bf9393d2ef0fa31c6b/Training%20at%20scale%20dda12ef43b0e43d99c4cb630ea460461.md)

[Feature visualzier](SAE%206b08b4ad57a342bf9393d2ef0fa31c6b/Feature%20visualzier%2021c7e6bfa2c141539e7eaf5783a6a16a.md)

[SAEs on large models](SAE%206b08b4ad57a342bf9393d2ef0fa31c6b/SAEs%20on%20large%20models%204107692463a6493b8c7069a8c2910958.md)

---

[https://www.notion.so/wlg1/Scene-2-SAE-math-draft-v1-bde8381733a941299abac40ea63587a1](https://www.notion.so/Scene-2-SAE-math-draft-v1-bde8381733a941299abac40ea63587a1?pvs=21)

Choosing better features is sort of like view selection.

---

[https://www.alignmentforum.org/posts/HpAr8k74mW4ivCvCu/progress-update-from-the-gdm-mech-interp-team-summary](https://www.alignmentforum.org/posts/HpAr8k74mW4ivCvCu/progress-update-from-the-gdm-mech-interp-team-summary)

---

[JBloom May SAE workshop](SAE%206b08b4ad57a342bf9393d2ef0fa31c6b/JBloom%20May%20SAE%20workshop%20fe2e004ec02742a88a0c5a6ec61d7415.md)

saes: # train steps, params, gpus (make list)
<https://transformer-circuits.pub/2024/april-update/index.html#training-saes>
<https://chatgpt.com/c/7dc14ed2-d932-456a-99ed-2314ba373a37>
    l1 for sparse, l2 for reconstuction

![Untitled](SAE%206b08b4ad57a342bf9393d2ef0fa31c6b/Untitled.png)

---

[https://github.com/callummcdougall/sae_vis/tree/main](https://github.com/callummcdougall/sae_vis/tree/main)