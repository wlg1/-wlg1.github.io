# List of papers- concept repr

Concept Representation

- [https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)
    - [https://arize.com/blog/decomposing-language-models-with-dictionary-learning-paper-reading/](https://arize.com/blog/decomposing-language-models-with-dictionary-learning-paper-reading/)
- REPRESENTATION ENGINEERING
    - [https://openreview.net/forum?id=aCgybhcZFi&referrer=[the profile of Sanmi Koyejo](%2Fprofile%3Fid%3D~Sanmi_Koyejo1)](https://openreview.net/forum?id=aCgybhcZFi&referrer=%5Bthe%20profile%20of%20Sanmi%20Koyejo%5D(%2Fprofile%3Fid%3D~Sanmi_Koyejo1))

Features

- Sprase SAEs
    - [https://docs.google.com/document/d/18bxKmrBN4rhhY6vwhpYDf5XC4bHt0eRy3Le9Aco2-1Y/edit](https://docs.google.com/document/d/18bxKmrBN4rhhY6vwhpYDf5XC4bHt0eRy3Le9Aco2-1Y/edit)
- [LINEARITY OF RELATION DECODING IN TRANSFORMER LANGUAGE MODELS](https://arxiv.org/pdf/2308.09124.pdf)

Universality Hypothesis

- UNIVERSAL NEURONS IN GPT2 LANGUAGE MODELS

Feature disentanglement editing

- Evaluating the Ripple Effects of Knowledge Editing in Language Models
    - [https://github.com/edenbiran/RippleEdits/tree/main](https://github.com/edenbiran/RippleEdits/tree/main)
- RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations

Behavior interpretability

- AtP: An efficient and scalable method for

Latent Space Representation

- The Linear Representation Hypothesis and the Geometry of Large Language Models
    - [The Linear Representation Hypothesis and](https://www.notion.so/The-Linear-Representation-Hypothesis-and-80d9fdf91ca94c51921e60bb8b0806d6?pvs=21)
    - [https://github.com/KihoPark/linear_rep_geometry](https://github.com/KihoPark/linear_rep_geometry)
    - [https://scholar.google.com/scholar?oi=bibs&hl=en&cites=830461866206681894&as_sdt=5](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=830461866206681894&as_sdt=5)
- **On the Origins of Linear Representations in Large Language Models**
    - [https://arxiv.org/abs/2403.03867](https://arxiv.org/abs/2403.03867)
    - This says concept c3 is completed (by cond prob) from c1 and c2 that input maps to. This is similar (but not same as) how analogies map onto inputs to find the new completion. This is a clue on how abstractions “match in”.
- [https://www.youtube.com/watch?v=Ey8iiP0NblU&ab_channel=PiSchool](https://www.youtube.com/watch?v=Ey8iiP0NblU&ab_channel=PiSchool)
- [https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a](https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a)
- [TRAVELING WORDS- A GEOMETRIC … TRANSFORMERS](https://www.notion.so/TRAVELING-WORDS-A-GEOMETRIC-TRANSFORMERS-df3eaa0941fc4f6bb1bcbd9a80dcbfba?pvs=21)

Disentangling Concept Vectors (Vision)

- [Disentangling Neuron Representations with Concept Vectors](https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/OMahony_Disentangling_Neuron_Representations_With_Concept_Vectors_CVPRW_2023_paper.pdf)
    - [https://github.com/lauraaisling/sw-interpretability](https://github.com/lauraaisling/sw-interpretability)
- Uncovering Unique Concept Vectors through Latent

Others

- Explainability for Large Language Models: A Survey: [https://arxiv.org/pdf/2309.01029.pdf](https://arxiv.org/pdf/2309.01029.pdf)
- Learning from Emergence: [https://arxiv.org/pdf/2312.11560.pdf](https://arxiv.org/pdf/2312.11560.pdf)