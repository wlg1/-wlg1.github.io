# Interpret Attention Head Circuits of Analogous Inputs

### Working on

[Initial Exploration and Brainstorm](Interpret%20Attention%20Head%20Circuits%20of%20Analogous%20Inp%20ecf2fbc540454764a3a09cbee6505cc3/Initial%20Exploration%20and%20Brainstorm%2083932ec8b792498ba8dd97ec3875818b.md)

### Done

[Learn a workflow of transformer interpretability expms](Interpret%20Attention%20Head%20Circuits%20of%20Analogous%20Inp%20ecf2fbc540454764a3a09cbee6505cc3/Learn%20a%20workflow%20of%20transformer%20interpretability%20e%206632ac4e1e1147feac98478886ee0ee5.md)

### Future Work

[Map old experiments code to new task](Interpret%20Attention%20Head%20Circuits%20of%20Analogous%20Inp%20ecf2fbc540454764a3a09cbee6505cc3/Map%20old%20experiments%20code%20to%20new%20task%20f01e9beab44a4634b62297274922c7f3.md)

More in-depth literature review

- Map GPT-2 small to small-modified? To medium?
- Write program (by chatgpt) to generate prompts of a certain pattern
- Find a way to automate generating many patterns and running them all at once, seeing which yield significant results. Brainstorming patterns requires chatgpt to self reflect after testing them.
- graph matching algos: filter based on local conds, then piece together global and check for continued consistency ('until the analogy breaks down')
local edge: inh head to mover head. if missing, circuit not present.

### Ongoing Issues