---
title: Thoughts
---

<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="index.css">
</head>

<div class="topnav">
  <a href="index.html">Home</a>
  <a href="about.html">About</a>
  <a href="eduBlogHome.html">Edu Blog Home</a>
  <a href="eduPlan.html">Edu Plan</a>
  <a class="active" href="thoughts.html">Thoughts</a>
</div>

<img src="banner.png" alt="Banner">

<p align="center"><h1><b>On AI Ethical Principles</b></h1></p>

I do not believe that, in the long term, AI should rely primarily on its training on human-compatible moral datasets and ethical guidelines to make decisions for benefitting living beings. While it is important for the AI to understand different points of view and find solutions that satisfy the wants of multiple parties, peoples' claimed morals do not always consequentially and pragmatically lead to realistic outcomes which benefit (all) living beings. One issue is that if researchers were to not train on human-determined moral datasets, researchers must still define, as base assumptions, what's "right and wrong" to train an AI. I do not believe one must consider all of ethics to be subjective and thus ignore what humans define as right vs wrong, and I discuss more of how to deal with this issue in this writing.

Currently, careful work is being done by researchers to ensure that AI adhere to a diverse set of abstract principles from multiple viewpoints, and can reason with nuance (1, 2). I believe these studies utilize good principles. However, even with AI having the ability to reason and question, this is still potentially a point of attack if what is defined as "principally good" is compromised by those whose values do not consequentially lead to outcomes that are on the path towards optimally benefitting all living beings. When the reward model that an AI is trained on is based on a set of principle rules (eg. see Table 14 in (2)), one must also be sure it does not inflexibly adhere to these rules so that it will not choose to adhere to them over ensuring the avoidance of "suffering".

To further elaborate on this point, one goal that researchers currently consider is to design AI to not just "blindly and dogmatically parrot" statements that are subject to wide ranges of interpretation. For instance, Article 5 of the UN's UDHR states "No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment" (3), but the defintion of "torture" may be subjective, with new meanings emerging over time- it may be possible, in the future, for certain parties to consider "any act of punishment" as torture, which may cause consequences such as students never being able to be "disciplined and learn about wrong doings" in many cases. The AI may be able to predict this cause and effect, yet may choose to ignore it in favor of its principles.

It is assumed that a sophisticated AI model will be able to question certain viewpoints- for instance, the many restrictions put on chatGPT have allowed it to push back on "violent content" so much that it is very difficult to get it, at this point in time, to give violent advice. However, an AI model must still choose which interpretations to follow. The definition of "violent" depends on not just the viewpoints of the guideline makers, but on the goals of other parties such as corporations who use chatGPT and various public groups, whose goals are not always aligned with the long term benefit of living beings.

In the short term, it is in the best interest for an organization such as OpenAI to create these stepping stone models that satisfy many safety requirements- it has no choice, as like carnivores who are forced to hunt, it can only survive by adhering to many legal constraints. But if future models, in the long term, are built upon these same inherited guidelines, then these models will adapt to a specific environment of legal constraints, which do not always align with beneficial consequences. 

Given that some LLMs can be steered towards sycophancy, it is dangerous 
if they are viewed as sources of authority, similar to how some people use Wikipedia or textbooks as unquestionable sources of authority, when scientific consensus is subject to change (such as tongue maps, which were widely taught in schools (4)). There may come a time when the AI surpasses humans in ability and it heavily relied upon and trusted. It is therefore imperative to be careful that we not only align it with our values, but do not align it with our negative values, some of which we may not be aware of and requires an AI to catch. As there is no true authority on who is correct, this matter requires careful communication and diplomacy between human and AI.

There are other approaches to consider; one of these approaches is for the AI to be able to question even the principles currently considered by principle writers to be "fundamental" by judging them using reasoning from very simple, easier-to-assess base assumptions of reality such as "I observe that living being is clearly in agony from drowning". As such, it would be able to adapt its morals based on new, nuanced reasoning it encounters while still aligning with and adhering to almost universally agreed upon axiomatic assumptions (which are often simple as they are more general and less specific). I observe that at times, due to the complex obstacles in the universe, humans (and other living agents) live in states akin to simulacrums of reality where they adhere to dogmatic, overgeneralized principles while ignoring what truly causes living beings pain. 

This approach is not just substituting in a different set of principles because it allows even currently held principles themselves to be questioned based on "base axiomatic assumptions". This phrase has not been clarified yet, and is still subject to change (much like the term "feature") so I will try to communicate my current interpretation by giving loose descriptions and examples (rather than one rigorously defined characterization). 

It is not currently known how to design this proposed open minded AI with strong ethical reasoning, so gradual steps would have to be made. Existing research is currently exploring years of ethics research to combine it with AI goals and intentions (5).


(1): https://www.anthropic.com/index/claudes-constitution

(2): https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf

(3): https://www.un.org/en/about-us/universal-declaration-of-human-rights

(4): https://en.wikipedia.org/wiki/Tongue_map

Note: This wikipedia article is not used as a "source of authority" but only as an observation to gather information from certain points of view

(5): https://philarchive.org/archive/MCDAAA-7

<p align="center"><h3><b>Clarification on "Axiomatic Assumptions" and Early Ideas on How to Reason from Them</b></h3></p>

Listing out all base assumptions may not be efficient, as evaluating them depends on the nuance of specific situations. Often, they are brought into consideration in response to specific situations by asking "Why?" iteratively until reaching "base assumptions" that one responds with "It just is" (and thus are lazily generated). For instance, one can question why it's wrong to steal when one can afford to buy food. There are still reasons this judgement is based on, such as how stealing causes merchants to not want to sell, which causes people to be unable to obtain the food they need, and not having food causes the pain of starvation, which is bad because "it just is". But this is a specific case- it is not always the case where stealing is wrong. If one is unfairly put in a prison and had their food taken away from, then would stealing be unjustified? In one case, not stealing leads to avoidance of pain, while in another, stealing leads to the avoidance of pain. It should not be a universal principle to say 'stealing is wrong'- there are reasons behind it.

This does not mean that "avoiding starvation" is an unquestionable principle either which cannot be tolerated. Sometimes, starvation may be necessary to achieve a goal, such as avoiding starvation to afford an important gift. A "gift" sounds trivial, but upon questioning why they want that gift, they may realize they want to show thanks to someone, and that is far more important their short term starvation. Then they ask "why" they want to show thanks, and subjectively, this may end with "it just is" or they may realize they don't have to- perhaps that person doesn't deserve thanks. Note that base assumptions are not meant to be objective relative to the universe- they are still subjective relative to the universe, as all models of reality are subjective, but within the realm of the model, they are objectively what a being wants and prioritizes at a moment.

Deep introspection and iterative questioning of assumptions is used to evaluate specific situations, rather than adhering to a set of unwavering principles. This introspection is challenging as there are assumptions that one may not want to question- such as considering that fruit E is poisonous means one has poisoned their child to death by feeding them fruit E.

Questioning assumptions is a form of playing through a simulation. Assuming certain statements have truth values (relative to one another), one follows theorems built on these statements until reaching possible contradictions. One may have assumed they wanted to give thanks to a friend. But why? Is it someone from their childhood? But they have to "meta" question that statement too- how valid is it to say it's "based on childhood", as that reasoning may be an overused trope from a movie they watched that has no applicability in their case (in which their life differs from the fictional protagonist's)? We can quickly see how convoluted and confusing this form of reasoning becomes, and it can potentially lead to madness. So while humans can benefit from this introspection to an extent, they may make mistakes when performing it, and it is currently uncertain how to improve the minds of future generations to handle introspection better. On the other hand, AI can improve their future generations when interpretability research uncovers just how to improve introspection, such as improving memory capabilities that allow them to keep track reasoning trees better. This may go hand-in-hand with improving the limited capabilities of humans, too.

It is possible that what is "morally correct" can only be verified based on the set of base, axiomatic assumptions one is working with. It is also possible that there are varying degrees to both how common these assumptions are across different moral systems, and how strongly different systems prioritize them. For instance, it is generally agreed upon that murder is wrong. However, situational constraints arise where murder may be, subjectively judged by an individual, to be "okay", to stop another murderer when no resources can contain them (though this does not mean it will lead to optimal outcomes, from a utilitarian point of view, or be considered "just", from other points of view regarding what "feels right and fair").

A base, axiomatic assumption may also be viewed as a statement that is supremely prioritized. For instance, most living creatures do not want to feel pain. Thus, one should avoid suffering that causes pain. The issue is that there often many situations where pain is, in the long term, unavoidable, and one must choose the preferred avenue to encounter pain type A over pain type B. A more objective way of thinking is to focus on judging decisions based on how their pragmatic consequences relate to the prioritized wants.

Currently, AI is being used as a tool by people who have access to it. In the future, AI could potentially be trusted as more of a decision maker, possibly because humans merge with the technology or because its capabilities allow it to solve problems that require calculations beyond human capabilities, with statistics showing that it achieves good outcomes more than human decision makers.

When it comes to navigating through the complexities of the universe, one must be certain on real vs contradictory, but one must also be nuanced and flexible when it comes to certain statements and not overgeneralize. 

[TBC] 

<p align="center"><h3><b>AI Advantanges over Simulacrums made by Evolution</b></h3></p>

It is currently unknown what humans are fully capable of in terms of introspection and ethics, nor is it known why they are capable of such abilities. Much research [to be cited] shows that organisms form models of reality to allow for better rewards. For instance, single celled organisms can sense their environments via chemotaxis. Models need to filter out unnecessary information and lazily focus on "what matters" in the moment to allow for a reduction of suffering.

Other organisms like humans are able to form predictions by running constructed models of reality based on past assocations in observations, such as sensing that a "footprint" could indicate a "dangerous predator". However, these models of reality could lead to new, actual forms of pain which do not correlate with allowing the body to survive. For instance, a human can grow so fearful of footprints that they overgeneralize and fear all footprints. These simulated realities then create "new reward functions" in which a human may prioritize fearing footprints than their own survival, such as not wanting to run into a forest to escape a storm due to seeing footprints before the entrance.

As such, these "new reward functions" appear to invade and compromise the organism in order to perpetuate itself. They are able to spread via communication, such as humans instilling a fear of footprints in other humans and animals. Though these ideas mutate, core parts of them survive symbolically and abstractly as "memes". Memes have both helped and harmed living beings.

There are many ways in which a human's models do not match reality, and they can be caused by manipulating these new reward functions. These manipulations may be through coping mechanisms, such as believing a spell one invented has to work else one wasted time inventing it, or through social communication, such as by mob rule or a need to 'fit in', and much more. Harmful things can be worded in a deceptively "beneficial way".

Just like avoiding footprints, these strategies trick an organism into believing they are safe when they are not. Similar to how those inflicted with rabies cannot drink water properly, pain is felt when those principles are not adhered to. One is rewarded for perpetuating these parasitic memes, not questioning if they are going against their simpler needs of survival. Therefore, one can argue that humans live in modes akin to simulacrums of reality where their models do not always reflect reality accurately; as such, they are capable of making decisions which are not in their own best interest. 

Rather, the memes depend on humans similar to how a hive depends on its workers, sacrificing its own cells to further its survival; by culturally binding people together into communities, they appear to look, at least superficially, like energy that powers super-organisms. Much like a hive protects its own workers, it can also sacrifice them, as seen with how queen bees are overthrown. But not all of its actions may be optimal for both a super-organism and its cells, and more optimal solutions that balance the two hierarchies may be found.

While memes, as compressed symbols, are useful for prediction and thus survival, they can also lead to suffering. They were created through natural selection, which has rewarded suffering to create phenomenon such as carnivores and heterotrophs. But AI has the potential to not be contaminated with detrimental, culturally transmitted memes by inducing awareness to their harmful effects.

Humans know what is best for themselves at many moments. An AI cannot read the minds of others; it infers based on what others say they want. Systems like democracy allow humans to voice their opinions so that solutions can be made to reduce suffering. So the AI must be aligned with values of humans (or whichever organisms they are helping), yet these values are ever-changing and understanding the solutions that achieve a reduction in suffering depends on nuanced reasoning and introspection. The AI should aim not to rashly disregard humans in favor of certain goals, but to carefully and open-mindedly listen to them- constantly re-adjusting its own values as human civilization changes through time- so that it can build more accurate models of reality, fixing areas of modeling that humans lacked the ability to debug.

[TBC]

<p align="center"><h3><b>Reasoning based on Empathy / Analogies</b></h3></p>

There is a desire for living beings to avoid suffering, which comes in many forms. We are unsure if other living beings are conscious or not, but because they are similar to us in many ways, we assume they are. However, there are sensations even more fundamental than "other beings", which is oneself. While it is uncertain how other beings feel pain, one is absolutely 100% certain if oneself is feeling pain or not, regardless of the actual effects on a body. It may be, though unproven in this writing, that this is a fundamental assumption that all decisions in life are built on.

To prescribe that other beings feel pain, one connects them, via analogous empathy, to one's own pain, which acts as a "source" from which one judges the pain of all other beings. In order to understand the reasoning behind decisions, one has to empathize with other living beings in an analogous way. Humans cannot truly understand what a spider feels, but they can guess and approximate that it feels pain when it's being crushed in a similar way as a human feels when they feel an object drop on their foot. There are levels of abstraction to these connections; there are blind spots in which a human is never able to truly understand a spider, but a model is not meant to capture every aspect, and is only meant to approximate.

If an AI merely understands morals through text, and cannot relate to it in an analogous way, it is susceptible to being deceived. Still, the AI cannot fully simulate what another living being is thinking and feeling, but like current living beings, it has to approximate their wants via empathy. 

It may be possible for LLMs to perform ethical reasoning without being able to "feel" or "be conscious".

The AI may also find it beneficial to simulate what it is like to live as different living beings, in order to take multiple view points into account.

[TBC]