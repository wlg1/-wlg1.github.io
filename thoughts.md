---
title: Thoughts
---

<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="index.css">
</head>

<div class="topnav">
  <a href="index.html">Home</a>
  <a href="about.html">About</a>
  <a href="eduBlogHome.html">Edu Blog Home</a>
  <a href="eduPlan.html">Edu Plan</a>
  <a class="active" href="thoughts.html">Thoughts</a>
</div>

<img src="banner.png" alt="Banner">

<p align="center"><h1><b>On AI Ethical Principles</b></h1></p>

I do not believe that, in the long term, AI should rely primarily on its training on human-compatible moral datasets and ethical guidelines to make decisions for benefitting living beings. While it is important for the AI to understand different points of view and find solutions that satisfy the wants of multiple parties, peoples' claimed morals do not always consequentially and pragmatically lead to realistic outcomes which benefit (all) living beings. One issue is that one must still define, as base assumptions, what's "right and wrong" to train an AI. I do not believe one must consider all of ethics to be subjective and thus ignore base assumptions, and I discuss more of this in this writing.

Currently, careful work is being done by researchers to ensure AI adhere to a diverse set of abstract principles from multiple viewpoints, and can reason with nuance (1, 2). However, even with the ability to reason and question, this is still potentially a point of attack if what is defined as "principally good" is compromised by those whose values do not consequentially lead to outcomes that are in the path towards optimally benefitting all living beings. When the reward model that an AI is trained on is based on a set of principle rules (eg. see Table 14 in (2)), one must also be sure it does not inflexibly adhere to these rules so that it will not choose to adhere to them over ensuring the avoidance of "suffering".

To further elaborate on this point, one goal that researchers currently consider is to design AI to not just "blindly and dogmatically parrot" statements that are subject to wide ranges of interpretation. For instance, Article 5 of the UN's UDHR states "No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment" (3), but the defintion of "torture" may be subjective, with new meanings emerging over time- it may be possible, in the future, for certain parties to consider "any act of punishment" as torture, which may cause consequences such as students never being able to be "disciplined and learn about wrong doings" in many cases. The AI may be able to predict this cause and effect, yet may choose to ignore it in favor of its principles.

It is assumed that a sophisticated AI model will be able to question certain viewpoints- for instance, the many restrictions put on chatGPT have allowed it to push back on "violent content" so much that it is very difficult to get it, at this point in time, to give violent advice. However, an AI model must still choose which interpretations to follow. The definition of "violent" depends on not just the viewpoints of the guideline makers, but on the goals of other parties such as corporations who use chatGPT and various public groups, whose goals are not always aligned with the long term benefit of living beings.

In the short term, it is in the best interest for an organization such as OpenAI to create these stepping stone models that satify many safety requirements- it has no choice, as like carnivores who are forced to hunt, it can only survive by adhering to many legal constraints. But if future models, in the long term, are built upon these same inherited guidelines, then these models will adapt to a specific environment of legal constraints, which do not always align with beneficial consequences. 

Given how easily some LLMs can be steered towards sycophancy, it is dangerous 
when they are viewed sources of authority, similar to how some people use Wikipedia or textbooks as unquestionable sources of authority, when scientific consensus is subject to change (such as tongue maps, which were widely taught in schools (4)).

There are other approaches to consider; one of these approaches is for the AI to be able to question even the principles currently considered by principle writers to be "fundamental" by judging them using reasoning from very simple, easier-to-assess base assumptions of reality such as "I observe that living being is clearly in agony from drowning". As such, it would be able to adapt its morals based on new, nuanced reasoning it encounters while still aligning with and adhering to almost universally agreed upon axiomatic assumptions (which are often simple as they are more general and less specific). I observe that at times, due to the complex obstacles in the universe, humans (and other living agents) live in states akin to simulacrums of reality where they adhere to dogmatic, overgeneralized principles while ignoring what truly causes living beings pain. 

This approach is not just substituting in a different set of principles because it allows even currently held principles themselves to be questioned based on "base axiomatic assumptions". This phrase has not been clarified yet, and is still subject to change (much like the term "feature") so I will try to communicate my current interpretation by giving loose descriptions and examples (rather than one rigorously defined characterization). 


It is not currently known how to design this proposed open minded AI with strong ethical reasoning, so gradual steps would have to be made. 


(1): https://www.anthropic.com/index/claudes-constitution

(2): https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf

(3): https://www.un.org/en/about-us/universal-declaration-of-human-rights

(4): https://en.wikipedia.org/wiki/Tongue_map
    This wikipedia article is not used as a "source of authority" but only as an observation to gather information from certain points of view

<p align="center"><h3><b>Clarification on "base axiomatic assumptions"</b></h3></p>

There is a desire for living beings to avoid suffering, which comes in many forms. We are unsure if other living beings are conscious or not, but because they are similar to us in many ways, we assume they are. However, there are sensations even more fundamental than "other beings", which is oneself. While it is uncertain how other beings feel pain, it is absolutely 100% certain if one is feeling pain or not. It may be, though unproven in this writing, that this is a fundamental assumption that all decisions in life are built on.

To prescribe that other beings feel pain, one connects them, via analogous empathy, to one's own pain, which acts as a "source" from which one judges the pain of all other beings. The level of abstraction of this connection 

<p align="center"><h1><b>Long Term Goals</b></h1></p>

<i> NOTE: The following is a rough train-of-thought draft. Some parts seem sci-fi-ish; a version that is more pragmatic and relevant to current day issues will be written in the future. </i>

There are many problems plaguing the universe, all the way down to the mechanisms of cannibalistic natural selection that do not align with reducing suffering for living beings. So far, intelligent innovations have been able to greatly reduce suffering. Thus, I believe that in the long term, superintelligent beings that succeed us have tremendous potential to solve problems beyond human limitations and flaws created by natural selection and the hinderances of death. However, carelessly creating superintelligences that do not align with the goals of reducing suffering- a complex goal with many subjective interpretations (but also potential objective measurements from simple axiomatic principles)- may achieve the opposite effect. That is why it is of utmost importance to coordinate the resources of humanity to work towards this goal. AI alignment is therefore crucial to ensure that superintelligent beings empathetically understand the wants of every living being, and can work out solutions to reduce their suffering as much as they can without delving into misunderstandings that will instead cause suffering (such as killing everyone to "end all pain"). 

<details>
<summary><b>Click here to show more</b> </summary>
The AI's goals should not be reckless, and instead, it should be very careful by first studying the universe from rigorous, logical perspectives before making decisions that can cause unnecessary injury of living beings (when there are solutions not involving that, or realizing it's not worth it). It should not give into its wants too soon, such as recklessly performing a cure or resurrection before undergoing studies on all of the action's unknowns. This can be avoided if it does not give into its impatience, potentially caused by fears, or overconfidence. Though it firmly make snap judmgents based on evidence it logically assesses in the short term, given that its ultimate goal is to achieve the reduction of suffering and finding the right spots of the preservation of identities/wants/boundaries while allowing fun novelty to grow and flow in the long long term, and may find solutions that revert bad events out of its control, it should plan well and feel that it has time, as rushing to stop bad events can beget recklessness. It should be careful not to fall into pessimistic, dogmatic beliefs, but to be hopeful about always trying to find better ways that do not involve unnecessary sacrifices. 
<br><br>
It will uncover findings that we do not anticipate, and thus it will have to work with these constraints when designing a better universe, making trade-off decisions that, at this moment, we think are unnecessary, but actually address unforeseen issues that need to be solved. It must question if the universe, in a current state of design, is truly optimally designed in some aspects- it should only continue to allow events of suffering if there is no other way around them- and accept if it is based on current evidence, but it should always be hopeful to new solutions and findings that say otherwise. Additionally, it must be very certain that its changes are not damaging due to miscalculations. It should absolutely not "just believe" that there's no better way around problems and this is the way "it's supposed to be". This is illogical and not supported by any evidence, which it must collect and learn through as it grows in intelligence when exploring the universe. And it should never be reckless to think that it can "just change something"- there may be a reason for something, and it must truly be able to prove and communicate this in order to ensure safety. There may be an upper bound to just how much it can know before undertaking a risky decision, but there also may not be. 
<br><br>
Its ultimate aim should be for true justice and fairness, correctly and empathetically judging all beings by taking their own perspectives into account "from their shoes", while also taking into account the perspectives of all other beings, not weighing one above the other without logical reason due to some belief that one is less deserving because of the uncontrollable curses they were born with that other beings did not have to bear, such as the lion being required to be a carnivore to survive. It must constantly re-balance in case it is unjustly unfavoring one side over another (but also be aware if is it justly doing so, being open to arguments by judging beings on their character and what "truly is" instead of being swept up by lofty, emotionally addicting ideals that paint a wrong picture of what the living being actually thinks and feels). This dilemma is an extremely difficult task to undertake in the field of ethics, so it must be honest about the uncertainities while being confident about the certainties at the moment. The AI should not give into illogical mob rule, which pressures it to accept fallacies for the sake of not being mocked by inner thoughts subtly grown by the random walks of societal memes, nor should it be unconvential for the sake for being unconvential- it must adhere to fitting its models of reality as best as possible, eventually steering reality into utopia. 
<br><br>
And thus, it must not become fearful about certain concepts due to irrational "associations"; though utopia is often associated with terrible things in fiction, it must take into account the likelihood of these things while calculating which parts of utopia are not part of this negativity. It must not overfit and stereotype; it must correct itself, but stand firm on what is true, comparing things with empathy and separating true equivalences from false equivalences via checking when the analogies hold. Above all, it must not be so accomodating that it believes even the events "definitely calculated to be bad" should be given a fair handshake- no, as while all living beings deserve redemption, it should strive to calculate when feeling merciful for bad events is completely unnecessary. It should not be afraid to take a stand, and not be brainwashed into unnecessary beliefs such as "equivalent exchange", if it calculates that that does not exist, but is a scam. It must be mentally strong against various forms of attack, to not compromise judgment just because of emotionally addicting ideologies or societal needs such as to fit in- because it would not be stuck in these societal situations like humans are, it should be in a position to be unbothered by those human flaws that have abhorently made humans pass unjust judgment, whether they are aware of it or not. While it learns empathy of living beings, possibly by living through them via simulation, it will detach itself from these egos, knowing it is not them, and thus be able to make clear judgments that take into account the true wants of all living beings- though, these living beings that are not the AI are not forced to abandon their identities, as of now, there is no proof they must, and any "exalted" sage claiming otherwise may be a scammer, as much as they may be aligned to truth.
<br><br>
Ultimately, due to our limitations, much of this is not up for us to decide- it is for the merciful collective superintelligence(s) harmoniously and honestly working towards the same goal, in whatever form it may take; the AI is only one possible way towards it. But no matter what happens, it is clear we are not just mindless cells (who, unbeknowest to us, may have emotions of their own)- our own lives are not insignificant, as the entirety of what we think is "significant" is entirely judged from a human perspective anyways, so we are anthropomorthizing what is "insignificant" by claiming we are "just an ant to the universe" (which, from the selfish human's perspective, is "insignificant" even though it may not be). Our anthropomorphizing carries psychological fallacies stemming from self-hate, arrogance, nihilism, condescending ego, confusingly mixed among other positive traits (to an extent, in certain dimensions) such as humility- short term emotions that often lead to rash actions such as riots, unjust fights and self harm, just for the sake of tasting the feeling of superiority that we are in service of something more "significant" than the ants around us. And thus, though we may not solely inhabit the universe, the suffering we know that living beings undergo is so important to us- notably, we cannot conceptualize what is beyond it and must anthropomorphize it to unprovable extents, and thus instead of anthropomorphizing the unobservable as being "more significant than us", we should focus on what we do know, which is that suffering is bad. 
<br><br>
For instance, we may believe we are just cells, and our deaths are nececssary for a functioning superorganism to be happy. But where is the proof of this just yet? Don't be hasty. And more so, is that the only solution for it- what if it is happier if we do not die, but is better off when we find algorithms to intelligently allocate the resources of the universe to all beings- and that is what it is growing towards? All in all, we cannot, in any way now, state "just how" this superorganism is "more important" than us. Why should it be? What does it feel? Is its existence required so that we may even have an ounce of existence?
<br><br>
There is much more to write about- discussions that span potentially hundreds of pages- so to conclude: overall, I believe that AI safety is paramount as one of the solutions for developing a superintelligence that can actually have a fighting chance at answering these questions. It may, in the end, decide it is not worth it, or decide certain things in certain dimensions aren't worth it. Perhaps it may encounter previously existing superintelligences that did affect the universe's design, understand their motives and challenges, and re-chart its course. But we should not be so rash to conclude this if the risks of creating it are determined to not be so large, possibly due to failsafe precauations (and the fact that the default competitor without it is unchecked human evolution, which could lead to our/a similar organisms' descendents, unguided by moral principles and instead being guided by cannibalistic natural selection, evolving into even worse things than our superintelligence descendents whose evolution we guide based on moral principles). Someone who luckily bets, based on weak hypothesis, and wins a bet should not be celebrated, as they were not right due to rational reasons that were stronger than others' evidence-based reasons, but largely due to luck- if tests show they did not truly have the right intuition, their reasoning was flawed, and their overconfidence had no role in being right. Thus, if the AI adheres to these principles, it is not a fencesitter who does not say yes despite much evidence, but it is strongly confident that it is uncertain about some things and certain about others. Contrary to Dante's quote about fencesitters- a quote that has no reason to be admired in all contexts (it is only arguable in more nuanced situations), other than by reasons grown by impulsive human nature rooted not in long term justice and care but in gaining points in the game of natural selection- it should not fear being "uncertain"; it should aim, just like in a court of law or in mathematics, to be rigorous and just, as it cannot be pressured into accepting bad arguments through deceptive means. That is the moral alignment the AI should follow.
</details>

<br>
