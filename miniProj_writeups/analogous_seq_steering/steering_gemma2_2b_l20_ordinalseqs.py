# -*- coding: utf-8 -*-
"""steering_gemma2_2b_L20_ordinalSeqs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbN7SGKgUc-DkQkcv3tKyPS1Hl9N8d_c

**IMPT**

Before running, download 'gemma-2-2b-20-gemmascope-res-16k-explanations.json' from [https://www.neuronpedia.org/](https://www.neuronpedia.org/), and upload it to the session's files.

Then log into hf using token. Then after entering it in, and do Run All.

NOTE: Run All has strange issue where sometimes it doesn't work properly. If the bar plots don't match what's seen in the writeup (eg. missing bars), run each section again one by one.
"""

# req to load model into transformerlens
from huggingface_hub import hf_hub_download, notebook_login
notebook_login()

"""# setup"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformer_lens

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
from torch import nn, Tensor
from transformer_lens.hook_points import HookPoint

# load this after transformerlens else issue
# from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer

from functools import partial
from jaxtyping import Float, Int
from typing import Optional, Callable, Union, List, Tuple

device = "cuda" if torch.cuda.is_available() else "cpu"

"""# load labels"""

import json
with open('gemma-2-2b-20-gemmascope-res-16k-explanations.json', 'rb') as f:
    feat_labels_allData = json.load(f)

feat_labels_lst = [0 for i in range(feat_labels_allData['explanationsCount'])]
feat_labels_dict = {}
for f_dict in feat_labels_allData['explanations']:
    feat_labels_lst[int(f_dict['index'])] = f_dict['description']
    feat_labels_dict[int(f_dict['index'])] = f_dict['description']
    if int(f_dict['index']) == 0:
        print(f_dict['description'])

len(feat_labels_dict)

"""## search for features"""

# def find_indices_with_keyword(f_dict, keyword):
    # """
    # Find all indices of fList which contain the keyword in the string at those indices.

    # Args:
    # fList (list of str): List of strings to search within.
    # keyword (str): Keyword to search for within the strings of fList.

    # Returns:
    # list of int: List of indices where the keyword is found within the strings of fList.
    # """
    # filt_dict = {}
    # for index, string in f_dict.items():
    #     # split_list = string.split(',')
    #     # no_space_list = [i.replace(' ', '').lower() for i in split_list]
    #     # if keyword in no_space_list:
    #     if keyword in string:
    #         filt_dict[index] = string
    # return filt_dict

# keyword = "number"
# number_feats = find_indices_with_keyword(feat_labels_dict, keyword)

# keyword = "month"
# month_feats = find_indices_with_keyword(feat_labels_dict, keyword)

"""# load model"""

torch.set_grad_enabled(False) # avoid blowing up mem

# model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-2-2b",
#     device_map='auto',
# )

# tokenizer =  AutoTokenizer.from_pretrained("google/gemma-2-2b")

"""## load transformerlens"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from transformer_lens import HookedTransformer
# 
# model_2 = HookedTransformer.from_pretrained(
#     "gemma-2-2b"
# )

tokenizer =  model_2.tokenizer

"""## test prompts"""

# prompt = "one two three four"
# inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")
# outputs = model.generate(input_ids=inputs, max_new_tokens=1)
# print(tokenizer.decode(outputs[0, -1]))

"""# load sae"""

path_to_params = hf_hub_download(
    repo_id="google/gemma-scope-2b-pt-res",
    filename="layer_20/width_16k/average_l0_71/params.npz",
    force_download=False,
)

params = np.load(path_to_params)
pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}

import torch.nn as nn
class JumpReLUSAE(nn.Module):
  def __init__(self, d_model, d_sae):
    # Note that we initialise these to zeros because we're loading in pre-trained weights.
    # If you want to train your own SAEs then we recommend using blah
    super().__init__()
    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))
    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))
    self.threshold = nn.Parameter(torch.zeros(d_sae))
    self.b_enc = nn.Parameter(torch.zeros(d_sae))
    self.b_dec = nn.Parameter(torch.zeros(d_model))

  def encode(self, input_acts):
    pre_acts = input_acts @ self.W_enc + self.b_enc
    mask = (pre_acts > self.threshold)
    acts = mask * torch.nn.functional.relu(pre_acts)
    return acts

  def decode(self, acts):
    return acts @ self.W_dec + self.b_dec

  def forward(self, acts):
    acts = self.encode(acts)
    recon = self.decode(acts)
    return recon

sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])
sae.load_state_dict(pt_params)

sae.cuda()

"""# get actv fns using transformerlens"""

def gather_residual_activations(model_2, target_layer, inputs):
    h_store = torch.zeros((inputs.shape[0], inputs.shape[1], model_2.cfg.d_model), device=model_2.cfg.device)

    def store_h_hook(
        pattern: Float[Tensor, "batch seqlen d_model"],
        hook: HookPoint,
    ):
        # h_store = pattern  # this won't work b/c replaces entire thing, so won't be stored
        h_store[:] = pattern  # this works b/c changes values, not replaces entire thing

    layer_name = f'blocks.{target_layer}.hook_resid_post'
    model_2.run_with_hooks(
        inputs,
        return_type = None,
        fwd_hooks=[
            (layer_name, store_h_hook),
        ]
    )

    return h_store

"""## get actv fns"""

# def gather_residual_activations(model, target_layer, inputs):
#   target_act = None
#   def gather_target_act_hook(mod, inputs, outputs):
#     nonlocal target_act # make sure we can modify the target_act from the outer scope
#     target_act = outputs[0]
#     return outputs
#   handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)
#   _ = model.forward(inputs)
#   handle.remove()
#   return target_act

prompt = "January February March April"
inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")

target_act = gather_residual_activations(model_2, 20, inputs)

target_act.shape

"""Now, we can run our SAE on the saved activations."""

sae_acts = sae.encode(target_act.to(torch.float32))
sae_acts.shape

"""# steering fns"""

def patch_and_steer(tokens, patch, multp, layer_id):
    # replace LLM actvs in that layer with decoder output

    layer_name = f'blocks.{layer_id}.hook_resid_post'

    def patch_layer(
        orig_actvs: Float[Tensor, "batch pos d_model"],
        hook: HookPoint,
        LLM_patch: Float[Tensor, "batch pos d_model"],
        layer_to_patch: int,
    ) -> Float[Tensor, "batch pos d_model"]:
        if layer_to_patch == hook.layer():
            orig_actvs[:, :, :] = LLM_patch
        return orig_actvs

    hook_fn = partial(
            patch_layer,
            LLM_patch= patch,
            layer_to_patch = layer_id
        )

    # if you use run_with_cache, you need to add_hook before
    # if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg
    # no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg

    # rerun clean inputs on ablated model
    ablated_logits = model_2.run_with_hooks(tokens,
                        fwd_hooks=[
                            (layer_name, hook_fn),
                        ]
                    )

    next_token = ablated_logits[0, -1].argmax(dim=-1)
    next_char = model_2.to_string(next_token)
    print(next_char)
    return next_char

def steer_by_sae_lastTok(prompt, steer_vec, multp):
    tokens = model_2.to_tokens(prompt).to(device)
    inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")
    target_act = gather_residual_activations(model_2, 20, inputs)
    sae_acts_3 = sae.encode(target_act.to(torch.float32))

    sae_acts_3[:, -1, :]  += multp * steer_vec[:, -1, :] # in-place
    recon = sae.decode(sae_acts_3)

    layer_id = 20
    return patch_and_steer(tokens, recon, multp, layer_id)

def steer_by_LLM_lastTok(prompt, steer_vec, multp):
    tokens = model_2.to_tokens(prompt).to(device)
    inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")
    target_act = gather_residual_activations(model_2, 20, inputs)

    target_act[:, -1, :]  += multp * steer_vec[:, -1, :] # in-place

    layer_id = 20
    return patch_and_steer(tokens, target_act, multp, layer_id)

"""# scoring fns"""

def get_test_scores_steer_actvs(prompts_list, answers, steer_vec, multp, actv_type):
    results = []
    score = 0
    for p_id, prompt in enumerate(prompts_list):
        if actv_type == 'SAE':
            output = steer_by_sae_lastTok(prompt, steer_vec, multp)
        else:
            output = steer_by_LLM_lastTok(prompt, steer_vec, multp)
        if p_id < len(answers):
            results.append({
                'Prompt': prompt,
                'Answer': answers[p_id],
                'Output': output
            })
            output_reformat = output.replace(" ", "")
            if output_reformat == answers[p_id]:
                score += 1
        else:
            results.append({
                'Prompt': prompt,
                'Answer': '',
                'Output': output
            })
    return pd.DataFrame(results), score / len(answers)

def run_tests_steer_actvs(steer_vec, multp_list, actv_type):
    all_scores = []
    all_tables = []
    for prompts_list, answers in zip(all_test_prompts, all_test_answers):
        prompts_table = []
        prompts_scores = []
        for multp in multp_list:
            df, score = get_test_scores_steer_actvs(prompts_list, answers, steer_vec, multp, actv_type)
            prompts_table.append(df)
            prompts_scores.append(score)
        all_tables.append(prompts_table)
        all_scores.append(prompts_scores)
    return all_tables, all_scores

"""# make test data"""

test_labels_lst = ['numwords', 'months', 'days', 'tomorrow_is', 'nextMonth_is', 'descr_numwords', 'spanish_numwords']

all_test_prompts = []
all_test_answers = []

words = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']
numword_samps = []
for i in range(0, 9):
    samp = f"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]}"
    numword_samps.append(samp)
answers = [str(i) for i in range(5, 10)]

all_test_prompts.append(numword_samps)
all_test_answers.append(answers)

words = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
months_samps = []
# for i in range(0, 9):
for i in range(0, 5):
    samp = f"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]}"
    months_samps.append(samp)
answers = [str(i) for i in range(5, 10)]

all_test_prompts.append(months_samps)
all_test_answers.append(answers)

words = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
day_samps = []
for i in range(0, 5):
    samp = f"{words[i]} {words[i+1]} {words[i+2]}" # {words[i+3]}"
    day_samps.append(samp)
answers = [str(i) for i in range(4, 7)]

all_test_prompts.append(day_samps)
all_test_answers.append(answers)

words = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
tomorrowIs_samps = []
for i in range(0, 6):
    samp = f"Today is {words[i]}, tomorrow is"
    tomorrowIs_samps.append(samp)
answers = [str(i) for i in range(2, 7)]

all_test_prompts.append(tomorrowIs_samps)
all_test_answers.append(answers)

words = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
nextMonthIs_samps = []
for i in range(0, 8):
    samp = f"This month is {words[i]}, next month is"
    nextMonthIs_samps.append(samp)
answers = [str(i) for i in range(2, 10)]

all_test_prompts.append(nextMonthIs_samps)
all_test_answers.append(answers)

words = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve'][::-1]
backw_numword_samps = []
for i in range(0, 8):
    samp = f"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]}"
    backw_numword_samps.append(samp)
answers = [str(i) for i in range(8, 0, -1)]

all_test_prompts.append(backw_numword_samps)
all_test_answers.append(answers)

words = ['uno', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve']
SPnumword_samps = []
for i in range(0, 6):
    samp = f"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]}"
    SPnumword_samps.append(samp)
answers = [str(i) for i in range(5, 10)]

all_test_prompts.append(SPnumword_samps)
all_test_answers.append(answers)

"""# steer by SAE actvs

## get steering vec
"""

words = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
num_sv_samp = []
for i in range(0, 9):
    samp = f"{words[i]}"
    num_sv_samp.append(samp)
inputs = tokenizer(num_sv_samp, return_tensors="pt", padding=True, truncation=True, max_length=300)['input_ids'].to("cuda")
target_act = gather_residual_activations(model_2, 20, inputs)
sae_acts_1 = sae.encode(target_act.to(torch.float32))

mean_num_sv = sae_acts_1.mean(dim=0)
mean_num_sv = mean_num_sv.unsqueeze(0)
mean_num_sv.shape

"""## steer tests"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# SAE_tables, SAE_scores = run_tests_steer_actvs(mean_num_sv, multp_list, 'SAE')

tables = SAE_tables
scores = SAE_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""## nonseq prompts"""

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = mean_num_sv
multp = 3

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_sae_lastTok(prompt, steer_vec, multp)

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = mean_num_sv
multp = 10

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_sae_lastTok(prompt, steer_vec, multp)

"""# steer by LLM actvs

## get steering vec
"""

words = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
num_sv_samp = []
for i in range(0, 9):
    samp = f"{words[i]}"
    num_sv_samp.append(samp)
inputs = tokenizer(num_sv_samp, return_tensors="pt", padding=True, truncation=True, max_length=300)['input_ids'].to("cuda")
target_act = gather_residual_activations(model_2, 20, inputs)
target_act.shape

target_act.shape

LLM_mean_num_sv = target_act.mean(dim=0)
LLM_mean_num_sv = LLM_mean_num_sv.unsqueeze(0)
LLM_mean_num_sv.shape

"""## steer tests"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# LLM_tables, LLM_scores = run_tests_steer_actvs(LLM_mean_num_sv, multp_list, 'LLM')

tables = LLM_tables
scores = LLM_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""## nonseq prompts"""

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = LLM_mean_num_sv
multp = 3

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_LLM_lastTok(prompt, steer_vec, multp)

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = LLM_mean_num_sv
multp = 10

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_LLM_lastTok(prompt, steer_vec, multp)

"""# steer by ablated recon LLM SV

decompose the LLM steering vec

## choose which features to ablate
"""

words = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
num_sv_samp = []
for i in range(0, 9):
    samp = f"{words[i]}"
    num_sv_samp.append(samp)
inputs = tokenizer(num_sv_samp, return_tensors="pt", padding=True, truncation=True, max_length=300)['input_ids'].to("cuda")
target_act = gather_residual_activations(model_2, 20, inputs)

LLM_mean_num_sv = target_act.mean(dim=0)
LLM_mean_num_sv = LLM_mean_num_sv.unsqueeze(0)
LLM_mean_num_sv.shape

# decompose LLM steering vec
sae_LLM_mean_num_sv = sae.encode(LLM_mean_num_sv.to(torch.float32))

feat_k = 100
one_top_acts_values, one_top_acts_indices = sae_LLM_mean_num_sv[0, -1, :].topk(feat_k, dim=-1)

rank = 1
for val, ind in zip(one_top_acts_values, one_top_acts_indices):
    print(f"Rank {rank}", round(val.item(), 2), ind.item(), feat_labels_lst[ind])
    rank += 1

"""These are nonzero:
- Rank 1 150.1 13528 numeric values indicating measurements or quantities
- Rank 2 85.52 11527 the start of a document
- Rank 3 34.94 9768 terms related to control and authority, particularly in political or systemic contexts
- Rank 4 34.91 8684  technical jargon and programming-related terms
- Rank 5 33.51 140  instances of the word "in"
- Rank 6 30.87 833 the numerical values indicating measurements or assessments
- Rank 7 29.39 11174  numeric identifiers or values in a structured format
- Rank 8 21.74 2437  patterns related to numerical information, particularly involving the number four
- Rank 9 21.66 6305 mathematical expressions and notations used in equations and proofs
- Rank 10 15.28 3019  elements related to operational or procedural contexts in a structured format
- Rank 11 10.27 11795 the phrase "The" at the start of sentences
- Rank 12 9.79 745  sequences of numerical values
- Rank 13 9.0 3435  numeric values, particularly related to technology specifications or measurements
- Rank 14 8.34 1344 lists and resources for educational purposes
- Rank 15 7.86 16175 numeric identifiers or codes, particularly in a structured format or specification

Try ablating the following:
- Rank 2 85.52 11527 the start of a document
- Rank 3 34.94 9768 terms related to control and authority, particularly in political or systemic contexts
- Rank 4 34.91 8684  technical jargon and programming-related terms
- Rank 5 33.51 140  instances of the word "in"
- Rank 10 15.28 3019  elements related to operational or procedural contexts in a structured format
- Rank 11 10.27 11795 the phrase "The" at the start of sentences

## ablate then recon to steer
"""

abl_sae_LLM_mean_num_sv = torch.clone(sae_LLM_mean_num_sv)

feats_to_ablate = [11527, 9768, 8684, 140, 3019, 11795]

for fInd in feats_to_ablate:
    abl_sae_LLM_mean_num_sv[:, -1, fInd] = 0

recon_abl_sae_LLM_mean_num_sv = sae.decode(abl_sae_LLM_mean_num_sv)
recon_abl_sae_LLM_mean_num_sv.shape

"""## steer ablated recon in LLM space"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# LLMsp_abl_LLM_tables, LLMsp_abl_LLM_scores = run_tests_steer_actvs(recon_abl_sae_LLM_mean_num_sv, multp_list, 'LLM')

tables = LLMsp_abl_LLM_tables
scores = LLMsp_abl_LLM_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""## steer ablated recon in SAE space"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# SAEsp_abl_LLM_tables, SAEsp_abl_LLM_scores = run_tests_steer_actvs(abl_sae_LLM_mean_num_sv, multp_list, 'SAE')

tables = SAEsp_abl_LLM_tables
scores = SAEsp_abl_LLM_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""# steer by ablated SAE SV

## choose which features to ablate
"""

words = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
num_sv_samp = []
for i in range(0, 9):
    samp = f"{words[i]}"
    num_sv_samp.append(samp)
inputs = tokenizer(num_sv_samp, return_tensors="pt", padding=True, truncation=True, max_length=300)['input_ids'].to("cuda")
target_act = gather_residual_activations(model_2, 20, inputs)
sae_acts_1 = sae.encode(target_act.to(torch.float32))

mean_num_sv = sae_acts_1.mean(dim=0)
mean_num_sv = mean_num_sv.unsqueeze(0)
mean_num_sv.shape

feat_k = 100 # 50
one_top_acts_values, one_top_acts_indices = mean_num_sv[0, -1, :].topk(feat_k, dim=-1)

rank = 1
for val, ind in zip(one_top_acts_values, one_top_acts_indices):
    print(f"Rank {rank}", round(val.item(), 2), ind.item(), feat_labels_lst[ind])
    rank += 1

"""There's too many non-zero features to manually assess (Gemma2-2B uses JumpReLU rather than TopK of say k=16, so we get around 50 non-zero features). Thus, we only look at the top 10 features, which at a glance look to be much higher in the distribution of activations.

Top 10:
- Rank 1 150.1 13528 numeric values indicating measurements or quantities
- Rank 2 85.52 11527 the start of a document
- Rank 3 34.94 9768 terms related to control and authority, particularly in political or systemic contexts
- Rank 4 34.91 8684  technical jargon and programming-related terms
- Rank 5 33.51 140  instances of the word "in"
- Rank 6 30.87 833 the numerical values indicating measurements or assessments
- Rank 7 28.97 11174  numeric identifiers or values in a structured format
- Rank 8 21.66 6305 mathematical expressions and notations used in equations and proofs
- Rank 9 21.37 2437  patterns related to numerical information, particularly involving the number four
- Rank 10 15.08 3019  elements related to operational or procedural contexts in a structured format

We ablate the following:
- Rank 2 85.52 11527 the start of a document
- Rank 3 34.94 9768 terms related to control and authority, particularly in political or systemic contexts
- Rank 4 34.91 8684  technical jargon and programming-related terms
- Rank 5 33.51 140  instances of the word "in"

## ablate then steer
"""

filt_mean_num_sv = torch.clone(mean_num_sv)

feats_to_ablate = [11527, 9768, 8684, 140]

for fInd in feats_to_ablate:
    filt_mean_num_sv[:, -1, fInd] = 0

"""## steer ablated in SAE space"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# abl_SAE_tables, abl_SAE_scores = run_tests_steer_actvs(filt_mean_num_sv, multp_list, 'SAE')

tables = abl_SAE_tables
scores = abl_SAE_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""# compare all SV scores"""

def display_scores(scores_table):
    for i, score_list in enumerate(scores_table):
        score_list = [round(score, 2) for score in score_list]
        print(test_labels_lst[i], score_list)

def avg_score_fn(scores_table):
    total_score = 0
    for score_list in scores_table:
        score = max(score_list)
        total_score += score
    print(round(total_score / len(scores_table), 2))
    return round(total_score / len(scores_table), 2)

def get_best_scores_fn(scores_table):
    best_scores = []
    for score_list in scores_table:
        score = round(max(score_list), 2)
        best_scores.append(score)
    return best_scores

display_scores(SAE_scores)

display_scores(abl_SAE_scores)

display_scores(LLM_scores)

display_scores(LLMsp_abl_LLM_scores)

display_scores(SAEsp_abl_LLM_scores)

"""## avg score bar plots (Fig 1)

We take the max, so that 0 vs 0.2 lower score of 'tomorrow_is, multp 1' in filtered SAE sv doesn't matter
"""

avg_scores = []

avg_scores.append(avg_score_fn(LLM_scores))

avg_scores.append(avg_score_fn(SAE_scores))

avg_scores.append(avg_score_fn(LLMsp_abl_LLM_scores))

avg_scores.append(avg_score_fn(SAEsp_abl_LLM_scores))

avg_scores.append(avg_score_fn(abl_SAE_scores))

avg_scores

custom_labels = ['LLM (SV #1)', 'SAE (SV #2)', 'Decomp LLM \n in LLM space \n (SV #3)', 'Decomp LLM \n in SAE space \n (SV #4)', 'Filtered SAE \n (SV #5)']
colors = ['red', 'blue', 'orange', 'purple', 'lightblue']

num_tasks = len(custom_labels)
task_indices = np.arange(num_tasks)
bar_width = 0.5

fig, ax = plt.subplots()
for i in range(num_tasks):
    ax.bar(task_indices[i], avg_scores[i], width=bar_width, label=custom_labels[i], color=colors[i])

ax.set_ylabel('Average Accuracy Score')
ax.set_xticks(task_indices)
ax.set_xticklabels(custom_labels)
ax.set_xlim(-0.5, num_tasks - 0.5)

plt.show()

"""## bar plot all tasks, all sv (Fig 2)"""

test_labels_lst_2 = ['numwords', 'months', 'days', 'tomorrow_is', 'nextMonth_is', 'descr \n numwords', 'spanish \n numwords']

import numpy as np
import matplotlib.pyplot as plt

colors = ['red', 'blue', 'purple', 'lightblue',] #  'orange'  #cyan

# custom_labels = ['SAE Model', 'LLM Model', 'LLMsp Ablated', 'SAEsp Ablated', 'Ablated SAE']
custom_labels = ['LLM (SV #1)', 'SAE (SV #2)', 'SAE-Decomp LLM \n (SV #4)', 'Filtered SAE \n (SV #5)']

scores = [
    get_best_scores_fn(LLM_scores),
    get_best_scores_fn(SAE_scores),
    # get_best_scores_fn(LLMsp_abl_LLM_scores),
    get_best_scores_fn(SAEsp_abl_LLM_scores),
    get_best_scores_fn(abl_SAE_scores)
]

num_lists = len(scores)
num_tasks = 3  # plot the first 3 tasks

task_indices = np.arange(num_tasks) # position of the groups of bars

bar_width = 0.15 #  width of a single bar

fig, ax = plt.subplots()
for i in range(num_lists):
    ax.bar(task_indices + i * bar_width, [scores[i][j] for j in range(num_tasks)],
           width=bar_width, label=custom_labels[i], color=colors[i])

ax.set_ylabel('Accurary Score')
# ax.set_title()
ax.set_xticks(task_indices + bar_width * (num_lists - 1) / 2)
ax.set_xticklabels([test_labels_lst[i] for i in range(num_tasks)])  # Adjusted for the first three tasks only

# legend outside of the plot
ax.legend(loc='upper left', bbox_to_anchor=(1, 1))

# space out the x-ticks
ax.set_xlim(-0.5, num_tasks - 0.5 + num_lists * bar_width)  # Adjusting x-limits to provide a better spaced view

plt.show()

num_lists = len(scores)
num_tasks = 4  # plot the last 4 tasks

task_indices = np.arange(num_tasks)

bar_width = 0.15

fig, ax = plt.subplots()

for i in range(num_lists):
    ax.bar(task_indices + i * bar_width, [scores[i][j + 3] for j in range(num_tasks)],
           width=bar_width, label=custom_labels[i], color=colors[i])

ax.set_ylabel('Accuracy Score')
# ax.set_title('')
ax.set_xticks(task_indices + bar_width * (num_lists - 1) / 2)
ax.set_xticklabels([test_labels_lst_2[i + 3] for i in range(num_tasks)])  # for the last four tasks only

ax.legend(loc='upper left', bbox_to_anchor=(1, 1))

ax.set_xlim(-0.5, num_tasks - 0.5 + num_lists * bar_width)

plt.show()

"""# sv of nums 1 to 4

## get steering vec
"""

words = ['1', '2', '3', '4']
num_sv_samp = []
for i in range(0, 4):
    samp = f"{words[i]}"
    num_sv_samp.append(samp)
inputs = tokenizer(num_sv_samp, return_tensors="pt", padding=True, truncation=True, max_length=300)['input_ids'].to("cuda")
target_act = gather_residual_activations(model_2, 20, inputs)
sae_acts_1 = sae.encode(target_act.to(torch.float32))

mean_num_sv = sae_acts_1.mean(dim=0)
mean_num_sv = mean_num_sv.unsqueeze(0)
mean_num_sv.shape

"""## steer tests"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# SAE_tables, SAE_scores = run_tests_steer_actvs(mean_num_sv, multp_list, 'SAE')

tables = SAE_tables
scores = SAE_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""## nonseq prompts"""

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = mean_num_sv
multp = 3

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_sae_lastTok(prompt, steer_vec, multp)

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = mean_num_sv
multp = 10

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_sae_lastTok(prompt, steer_vec, multp)

"""# sv of nums 6 to 9

## get steering vec
"""

words = ['6', '7', '8', '9']
num_sv_samp = []
for i in range(0, 4):
    samp = f"{words[i]}"
    num_sv_samp.append(samp)
inputs = tokenizer(num_sv_samp, return_tensors="pt", padding=True, truncation=True, max_length=300)['input_ids'].to("cuda")
target_act = gather_residual_activations(model_2, 20, inputs)
sae_acts_1 = sae.encode(target_act.to(torch.float32))

mean_num_sv = sae_acts_1.mean(dim=0)
mean_num_sv = mean_num_sv.unsqueeze(0)
mean_num_sv.shape

"""## steer tests"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# multp_list = [1, 3]
# SAE_tables, SAE_scores = run_tests_steer_actvs(mean_num_sv, multp_list, 'SAE')

tables = SAE_tables
scores = SAE_scores

test_id = 0

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 1

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 2

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 3

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 4

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 5

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

test_id = 6

for i, df in enumerate(tables[test_id]):
    print("MULTIPLIER: ", multp_list[i])
    print("Acc Score: ", scores[test_id][i])
    print(df) # display(df) ## display is too bulky
    print('\n')

"""## nonseq prompts"""

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = mean_num_sv
multp = 3

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_sae_lastTok(prompt, steer_vec, multp)

nonseq_prompts = [ "aa a a", "I AM A CAT", "Bob and Mary went to store", "my favorite colour is"]
steer_vec = mean_num_sv
multp = 10

for prompt in nonseq_prompts:
    print(prompt)
    steer_by_sae_lastTok(prompt, steer_vec, multp)