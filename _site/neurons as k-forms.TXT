1:

The reason we want study differential geometry for NN is because NN transform non-Eucldiean data (such as manifolds and graphs) into various intermediate objects in latent space, resulting in objects in a final space that can be used for classification, regression, etc. The results depend on what the NN does and doesn’t preserve. One of the things the NN should preserve for manifold data is local data. A manifold is a set (or a space, when viewed from a geometric perspective) of points such that locally each point has a neighborhood that maps to a Euclidean space. In geometric deep learning, we want to preserve the local structure. For example, if a manifold is a 3D body such as the Earth, we want to make sure a local neighborhood of it, such as distances in New York City, will preserve its properties in manifolds in latent space. If we want the NN to do analysis on NYC given map data of the Earth, we need to preserve accurate information about NYC.

These local-preserving deformations on manifolds onto other manifolds in latent space are called isometries. We must make sure the functions (composites of neuron functions) learned by the NN algorithms are isometries, so we have to prove that the algorithms are learning isometries. If we do not prove this, the NN is at risk for not preserving local data. We need the learning algorithms to take into account the relation between local data and the global manifold itself.

<<<
2:

Prove NN are learning isometries. Isometries work on local data between two manifolds. When you make a change in the domain, you must make sure it’s reflected in the range. And know how much- the derivative of a function. Before, it was delta_x shows a change in f(x). But instead of a linear axis, this time it can be on a vector that exists in multiple axes. This is a directional derivative. 

So we need to define how to study calculus on manifolds. We start by applying it and see where it fails. Differential calculus assumes the changes are done on a Euclidean space, but globally the manifold is not guaranteed to be a Euclidean space- if you take two points not in the same local neighborhood and try to use the tools of calculus on them (such as using Euclidean distance formulas), you’ll get wrong answers. So you have to slightly modify calculus so its terms are defined in local space, then relate local results to the entire global manifolds (from local neighborhood to another local neighborhood).

<<<
3:

First distinguish between local tangent space and global manifold space. Tangent space is around a point, and it’s separate from the manifold itself. This is very important. Tangent space is a vector space. Vectors have direction because they’re relative to a base point. On the other hand, there is no base point on a manifold- it’s coordinate free. So a manifold space is a collection of points, while a tangent space is a collection of vectors. The tangent space is not part of the manifold space- it’s an interpretation of the manifold from the relative perspective of one of its points. Every point has its own tangent space. So now we have a Euclidean vector space and can perform calculus in it. 

<<<
4:

The main terms of ch2 are: tangent bundle and section, and directional derivative. First, define functions on a manifold.

1) Functions between manifolds
2a) Functional: Vector space to R ; (also called linear functional)
    Vector to Point
2b) Transformation: Vector space to R^n
    Vector to Vector

No longer are we in 1D calculus where we just go from left to right on a single x-axis, AND no longer are we in multivariable calculus where we has differences on the unit vectors of multiple axes; in vector calculus, there are multiple directions that are BETWEEN axes instead of just on it. These are on vectors. We want to see how the manifold is related to the tangent vector space- this is a bridge between global and local. The tangent vector space (TR) is always a dimensional lower than the manifold’s dimensions. It is isomorphic to a vector space (R) not defined on a manifold; thus, TR is a different object than R. TR is “glued” on top of the manifold whereas R has no relation to the manifold. We don’t want to see TR in isolation; we want to see how it’s related to the manifold.

The bridge between the local and global is called the tangent bundle, which is a collection of all the tangent spaces {TR}, along with the manifold itself. This is how you glue the isolated R spaces onto the manifold, so that now they’re defined as TR. 

A section takes a point from each vector space and creates a vector field, which is a global object since it requires the shape of the entire manifold (all its points) to be defined, instead of being defined within the vector space of a single point. It’s defined using all the tangent spaces but only looks at one vector from each of them. In other words, it assigns a vector to each point. This is like taking a poll of the opinion of each point in a room.

<<<
5:

Without a base point, you cannot have a vector. Make sure your derivative definitions on vectors include this information about ‘base point’. In 1D calculus, there were 2 points on the x-axis needed to define a derivative. In vector calculus, you use the base point and a point in the direction of the vector. Since a vector consists of multiple axis components, you need to define the derivative in terms of how the vector changes from the base point in all components (x, y, etc).

A collection of points does not always form a flat surface because the points can have more than 2 components (called coordinates when in the context of a vector space). Since they can be on different heights relative to each other, they form curves on a manifold.

A functional can map from (x,y) space to z space. This means if you have a 2D tangent space, you can map a point with 2 coordinates onto a height. Map the base point then the tail point of the vector; using a functionals, you can 2 points on the manifold. So a functional relates a local space to a global space. (p45, Fig 2.23)

For instance, using functional f, a base point (0,0) in a vector space R can map to a point p1 in the manifold: f(0,0) = p1. Then a vector (1,3) in R can map to p2 in the manifold: f(1,3) = p2. When R is seen in relation to f, it is referred to as TR. 

However, if you map (x,y) to (p,q), you no longer have a functional because you are mapping between two coordinate systems, or vector spaces. This is a transformation. This range is not a manifold as a manifold may be coordinate free; the z in the manifold is not defined relative to a base point (?) (p35)

<<<
6:

Now that we defined a difference, we define the limit of a derivative by shrinking this difference towards 0. This formula using limit and infinitesimal can be written short-hand using 3 types of notation: Lagrange f’, Leibnitz d/dt, and Netwon y_dot.

The short-hand is useful because we do not have to write out all the coordinates and their infinitesimals; this is very useful when there are many dimensions.

Usually in 1D calculus we have rise / run, where rise is in the 1D y-axis and run is on the x-axis. In vector calculus, the rise is the z-coordinate and run is a vector in (x,y) space. So instead of thinking about run, think about how z points change over any vector, not just the unit vector which is on a single axis. We need to go to a linear combination of points on axes. Run used to be a distance on the x-axis, but now we need the length of a vector relative to a base point.

<<<
7:

(p50) We want to show how vectors can map to partial differential operators. To do so, we must rewrite the directional derivative as an “operator”. Then we will map components of the operator (the partial differential operators) to components of the vector (the unit vectors). This is a central idea about duality: how object (vectors) can be seen as relations (functions/operators). This leads to a fractal-like propagation where functions beget higher-order functions.

First, let’s review the definition of directional derivative so far: In the context of derivatives, we want to see how a function changes relative to a change in the input space. For directional derivatives, we want to see how a function (such as a functional) changes relative to a change from a base point to the direction of a vector. 

An operator is a function that takes other functions as input. We will associate the vector with an operator and the function with the input to the operator. This means “what interpretation do you get when you apply the vector onto the function.”

When defining a directional derivative in terms of operators, we don’t do the other way around (calling the function f the ‘operator’) because the function f already takes in a vector input and does not output a derivative, but a point; thus, we cannot have it mean multiple things at once. So we define a NEW function- a tangent vector- onto f again to get a derivative interpretation.

An important idea of this association is that a vector has components, and so does an operator. When you apply an operator to a function, both the operator and the function has 2 components. Using vector/matrix notation, we see how each coordinate interacts with other coordinates. The matrix notation of an operator has partial derivatives as components, whereas the vector’s components are the Euclidean unit vectors. Thus, we come to the central idea: object (vectors) can be seen as relations (functions/operators).

Intuitively, it is obvious that there is a connection between direction (vectors) and derivatives (change in a direction). In linear algebra, a vector is a linear combination of unit vectors. With this analogy between vector and operator, the directional derivative is a linear combination of partial differential operators. So this analogy allows you to carry over tools from linear algebra to be applied to directional derivatives. (p52, Fig 2.27)

<<<
8:

Now you can define vector calculus definitions in terms of theorems and objects from linear algebra. Linear operators in linear algebra have properties such as distributive property (p51). You can use these identities to derive Leibnitz rule, which can be used to further prove other identities are true.

From a neural network perspective, each vector is an object, and the unit vectors are features of that object. For instance, if each point in some space represents a house, an axis in this space can represent the feature of a window, and another can represent a door. The linear combination of features- including window and door- create a house.

In machine learning, we can manually tell an algorithm what are the features to use to define an object. But for neural networks, the algorithm learns by itself what features to use. Each neuron learns to recognize certain features. So it does not always decide to use human-intuitive features such as window or door. Other times, a neuron may not learn about a “window” in general, but specializes to recognize “left window”; then, a “left window” neuron and a “right window” neuron both tell their decisions to a “window” neuron, which takes multiple sources into account to make its decision (in this way, a neuron can apply causal, analogical reasoning based on its interactions with other neurons). A neuron may also be polysemantic in recognizing vastly different features- for instance, a neuron may recognize “left window” and “bear”, which have little to do with each other and don’t share many abstract similarities compared to other features recognized by other neurons such as “right window” or “deer”. Additionally, what a neuron specializes in may not be a feature captured by one word in our language (such as “a red cat next to a window”- this is so specific that our language decides not to turn it into a meme as it’s not general enough to be efficiently applied to many different cases), or it recognizes features that have absolutely NO human-intuitive interpretation using human-intuitive features (such as red cat, or window) as building blocks (Eg. it recognizes pixels of range (4 to 60) and (250 to 500) in locations that are next to pixels of value 3, stronger when the pixels are on the right-side of an image, if the CNN is able to identify directions in an image by combining multiple location-agnostic neurons that locally don’t care about location due to convolution but when interacting in a group, their role is sent to a neuron that labels all incoming inputs from the previous layer as “right-side”).

Now these neurons- which learn composable features- are all unit vectors for another neuron because they are inputs for that neuron. Since all neurons are functions, each neuron takes as input a function. Thus, neurons are operators. The duality we have shown for directional derivatives appears again.

<<<
9:

This duality also appears in how cells are objects whose interactions with other cells become an object itself (an organism). It’s also how the organisms are objects whose interactions become an object (a society). When a cell that takes in information from another cell, the two cells and their interaction is an operator function taking in another function; it’s function composability. This interaction itself is an object- another vector, or function. And thus, it can be used as input to a third function.

<<<
10:

(p53) A differential k-form is a linear functional that takes in k tangent vectors to the manifold. The directional derivative vf takes in two parts: an operator v and a linear functional f. That f is a differential k-form because it takes in vectors and outputs a point on the manifold.

Since the unit vectors were a basis of a tangent space, now the pd operators are a basis too.

<<<
11:

In the NN analogy, the neuron is a k-form that acts on the original data manifold, say the distribution of all houses using each pixel as an axis. Since a vector is a neuron, which measures features, the vector space on a point consists of all the neurons interpreting that point. A vector in this space is a linear combination of neurons- that is, an interpretation that takes into account multiple perspectives.

For example, say we have an image of a house that is a point in the house data manifold. Say there are 3 neurons in the first layer that each output a scalar value given the pixel values (coordinates) of that point. They cannot see the entire data manifold, but they see a vector in a tangent space where the base point is the point of all 0 pixels (blank image) and the tail end of the vector is the house data point. One neuron outputs a value 5, another a value 6, and another a value 2. 

In the NN interpretability model we’re constructing, we’ll construct a latent space in terms of these neurons that will be useful for interpreting what directional derivatives mean in these spaces, and will beget insightful interpretations.

Let these values appear in a new latent space, consisting of the 3 neurons as basis vectors. It is a local tangent space on the global latent manifold. The NN has transformed the entire data manifold into a new latent manifold, but so far in our description it is NOT one cohesive function- there are 3 separate functions that send the house data point to 3 different points in 3 different unit (basis) vectors. 

The base point of this latent space, for the first layer, consists of coordinates that are all input dimensions for all basis vectors in the latent space. Given a latent space with 2 neurons, if neuron 1 (n1) has inputs (x,y) and neuron 2 (n2) has inputs (y,z), the base point will have coordinates (x,y,z) in the input space, and coordinates (0,0) in the latent space. Each neuron vector is linearly independent from the other because they contain a single value within the coordinates; the vector corresponding to neuron 1 will always have value 0 in the 2nd coordinate. Thus we construct a latent space such that neuron x is defined as a unit vector with value 1 in coordinate x and 0 in every other coordinate.

Thus, a1*n1 + a2*n2 is a vector within this latent space, where a_i are the output values of each neuron. So 5n1 + 6n2 + 2n3 is a vector within this latent space.

A modeling mistake that can be made here is to say that 5w1 + 6w2 + 2w3 is a vector within this latent space. This is not so because we have defined the basis for this latent space using neurons, so the weight vectors are not basis vectors. These modeling mistakes occur due to how superficially similar 5w1 + 6w2 + 2w3 looks to a linear combination; we have scalars and variables. But once you follow the logic of the assumed statement, you realize it has contradictions- the weight variables are not vectors within the latent space, so this cannot be a vector within it. It fails the analogical reasoning condition tests.

When a neuron in the 2nd layer takes this vector V as input, it uses the function A(VW+b). Thus, this 2nd layer neuron is a 1-form because the linear combination of the 3 basis vectors forms one vector. 

Without the 2nd layer neuron, each of these observations do not interact with each other- but with it, the 3 layers are used in a logical algorithm to calculate a decision. An analogy is if the 2nd layer neuron listens to the accounts of 3 people. Say this 2nd layer neuron’s job for neurons it outputs to is to decide if there is a window in the house data point. The neuron connected to it by w1 measures windowsills, which are heavily important features for determining if there is a window, so w1 is big. The w2 neuron measures cats, which sometimes are associated with windows but not often, so it uses the w2 “feature” somewhat but not too much- not as much as w1. 

It could also be that this neuron does not determine windows directly, but is part of a long chain of reasoning where it processes the interaction of w1 and w2. It can act similar to a logical gate- if w1 and w2 are both high or both low, this 2nd layer neuron is also high, but if one is high and the other is low, this 2nd layer neuron is low. This 2nd layer neuron then passes this interaction of w1 and w2 to more neurons until a “window” decision is reached.

The neurons in a filter of a CNN consequently mean there is a neuron in the next layer for each patch the filter goes over. That is, if the filter goes over 10 patches, there are 10 neurons each with different inputs, but all these neurons use the same weights. Instead of using all the pixels of the house data point as input, each of these next layer neurons only uses a subset of pixels, some of which may overlap depending on the stride size.

Now that we have defined a neuron as a 1-form, what is a directional derivative? It is an interpretation of what that neuron N recognizes in terms of either a basis vector (an input neuron of N) or a linear combination of basis vectors (a weighted combination of N’s input neurons). The basis vector can be viewed as a feature that’s a component of the linear combination of basis vectors, while the combination can be viewed a combo of features. In both cases, slightly nudging an input vector would slightly nudge the neuron’s output value in its output latent space. We have seen a sort of “continuum” or a spectrum of clustered values in latent spaces, such as in StyleGAN editing. There is input editing (choosing a similar vector within latent space while keeping the weights the same) and weight editing (changing the NN itself to have it give slightly different outputs for the same input).

For instance, if an input vector (1,2,4) makes neuron N output “dark red”, a similar input vector (1,2,40) may have neuron N output “light red” instead. One can then hypothesize that the third basis vector captures brightness. Then, we can use directional derivatives to get more precise measurements on how the output would change given the amount we nudge (1,2,4) in the “higher” direction of the third neuron. This more precise measurement on how changing input will change output is useful for calculating which input will get us a desired output. 

Not only that, but we can calculate how changing inputs to a neuron changes a neuron’s output within the intermediate layers. How does changing the values of the third neuron change the output of a neuron we hypothesize has learned to recognize red cats? What about to another neuron that recognizes red pots? Is there a trade-off? This is useful for neuron circuit interpretability.

Additionally, we can use an algorithm that takes these latent change measurements into account to update neuron weights. Weights are already updated through measuring partial derivatives (which make up the gradient vector field) via backpropagation, but the gradient descent update algorithm does not take into account interpreting the neurons to try to figure out how they relate to some manually set goal in Style Editing.

<<<
12:

Neurons interpret other neurons. The linear functional is an interpretation. They send points from one latent space of manifold A to manifold B, and since each point on a manifold as its own tangent space, this “activation value tuple”, where each component is an activation value from a neuron of the previous layer, has its own tangent space. Each data point is mapped to only one other data point in the new latent manifold space; a manifold is a collection of data points (observations). 

<<<
13: 

The tangent space of a data point is how neurons of layer L interpret the local space of a manifold. All neurons only work on local data; they cannot see the big, global picture. This vector space is a set of all interpretation vectors using layer L neurons as a basis. Thus, it includes more than just independent basis neurons; it includes linear combinations of them.

A dual space is a set of all linear functionals on a tangent space. That is, given a data point’s tangent space, the dual space is the set of all layer L+1 neurons interpreting layer L’s interpretations of that data point. It is a set of all neurons acting on layer L interpretation vectors, but does not include combinations of layer L+1 vectors.

Each vector space has its own dual space. A vector space is a set of all inputs to an element in a dual space. This is where duality comes in; on one hand, you have a set of objects (the vector space), and this always has a corresponding set of functions that output scalars which together make up a vector in a new tangent space. Thus, this forms a self propagating chain of tangent spaces. While a neuron is itself a function of a vector of functions, it is sent to a vector of functions too. It becomes part of a collective- the vector. They become part of the basis for another tangent space. This forms a recursive pattern.

The neurons only exist in relation to other neurons. If we change the values of a single neuron, how does it change the emergent society around it? This can be used to construct an analogical model used to reveal insights about cooperation in organisms. The more analogical tests the model passes, the stronger it is; but if contradictions in analogical reasoning are found, and if those contradictions make heavily-relied-upon statements (meaning, they’re used in a lot of proofs) uncertain, then the model would seem more likely to be based on superficial analogies (confusing the map for the territory) instead of structural analogies.

<<<
14:

Tangent space and dual space are very general and thus can be used in many models about agent interaction. Manifolds are more specific, as they require an object that is locally Euclidean near every point. A k-form field (or a co-vector vector, which is dual to a vector field) takes a particular vector at each point- a particular linear combination of neurons at each point- and gives an interpretation at each point. This is similar to how each input data point to a neural network outputs a particular value. Thus, the neural network is a k-form field, as we are choosing which particular linear functional composite to use on the local neighborhoods of some data manifold. 

A vector field chooses a vector at each point; so for each data point, you can choose what linear combination of neurons interprets it the highest. An activation maximization algorithm chooses an input which gives the highest activation value for a particular neuron; it is also similar to a vector field.

<<<
15:

(p59) There’s 2 ways to visualize a k-form (a neuron): they can be objects themselves in the dual space set, or they can functions on the corresponding vector space. A neuron “starts off” as a function in a vector space but becomes its own object in the dual space. This is similar to how the interaction between cellular organisms “starts off” as just interactions but emergence makes the society becomes its own organism, which can interact with organisms on its own level to form even higher-level organisms (?)

The components of the neuron function project each component of the collective vector onto an activation value on a basis vector of the dual space. “A function on a vector goes into a vector of functions, a function on a vector goes into a vector of functions, and so forth”.