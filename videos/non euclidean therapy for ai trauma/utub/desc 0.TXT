1. NOTE: Before watching the video, read the Errata (section 4) below or in the pinned comment for a few important corrections in the explanation.

PATIENT ALICE: An Artificial Intelligence suffering from hallucinations of a lost puppet show. These hallucinations need to be erased.

GENERATIVE MODEL TYPE: Diffusion-based.

PRESCRIBED TREATMENT: A Latent Space Editing method that involves the Pullback, the Jacobian Matrix, Eigenfaces and SVD.

-------------------------------------------
2. This video experimented with a different approach, combining a mystery story told by analog VHS effects with an explanation of a very recent paper about latent space editing in diffusion models. This is my only entry to #some3 

Link to the paper this was based on:
"Unsupervised Discovery of Semantic Latent Directions in Diffusion Models"
https://arxiv.org/abs/2302.12469

-------------------------------------------
3. The following are chapter timestamps. They can be used to skip to math-only or non-math sections.

Timestamps: [spoilers ahead]
00:00 - Patient Introduction -- [No Math]
02:08 - Manifolds and Pushforwards
06:38 - The Three Functions
09:55 - The Lost Show -- [No Math]
12:32 - Diffusion Models and the U-Net
14:13 - Matrix Multiplication and the Change of Basis Neurons
20:01 - The Jacobian Matrix
26:17 - The Pullback and the Dot Product
28:42 - A treat before treatment -- [No Math]
31:09 - The Treatment -- [No Math]
36:13 - Finding the Error 
39:31 - Correlations in Matrices
42:13 - Superposition
45:23 - W^T W
47:24 - Eigenvectors of W^T W
49:38 - The Trauma -- [No Math]
51:43 - Singular Value Decomposition
54:30 - Reunion -- [No Math]

Links to cut versions with only the math portions and only the no math portions may be made in the future.

-------------------------------------------
4. ERRATA

4.1. This video refers to needing to find singular vectors to "compute the Jacobian" but that's poorly worded; it actually means "for multiplying the Jacobian with input vectors to compute the Jacobian output" and other operations using J. The Jacobian matrix, as stated in the paper, is computed via "a sum-pooled feature map of the bottleneck representation of H" to reduce the number of parameters to compute.

4.2. This video states that the SVD is used to compute both the Jacobian and the semantic directions. But the SVD is actually only used to obtain the semantic directions. To compute the SVD, in section 3.2 of a subsequent paper, the Power Method is used to compute the singular vectors without computing (M^T)(M). An updated version will fix this by stating that "dimensionality reduction" should be performed on J before multiplying with it; this was not done in the paper, so the video will take a different approach than it for the sake of explaining dimensionality reduction. This would fix the "circular issue" of needing the Jacobian J to compute J^T J to get the eigenvectors used to compute J.

Link to this paper "Understanding the Latent Space of Diffusion Models
through the Lens of Riemannian Geometry":
https://arxiv.org/abs/2307.12868

4.3. Other Fixes (for the future update):
37m35s: This could be misleading; singular vectors are neglected, not dropped, so the matrix would have the same dim (see: https://stats.stackexchange.com/questions/107533/how-to-use-svd-for-dimensionality-reduction-to-reduce-the-number-of-columns-fea)
46m: this transpose should switch the elements to row a c and row b d
53m20s: this should say "square root of eigenvalues"
55m55s: this is an audio glitch. it should be 'you weren't such a'

In the future, an updated version with improved narration and visuals (such as for the patient voice during the latter half) may be uploaded. This video was not re-uploaded with fixes due to 8/18 being the deadline for #SoME3.

---
A behind-the-scenes video going into more depth about the paper this was based off of may be made soon. It will address some issues not explained (due to time) in the video.

---------------------------------
5. Helpful Resources:
https://www.youtube.com/watch?v=vSczTbgc8Rc&t=673s&ab_channel=VisualKernel
SVD Visualized, Singular Value Decomposition explained

https://www.youtube.com/watch?v=OzeDqsVoTFc
The Power Method

--
Prequels / supplementary videos to this story:

https://www.youtube.com/watch?v=DHjwbleAgPQ
Why do Neural Networks use Linear Algebra? || The Visual Intuition of Cat Mathematics

https://www.youtube.com/watch?v=xI7tAjoe4oc&t=174s
THE AI AMONG US in your Non-Euclidean Mind 〘 Analog VHS Infomerical 〙

---
More References:
https://transformer-circuits.pub/2022/toy_model/index.html

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
analog horror, vhs, liminal, backrooms, some3, some4, 3blue1brown, education, chatgpt, ai safety, neural networks, machine learning, math, matrix, diffusion

<<<
This is a prequel / supplementary video to the #SoME3 entry, Non-Euclidean Therapy for AI Trauma:
https://youtu.be/FQ9l4v7zB3I

<<<
Around 22m of no math

<<<
post on discord share:
This video experimented with a different approach, combining a mystery story told by analog VHS effects with an explanation of the Jacobian Matrix, Eigenfaces, and SVD. It's based on a very recent paper about latent space editing in diffusion models. 

<<<
Pinned comment:

ERRATA (important to read)

1. This video refers to needing to find singular vectors to "compute the Jacobian" but that's poorly worded; it actually means "for multiplying the Jacobian with input vectors to compute the Jacobian output" and other operations using J. The Jacobian matrix, as stated in the paper, is computed via "a sum-pooled feature map of the bottleneck representation of H" to reduce the number of parameters to compute.

2. This video states that the SVD is used to compute both the Jacobian and the semantic directions. But the SVD is actually only used to obtain the semantic directions. To compute the SVD, in section 3.2 of a subsequent paper, the Power Method is used to compute the singular vectors without computing (M^T)(M). An updated version will fix this by stating that "dimensionality reduction" should be performed on J before multiplying with it; this was not done in the paper, so the video will take a different approach than it for the sake of explaining dimensionality reduction. This would fix the "circular issue" of needing the Jacobian J to compute J^T J to get the eigenvectors used to compute J.

Link to this paper "Understanding the Latent Space of Diffusion Models
through the Lens of Riemannian Geometry":
https://arxiv.org/abs/2307.12868

>next reply

In the future, an updated version with improved narration and visuals (such as for the patient voice during the latter half) may be uploaded. This video was not re-uploaded with fixes due to 8/18 being the deadline for SoME3.

A behind-the-scenes video going into more depth about the paper this was based off of may be made soon. It will address some issues not explained (due to time) in the video.

>next reply (do this so it can be edited later on)

[This comment is separated so it can be edited later if more needs to be added]
3. Other Fixes (for the future update):
37m35s: This could be misleading; singular vectors are neglected, not dropped, so the matrix would have the same dim (see: https://stats.stackexchange.com/questions/107533/how-to-use-svd-for-dimensionality-reduction-to-reduce-the-number-of-columns-fea)
46m: this transpose should switch the elements to row a c and row b d
53m20s : this should say "square root of eigenvalues"
55m55s : this is an audio glitch. it should be 'you weren't such a'
