SCENE 4:

So, what does this visual we've just seen have to do with a neural network? The answer actually has been hidden in front of our eyes this entire time. 

Let's analogously map multiplication into matrix multiplication by putting our terms in brackets. 

Then we'll add 0 to the left side, and also the Identity function, which doesn't change anything. Finally, we'll represent output values using a variable.

This is the exact same equation as before, just written in a different form.

Let's compare it to a neuron equation, which is what a neural network uses to compute a neuron activation. All the variables are matrices. The sigma is an activation function, which in our case is just the identity, and b is the bias, which we have as 0. So our equation is just a very simple neuron. 
    Ïƒ(WX+b) = A appears below

Now, we see that our unit conversion visual has been hiding a neural network neuron all along.

<<<
W contains weight connections that the X inputs are multiplied with, and A contains the neuron activations, which in our case, is the nap neuron. 

In fact, one can think of this nap neuron as just a single layer neural network with one neuron.
    show matrices

Or with two neurons, if you think of the input as a nose neuron represented by the equation 1*x.
    fade in connection of same input through 1* input to nose neuron

<<<<
So a neural network can be viewed as a function that's composed of many smaller functions. Note that it can't approximate every function, but different types of neural networks can approximate a wide variety of functions that are useful for prediction.

Of course, a neural network is more than just this equation, so rather than saying this equation is a neural network, we can say it's analogous like a neural network.

But what allows large neural networks to do complex tasks that a simple neural network can't do? Let's see what happens when we add more connections to our network, starting by just adding one more.

