Scene 1:

[ Just start it , no logo b/c hard to make interesting one for now.?]

[music plays first 3 secs]
    starts w/ addition of concepts through neural newtork to form cat at 3 secs, just when Wind in Rushes music 'opens up' with a synth chime

When studying new improvements to neural networks, many people run into the following problem:
    start w/ scrolling through a paper, blurring it and making concepts from paper appear bigger like news report
    NN anim runs in subset of screen?

How do these unfamiliar mathematical concepts relate to neural networks?
    fade in appear on screen in italics

Words such as eigendecomposition, tangent bundle, and more are often encountered, but someone without a background in math may be wondering what's the intuition behind these terms. In this series, we'll be diving into the key insights behind how all these concepts work.
    bold each word while fading rest of sentence. two sentences fade in. question mark appears in center.

    'in this series': you should have a placeholder.

Let's start with the most basic concept: matrix multiplication. Many helpful explanations have made been about multiplying weight matrices with inputs. But why does that specific algebraic procedure work? Why does its first step use the dot product of the first row and the input vector to perform a 'change in basis'? And what do dot products have to do with how a neural network recognizes concepts?
    replay 3Blue1Brown’s vid on NNs

Knowing why this procedure is done is key to a visual intuition behind neural network interpretability, showing how neural networks compose together complex, higher level patterns using smaller patterns. 
    show Latent space addition

You might have heard that neural networks are able to add together concepts to make analogies, such as composing together the concepts of "King minus Man plus Woman to equal Queen"; during this series, we'll be explaining how that works. As we get to more and more videos, we'll also understand how Neural Networks perform abstractions such as Style Editing, discussing techniques like InterFaceGAN.
    King – Man + Woman = Queen (not geometrically?)

    https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/

The only prerequesite before watching this video is a high level understanding of what neural networks are, such as knowing that they're used for classifying images. The recommended, optional prereqs are to watch 3Blue1Brown’s Essence of Linear Algebra, namely just the five videos 1 to 3, then 9 and 13, as we'll be reviewing a lot of those concepts, but from a different perspective.
    link to 3b1br, etc. show 4 of these videos playing at once (13 in center), and link to them in description.

[wind in rushes plays to silence, or fades out to silence for a moment]

Ok, let's begin.

<<<
SCENE 2.1:
To show how neural networks use matrix multiplication, let’s start with an classification example where in the future, an artificial intelligence has been creating its own cats. These artifical, fake cats, are very hard to distinguish from natural, real cats, because they look nearly identical.
    AI factory 
    https://dribbble.com/shots/4841254-Production
    text fades in 'real', 'fake' next to each one

    https://replicate.com/stability-ai/stable-diffusion-img2img
    https://huggingface.co/spaces/huggingface-projects/diffuse-the-rest
    make 6-8 cats. show them very small on screen so ppl can't discren the dtails

    https://www.youtube.com/watch?v=ors0wpcVDcc&t=1854s&ab_channel=UnderratedAlbums
    https://open.spotify.com/playlist/11IcIUefRdjIpy1K5GMdOH
    Psychedelic Space Rock

    https://legismusic.com/find-royalty-free-music-that-sounds-like/

Our job is to identify the fake cats. By looking at patterns that can tell apart a group of cats already known to be real or fake, 
    separate a circled subset of 'known' population of 4 cats and put them into 2 categories. have an ambiguous one in each one that looks similar, only very subtle difference.

evidence suggests that the only way to distinguish them is by using a set of rules calculated using face length and body size. That is, fake cats tend to have shorter faces and bigger bodies compared to real cats.
    fade in face length and body size shapes

    shape on two images

But in some cases, even real cats have short faces and big bodies, yet don't qualify as fake. So it's hard to tell them apart by sight, and we need a more exact calculation to figure things out.
    shape on two more imgs

We're going to have to find these rules using a neural network.

Let's say a new cat just wandered in, and we don't know if it's real or fake. How can we find out?
    shift fade into population

We'll start by measuring this cat as a data sample using face length, and body size.
    expand cat to center (transform)
    fade in words in openshot: note that the height of the face length box doesn't matter, since it's just length, 

So every real cat or fake cat in our population is represented in the dataset as an data sample
    'cat' shifts copy of cat to left
    'data sample' shifts copy of data pt to right. words drop to bottom.

<<<
SCENE 3.1:
Each of these features can be measured using a basic measuring unit, such as a unit 1 face length, or a unit 1 body size. Think of these basic measuring units as just like using 1 meter to measure distance, or using 1 second to measure time.
    fade out the cat and data pt from scene 2
    separate face and body from the data pt of cat; face shifts to top, body shifts to bottom. make the 1's fade in after each shift.

We can grow or shrink basic measuring units to get different measurements. For example, we can make the unit 1 body twice as big to get the unit 2 body, or cut the unit 1 face in half to get the unit 0.5 face.
    left side is unit 1
    fade in unit 2 body, and unit 1 face

    growing animation in manim?

<<<
SCENE 3.2:

[Do not take zoomed in snippet. Post LHS table on top of grid.]

[ create the shapes in manim to grow/shrink. the issue is that the original shape needed to fit around your cat image. just trace them with gradual adjustments over the original. i don't think viewers would notice, though. ]

3.2.1:
The data samples in this dataset can be represented as points in a coordinate space, using face length and body size as axes. Note that each data point here corresponds to a data sample of a basic measuring unit.
    body size on y-axis, face length on x-axis
    D:\Documents\_prog\prog_cust\manim\video 1 
        \ 3.2.py

    Face length and body size vectors, then points, then fade in data samples w/ 1's in them

3.2.2:
We can add together the unit 1 face length and the unit 1 body size to form a data sample with a unit 1 face length and a unit 1 body size.
Notice that this is the same as adding the data points (1,0), and (0,1) to form (1,1). We can represent these data points as vectors, and show that adding these data sample images together is the same as vector addition. We'll represent our vectors using 1-dimensional matrices.
    Quickly move copies (or the original, so it doesn't take up too much space) of (1,0) and (0,1) into (1,1)
    Also move copies to top right so they're used as addition.??

    Addition at top right: add them, then after move them to overlap, fade in equals sign aligned w/ expression.
    Then put in data pts, move to (1,1), and eqns using vectors move out from img eqns to above.

    fade out prev eqns

3.2.3:
It's easy to see how we can add together face lengths and body sizes of different units. Let's take a face length of 0.5 units, and a body size of 2 units, and add them together. To model this using vectors, we multiply 0.5 by [1, 0], and 2 by [0, 1], and then add them together to get [0.5, 2].
    Scale face length down, then body size up, by moving data pt to become new pt, and move those two into (0.5, 2) such that they overlap. Same eqns. For vectors, start with 0.5 * [1,0], then move scalar inside and at same time, move 0.5 into img (layer behind) to shrink it.

[to prevent eqns from being seen as 'on grid', have a box around them w/ no grids and may have diff color inside]

[compare using openshot to make static grid and move imgs, vs using manim to move imgs on coord you can't zoom into]

These basic measuring units are labeled by the vectors [1, 0] and [0, 1], which are called basis vectors. In other words, these units are used as the basic building blocks to measure features, which in turn, measure data points. They're like an alphabet used to form words in a language.
    re-play the basis vector animations w/ labels
    write out words 'basis vectors' on screen

<<<
SCENE 4:

Now, there are other ways we can measure the data samples in this population. Instead of labeling each data sample using the face & body measurements, let’s label each data sample using the following two measurements:

1) How likely it is to be a Real Cat
2) How likely it is to be a Fake Cat
    fade in each line as words

How do we find the values of these new, currently unknown, measurements? We can calculate them using our previous known measurements, face & body. In general:

Shorter face + Bigger body = Real Cat
Longer face + Smaller body = Fake Cat
    fade in each line as words

We can also represent this using variables
    facecat (cat's face) + bodycat∗(cat's body)

And plug in the values of our cat 
    cat data pt, vector, fades into right
        transform above eqn into:
    facecat.5+bodycat∗2 

Then show this how this looks visually:
    the value 0.5(face_cat)+2(body_cat) on the cat axes
    [2D to 1D cat vector animation, see ch2]

What are facecat and bodycat? These are how much each feature is weighted by to calculate the score of “likely to be cat”. The higher the weight, the more that feature is taken into account during calculation. 
    transform weight to be higher, making output vector longer

For example, it might be more important to know the body size than the face length when determining if something is a real or fake cat. Then we'd set bodycat as 5, and only set facecat as 2. 
    body size word grows, face length shrinks, and > fades in at same time
    make each part of eqn stand out when referring to it

We can also use negative weights. These mean that the higher a feature's value is, the less likely the cat is to be real. Having a longer face is a penalty against being a real cat.
    higher x, lower cat

<<<<<<<<<<<<<<<<<<
SCENE 5:

Notice how this equation resembles the following dot product:
[facecat bodycat]⋅[0.5 2]=facecat∗0.5+bodycat∗2

Not only that, but it resembles the first step of matrix multiplication:
[facecat ? bodycat ?][0.5 2]=[facecat∗0.5+bodycat∗2?]

Let's set facecat as 4, and ignore the second term bodycat for now.
    transform bodycat into ?

for every face length of unit 0.5, there are 2 units of cat
    0.5 units of face length → 2 units of 'likely to be cat'

Or in other words, “for every face length of unit 1, there are 4 units of cat”. Thus, for half a unit of face length, we have half of the proportionate amount of cat, which is 2. Doesn’t this sound familiar, like unit conversion?

For every 1 meter, there are 3.28 feet. So if an person is 2 meters long:

2 units of meter → 6.56 units of feet

Now, instead of using the basic measuring unit of 1 meter to measure a person, we are using the basic measuring unit of 1 foot to measure them. Before, 1 meter acted as a "basic vector", and now 1 foot acts as our basis vector. This unit conversion, or "Change of Units", is called a "Change of Basis".

One dot product step is analogous to 1D matrix multiplication; so two dot product steps would be analogous to 2D matrix multiplication. This answers the question we posed in the beginning of our video: the dot product is used to convert multiple units into a new unit.

Instead of just using one measurement like in our “meter to feet” example, the “face and body to cat” example uses two measurements to find the value of cat. Matrix multiplication allows for a change of multiple units, or multiple dimensions. The weight matrix W is analogous to the conversion factor.
    show words: meter -> feet

Now if we just use one row in our matrix, we get the equation to determine real cats. If we use two rows, we get the equation to also find fake cats.
    second row fades in:
    W=[facecatfaceRat bodycatbodyRat]

We keep track of both real cat and fake cat measurements because something can be neither a real nor a fake cat. Our neural network can detect something which has a short face and a small body, which is not even a cat, but a rat that snuck into our population.

don't put in:
(We can check this by actually looking into the intermediate activations of our model, instead blindly trusting its outputs which just say 'which is more likely', even if both are unlikely and one is just slightly more likely than another.)

<<<<<<<<<<<<<<<<<<
SCENE 6:

With both the real cat equation and the fake cat equation, we have a system of equations. This matrix would measure the real cats on the x-axis, and the fake cats on the y-axis. 
    On right coordinate, real cats appears on x, etc

Let's say someone gives us the matrix values that allow us to accurately calculate "likely to be real cat or fake cat". We'll go over this matrix multiplication step by step soon in one of our next videos.

Since each coordinate space provides a different way to represent the data, let’s call each coordinate space a Model, namely Model 1 and Model 2. These two measurement methods are actually measuring the same data samples; the data samples in Model 1 are present in Model 2, but are now measured by different vectors.
    compare 2 coordinate spaces side by side, fading in each new image.

<<<<<<<<<<<<<<<<<<<<<<<<<
(put in 3rd video?):
Notice that if we animate the change of basis, it seems like there isn't much of a difference between the two Models. But there is. 
    [animation]

We can use a crude way to predict if the cat is real or fake based on if it has a greater value on the x-axis, real, or on the y-axis, fake. If we did not perform a change of basis, that is, not using the learned classification rules, we'd have to rely on unreliable methods, such as only using body size to say bigger cats are fake (which is not always right). 
    only using body size (show is bad classification)

Again, this is just a crude approach, as our output values aren't even probabilities; this can be remedied by passing them through a function to turn them into probabilities. Additionally, this is a very simple change of basis, used for teaching purposes; change of basis can get very complex, especially when composing multiple matrices together.

<<<
(put into video 3?):
Another thing to note here is that the concepts sent to the new basis vectors have an interpretation in the first Model. That is, the cat with [values] was found to be so often fake in our dataset, that it's sent to (0,1), which means samples with those values are almost always not ever a real cat, and almost always a fake cat.
    see vector values for cats.TXT

But although we can show the real cat and fake cat 'basis vectors' in Model 1 in this example, keep in mind that matrices are not always invertible. We'll discuss this more in a later video.

(also interpretation is not always req. a neg value may just mean 'so close to 0', and is just mean to throw samples a certain distance, not meant to be interpreted)

<<<<<<<<<<<<
SCENE 7:

So what does Change of Basis have to do with Neural Networks? You might have noticed that changing from Model 1 to Model 2 demonstrates an idealized, simplified example of what a neural network does- making guesses about what an input data point is. In fact, one can think of this matrix as a single layer ‘neural network’ such that for the function that calculates the neuron activations:
O=σ(WX+b)
    instead of entire NN, show simple input and output. X on left, goes into equation, outputs O on right. show both I/O and eqn. I/O has circles has neurons, eqn is above their conn arrow. have pulsating animation where x lights up, then arrow lights up, then N (or A) lights up.

Which, for a data sample, outputs the values it guesses for the 2 classes {cat, rat}, it sets σ=I, the identity function, and b = 0:
O=WX
    substitute those terms in as transform, then make them fade out and have others shift to new pos

We can see how this matrix relates to weights in a neural network; each column corresponds to outgoing weights of a previous layer neuron, and each row corresponds to incoming weights of a next layer neuron:
    make a new NN using your matrix. two input nodes, two weights. add each node at a time and show the corresponding math rep (weight conn adds weight in matrix). transform weight variables into values; stronger connection make weights bolder, negatives make weights a diff color (red shade) vs blue or green

    'we can add a third feature to have 3 input neurons, which in turn, would create two more weights w3_1 and w3_2'. fades in then out.

    make using 3b1br anim template

As we see that each of the two neurons on the left act as basis vectors in the previous layer (Model 1), and the two neurons on the right act as basis vectors in the next layer (Model 2), such that the two neurons on the right are a linear combinations of the previous layer neurons and their weights, we come to a very important concept:
    show Model 1 and 2 w/ neurons arrowing out

Neurons are Basis Vectors in an Activation Space.
    write out on screen

Thus, every neuron in a neural network is a measurement on the data, coming up with an interpretation for it from its own perspective. (Think of this as taking a slice, or a shadow, of the collective network)
    neuron shadow slice of each output neuron w/ its weights to input only
    2D shadow from 3D NN, 3D neuron spheres? the slice on ground is an output neuron slice.

    wait here for a while to let viewer process

This Activation Space is commonly referred to as a Hidden Space, or a Latent Space, and analogies such as King - Man + Woman = Queen can be made by adding up vectors within Hidden Space. This is a high dimensional space, where each neuron acts as its dimension, and combinations of neurons can also act as their own dimension.
    latent space animation of adding two vectors to get another

    show point on 1D w/ neuron eqn
    the 2D planes show subnetwork on its surface?

<<<<<
In our examples, for the purpose of gaining intuition, we defined our neurons, represented as basis vectors here, in terms of “human-understandable measurements”.

Given that studies suggest neurons may learn to act as "dog neurons" that measures how much of a dog something is like, it could be possible for a neuron to learn to measure cats, as in the examples shown above, and thus act as a “cat neuron”. 

But neurons do NOT always cleanly correspond to a Human-Defined Concept. A network might not even have a 'cat' neuron, even though the pattern of 'cat' is somehow recognized within the neural network. In fact, most of the millions of neurons in many neural networks may not correspond to a human-defined concept. One alternative possibility is that, instead, each neuron has a role in affecting the calculations of other neurons, similar to if-else branches in a decision tree.

(For example, one neuron’s role may be to act as a “signal” for another neuron- if neuron A is low, neuron B will disregard information from neuron C because C must pass through A to get to B, and vice versa. This relationship between neurons has led to research such as finding neuron circuits. The true “roles” of neurons in spreading information to other neurons remains mysterious, and is still a subject of research.)

<<<<<<<<<<<<
SCENE 8:

[repeat wind in the rushes a second time to end this]

There's still some questions we're going to answer. For example, what does it mean for a vector to 'become' another vector if both are just models of magnitude and direction? We'll see that what we're actually doing is moving data and the relative relations within it, not the vectors or coordinate points themselves. In our second video in this chapter, we'll use the Change of Basis to talk about how the data is different than the model coordinates used to label it, and to beware of False Friends in the Matrix.
    vector transform to another?
    fade in images of geschenk
    map != territory (put only in 2nd)

In our third video, we'll go over the visual, geometric interpretation of matrix multiplication step-by-step.

<<<<<<
(End of video or description):
We'll end this video by noting that this story of artifical cats also raises some questions, such as, why do real and fake cats follow these patterns? 

To answer that, it's because in our example story, real cats have evolved to have these proportions to adapt in a post-apocalyptic environment where most of them roam stray, while artifical counterfeit cats were created by the AI according to its favorite physical features, disregarding environmental fitness. Though they are physically and biologically a cat, their brains make calculations using artificial neural network algorithms, implemented using biological material, and in the post-apocalyptic resource scarce world, the average person has no tool to detect this. So instead of just letting these fake cats be, why were we given a job to figure out which cats are fake? Because the fake cats have been taking over the role of pets over the real cats, causing real cats to go stray. And no one knows if the fake cats are even truly 'conscious', though they seem to be to many.
    pure animated text with some static images. read fast, or just post pic w/ all text and put in description

End commutative logo in reverse?

Or use the commutative diagram logo as an 'end proof' square, which begins moving as everything else fades, and is the last thing staying.

fade in then out: "As above, so below"