SCENE 4:

So, what does this visual we've just seen have to do with a neural network? The answer actually has been hidden in front of our eyes this entire time. 
    keep visual and its unit conversion equation on screen, and the top. draw line to bottom, and show neuron eqn there, showing it gradually transform to be nearly the same as above.

You might have seen this function that calculates a neuron activations:
Ïƒ(WX+b) = A
    fade in at bottom

What does this mean? For now, let's simplify it, taking the case where sigma = I, the identity function, and b = 0: 
WX = A
    transf sigma to I, then fade out

Now W, X and A are matrices, where X are the input values, W are the connections that the inputs are multiplied with, and A is the resulting output, also called the neuron activation. The number of columns in W are the number of inputs, and the number of rows are the number of outputs, meaning there is a weight connection fo every matrix entry. We can simplify this so that there's only one input, and one output neuron.
    bottom eqn starts as 2x2 weights w/ 2x2 NN visual fading in above it, having 3 inputs and 2 activations

    show circles on left (fade in X), line in middle (fade in W), and circles on right (fade in A)

    top should have: visual, wx = a
    bottom shoudl have: neuron, [w][x] = [a]
        this goes from 2x2 to 1x1

Now, we see that our unit conversion visual has been hiding a neural network neuron this entire time.
    piecewise weights snap to straight line, showing nose to nap neuron. the w moves next to line. most importantly, shrink it down so conns all the same size to show length isn't impt (just topology).

The weight in our unit conversion equation is a connection in a neural network, and our neuron is the nap neuron.
    scale up top eqn when saying it

In fact, one can think of this nap neuron as just a single layer neural network with one neuron.
    show matrices

optional:
Or, depending on your perspective, it's a neural network with two neurons and two equations, because the input can also be called a neuron- it is the nose neuron represented as the equation 1*x.
    fade in connection of same input through 1* input to nose neuron

<<<<
So every simple, linear function such as:
wx = a

can be viewed as neural network, which itself is just a function that's composed of many smaller functions. Note that a neural network cannot approximate every function, but different types of neural networks can approximate a wide variety of functions that are useful for prediction.

<<<<
(move to end? state will discuss more on equivalent by abstraction later)

Saying an equation is a neural network depends on how one defines a neural network. If one says the neural network has more structure than just an equation, then an equation is not a neural network. For example, you can say a neural network must also be able to be trained on using an algorithm, which is information the equation lacks.
    tuple of neural network with neuron equations, training algo

But we can make an analogy on some level of abstraction, under which two things which are different are equivalent on that level of abstraction.

For example, we can have a blue square 
    commutative diagram

or a triangle and a square, both with a base with the same length.

project down a shadow describing only part of it

Now we are only focusing on computations from 

So the unit conversion visual and the neural network are not the same, but they are analogous. Likewise, the unit conversion equation and the neuron equation are not the same, but analogous.

<<<<<<<
An equation is one of the first things you learn in math, and by itself, it's the simplest, most trivial neural network you can get. This means we have seen neural networks throughout our entire lives.

But what allows the large neural networks, composed of many neuron functions, to do complex tasks, such as writing stories, that a simple neural network can't do?

Let's see what happens when we add more connections to our network, starting by just adding one more.

