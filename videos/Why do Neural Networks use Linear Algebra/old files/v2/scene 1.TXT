Scene 1:

[ Cat + Person = cat person thru NN]
[use cat person mascot when talking w/o anims]
    [music plays first 3 secs]
    starts w/ addition of concepts through neural newtork to form cat at 3 secs, just when Wind in Rushes music 'opens up' with a synth chime and down drops 'Cat Person Mathematics'

When studying new improvements to neural networks, many people run into the following problem:
    start w/ scrolling through a paper, blurring it and making concepts from paper appear bigger like news report
    NN anim runs in subset of screen?

    cat person mascot from intro goes in here

How do these unfamiliar mathematical concepts relate to neural networks?
    fade in appear on screen in italics

Words such as eigendecomposition, manifold, and more are often encountered, but someone without a background in math may be wondering what's the intuition behind these terms. In this series, we'll be diving into the key insights behind how all these concepts work.
    bold each word while fading rest of sentence. two sentences fade in. question mark appears in center.

    'in this series': you should have a placeholder.

Let's start with the most basic concept: using matrix multiplication to recognize concepts. Many helpful explanations have made been about multiplying weight matrices with inputs. But why does that specific algebraic procedure work? Why does its first step use the dot product of the weight matrix's first row and the input vector? And what do dot products have to do with how a neural network recognizes concepts?
    replay 3Blue1Brown’s vid on NNs?

Knowing why this procedure is done is key to a visual intuition behind neural network interpretability, showing how neural networks compose together complex, higher level patterns from smaller patterns. 
    show Latent space addition

You might have heard that neural networks are able to add concepts together to make analogies, such as composing together the concepts of "King minus Man plus Woman to equal Queen" in a high dimensional world called Latent Space. You might have also heard that Neural Networks can perform abstractions such as Style Editing, like with InterFaceGAN. Throughout this series, we'll be explaining how all those work, and more.
    King – Man + Woman = Queen (not geometrically?)

    https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/

    style editing example

Aside from knowing how equations work, there are no prerequesites to know before watching this video. We'll explain a lot concepts from scratch, so that you don't need to know anything about linear algebra or neural network math to follow along, but if you're unfamiliar with terms like matrix, it's recommended to watch 3Blue1Brown’s Linear Algebra playlist, namely just videos 1, 2, 3, 9 and 13, and also the first video in their Neural Network playlist; though just like you can have two classes that teach the same material, we'll be reviewing many of their concepts, but from a different perspective.

    link to 3b1br, etc. show videos playing at once  and link to them in description.

    https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown

[wind in rushes plays to silence, or fades out to silence for a moment]

Ok, let's begin.