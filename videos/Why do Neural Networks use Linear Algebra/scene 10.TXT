[Start w/ coordinate space and vectors]

Here's an analogy that helps drive home the difference between model and reality. We’ll look at two data samples: a dangerous substance, 
    fade in poison

and an object which is given to someone out of gratitude.
    fade in gift

And instead of using numbers, let’s use letters to label our entities. This means our models will resemble languages, some of which also use letters to label entities. So our first model, or language, is labeled as follows
    fade in gift text

To an English speaker, this may look wrong, because the object on the pink vector should be called something like ‘poison’, not ‘gift’. But in German, this dangerous substance is in fact called ‘gift’. 
    Fade in german on axis

If a German speaker tells the English speaker that they’re giving the English speaker a gift, the English speaker may be delighted because they think they’re getting something good. But they shouldn’t be, because what they’re ACTUALLY receiving would kill them.
    bottom of coordpsace; stick figures w/ speech bubbles and thought bubbles and flags exchanging gifts

Since there is a misunderstanding, the English speaker needs to know what ‘gift’ is actually referring to; or in other words, to know the right English word to use for what the German speaker is thinking about. So they need to translate from the language above, which resembles German, to English as follows
    show CoB, transform german to english, then show both
    fade in poison arrow and text
    fade in eqns: show this (scaled big) in manim. color input and output. do anim slow. 10.1_mat.py

Think of the weight matrix as the translator app. The input to the matrix is [-1,2], which is the pink vector where our dangerous substance is at in German space. The output of the matrix is [-4, 1], which is poison is at in English space. So the transformation moves the substance from gift to poison, allowing the English speaker to know what it truly is in their language.    
    repeat anim (10.1- slower tr)

In summary:
    show below 2img comparison
    ‘gift’ in German  != ‘gift’ in English 
    ‘gift’ in German  ~ ‘poison’ in English
        10.2.py: see 9.2.7.py 

Relating this back to using numbers as labels:
    transform into vectors

Now let's label the good object with the German word 'geschenk'. Notice that when we translate it into English, this 'geschenk' vector doesn't point to any data sample. In fact, the label ‘geschenk’ does not mean anything in English. The same goes for the label poison in German. Not all labels have to point to an data sample; so in some coordinate spaces, they just mean nonsense. This follows from the advice to not confuse the map with the territory in an analogy.
    inverse transform, then show the 2img comparison

Given that word like gift that has different meanings in different languages is called a false friend, we take heed of the following warning: To Beware of False Friends in the Matrix.
    write it out

[show poison example in BOTH start of new vid and end of vid 1.]

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
We've learned that an entity is projected from high dimensional space down to a subspace of a group of neurons, casting a shadow, or a distorted reflection. 
    GEB image with moving shadowy effect (os transitions)

Though viewed from different perspectives, these shadows are often entangled, meaning that a change in one dimension is analogously reflected in another, like in a house of mirrors.
    https://www.youtube.com/watch?v=F-BqDWG72iM&ab_channel=RobertJ.Fuller

Now that we've connected the geometry of linear algebra to neural networks, we can finally begin to study its world of concepts. What lies in this mysterious realm of AI imagination?
    AI animate DMT space, feature viz
    neural network feature visualization interpolation

    https://www.youtube.com/watch?v=5snZ0ba9RjU&ab_channel=schizo604

    https://colab.research.google.com/github/eps696/aphantasia/blob/master/IllusTrip3D.ipynb
