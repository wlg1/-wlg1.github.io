SCENE 4:

So, what does this visual we've just seen have to do with a neural network? The answer actually has been hidden in front of our eyes this entire time. 
    keep visual and its unit conversion equation on screen, and the top. draw line to bottom, and show neuron eqn there, showing it gradually transform to be nearly the same as above.

You might have seen this function that calculates a neuron activations:
Ïƒ(WX+b) = A
    fade in at bottom

What does this mean? For now, let's simplify it, taking the case where sigma = I, the identity function, and b = 0: 
WX = A
    transf sigma to I, then fade out

Now W, X and A are matrices, where X are the input values, W are the weight connections that the inputs are multiplied with, and A is the resulting output, also called the neuron activation. 

The number of columns in W are the number of inputs, and the number of rows are the number of outputs, meaning there's a connection fo every matrix entry. We can simplify this so that there's only one input, and one output neuron.
    bottom eqn starts as 2x2 weights w/ 2x2 NN visual fading in above it, having 3 inputs and 2 activations

    show circles on left (fade in X), line in middle (fade in W), and circles on right (fade in A)

    top should have: visual, wx = a
    bottom shoudl have: neuron, [w][x] = [a]
        this goes from 2x2 to 1x1

Now, we see that our unit conversion visual has been hiding a neural network neuron this entire time.
    piecewise weights snap to straight line, showing nose to nap neuron. the w moves next to line. most importantly, shrink it down so conns all the same size to show length isn't impt (just topology).

The weight in our unit conversion equation is a connection in a neural network, and our neuron is the nap neuron.
    scale up top eqn when saying it

In fact, one can think of this nap neuron as just a single layer neural network with one neuron.
    show matrices

Or with two neurons, if you think of the input as a nose neuron represented by the equation 1*x.
    fade in connection of same input through 1* input to nose neuron

<<<<
So a neural network can be viewed as a function that's composed of many smaller functions. Note that it can't approximate every function, but different types of neural networks can approximate a wide variety of functions that are useful for prediction.

Of course, a neural network is more than just this equation, so rather than saying this equation is a neural network, we can say it's analogous like a neural network.

But what allows large neural networks to do complex tasks that a simple neural network can't do? Let's see what happens when we add more connections to our network, starting by just adding one more.

