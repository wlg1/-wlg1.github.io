OUTLINE:

SCENE 2:
First only talk about predicting naps from nose tip (positive only)

SCENE 3:
Show nose tip on its own number line, growing in images. talk about its basis vector.

SCENE 4:
Then show it being mapped to nap, saying this change of units is a change of basis, and unit conversion. show this as a matrix.

<<<
SCENE 5:
Next, introduce ear length on its own number line. map this to nap ADDED to nose's mapping. use input [1,1]. say this is w1*nose + w2*ear = nap. show this as a matrix.

SCENE 6:
Then show the 2 number lines being orthogonal, in order to form data points. First show basis vectors forming data pt. THen go in reverse; show the data pts being broken into each axis, separating. 

SCENE 7:
Then move these axis to the pinball machine to calculate the value of nap for that data pt. in each step of visual, show BOTH the matrix multp and the linear eqn analogous form.
    on the pinball machine visual, just rotate the bottom input (ear) to horizontal. 

<<<
SCENE 8: conclusion
now we talk about caps. this is just a second measurement, and thus a second row in the matrix. show Model 1 and Model 2, showing the rotation animation. say in future video, will discuss matrix multp visually looks with more than 1 row in the weight matrix.

end by hinting at inverse of change of basis.


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Scene 1:

[ Cat + Person = cat person thru NN]
[use cat person mascot when talking w/o anims]
    [music plays first 3 secs]
    starts w/ addition of concepts through neural newtork to form cat at 3 secs, just when Wind in Rushes music 'opens up' with a synth chime and down drops 'Cat Person Mathematics'

When studying new improvements to neural networks, many people run into the following problem:
    start w/ scrolling through a paper, blurring it and making concepts from paper appear bigger like news report
    NN anim runs in subset of screen?

    cat person mascot from intro goes in here

How do these unfamiliar mathematical concepts relate to neural networks?
    fade in appear on screen in italics

Words such as eigendecomposition, orthogonal projection, and more are often encountered, but someone without a background in math may be wondering what's the intuition behind these terms. In this series, we'll be diving into the key insights behind how all these concepts work.
    bold each word while fading rest of sentence. two sentences fade in. question mark appears in center.

    'in this series': you should have a placeholder.

Let's start with the most basic concept: matrix multiplication. Many helpful explanations have made been about multiplying weight matrices with inputs. But why does that specific algebraic procedure work? Why does its first step use the dot product of the weight matrix's first row and the input vector? And what do dot products have to do with how a neural network recognizes concepts?
    replay 3Blue1Brown’s vid on NNs?

Knowing why this procedure is done is key to a visual intuition behind neural network interpretability, showing how neural networks compose together complex, higher level patterns using smaller patterns. 
    show Latent space addition

You might have heard that neural networks are able to add together concepts to make analogies, such as composing together the concepts of "King minus Man plus Woman to equal Queen" in a high dimensional world called Latnet Space. You might have also heard that Neural Networks can perform abstractions such as Style Editing, like with InterFaceGAN. Throughout this series, we'll be explaining how all those work, and more.
    King – Man + Woman = Queen (not geometrically?)

    https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/

    style editing example

There are no prerequesites before watching this video. We'll explain a lot concepts from scratch, so that you don't need to know anything about linear algebra or neural network math to follow along, but if you're unfamiliar with terms like a matrix, I recommend watching 3Blue1Brown’s Linear Algebra playlist, namely just videos 1, 2, 3, 9 and 13, and also their first "neural network" video, though we'll be reviewing many of their concepts, but from a different perspective.

    link to 3b1br, etc. show videos playing at once  and link to them in description.

    https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown

[wind in rushes plays to silence, or fades out to silence for a moment]

Ok, let's begin.

<<<
SCENE 2: cat people

SCENE 2.1:
To show why neural networks use matrix multiplication, and how this is related to the geometry of Latent Space, let’s start with an example where in the future, cat people roam the world. Some evidence suggests that how much a cat person enjoys naps can be predicted by measuring their nose tips and ear lengths.
    cat faces pop in

    one of them in sleeping. zoom in to make them be the only one on screen as the zzz play

    highlight the shapes

For example, a lot of cat people with long nose tips and long ears are said to enjoy naps.
    zzz with smile

But we don't know the exact set of rules to figure this out.
    ? * nose_tip + ? * ear_length = nap happy

We can also predict how much each cat person enjoys wearing baseball caps by measuring their ear lengths and nose tips, using different weighings of these features than before. But again, we also don't know this formula.
    ?? * nose_tip + ?? * ear_length = cap happy

    next to image of cat person smiling while thinking of a cat with a cap

So we're going to have to find what these rules are using a neural network.
    make these appear in input and output nodes

<<<<<
Let's take a cat person named Tom. How much does he like naps, and how much does he like caps?
    shift fade into population

We'll start by measuring Tom as a data sample using nose tip, and ear length.
    expand cat to center (transform)
    fade in words in openshot: note that the height of the nose tip box doesn't matter, since it's just length, 

So every real cat or fake cat in our population is represented in the dataset as an data sample
    'cat' shifts copy of cat to left
    'data sample' shifts copy of data pt to right. words drop to bottom.

<<<
SCENE 3: units

Each of these features can be measured using a basic measuring unit, such as by a 1 unit of nose tip, or 1 unit of ear length. Think of these basic measuring units as just like using 1 meter to measure distance, or using 1 second to measure time.
    fade out the cat and data pt from scene 2
    separate nose and ear from the data pt of cat; nose shifts to top, ear shifts to bottom. make the 1's fade in after each shift.

We can grow or shrink basic measuring units to get different measurements. For example, at the bottom right, we can make the unit 1 ear twice as big to get the unit 2 ear, or cut it in half to get the unit 0.5 ear.
    left side is unit 1
    fade in unit 2 ear, and unit 1 nose

    growing animation in manim?

At the top right, the higher the nose tip points, the higher the value. It can also point down to get a negative measurement. The lower the tip points, the lower the value.
    nose tip animation with +/- number next to it

<<<
3.2:

show the two on 2 separate horizontal number lines, going bigger and smaller. top is nose, bottom is body (this is for scene 4, too, when nap in middle).


<<<
SCENE 4: linear eqn, weights

4.1:
Now, we want to find out how much the cat people in this population enjoy naps.
    fade in each line as words w/ image

    new 1D coordinate space, completely new Model. This is NOT the same as our input space from before, even though it's measuring the same data.

    nap number line of napping cat faces, with smile getting bigger with each move up, and frowning when move to -1

<<<
4.2:
How do we find the values of this new, currently unknown, value? We can calculate it using the measurements already know, such as nose. It was noticed that the longer a cat person's nose pointed up, the more they liked naps. In fact, each time a cat person's nose that pointed up was longer by 1 unit, they usually enjoyed naps twice as much.
    noses pointed up; show image of unit 1
    longer noses -> enjoy naps
    list out 1, 2, 3 on left
    1 units of nose tip → 2 units of 'enjoy naps'
    2 -> 4 (image on each side)
    3 -> 6

This is similar to unit conversion. For every 1 meter, there are 3.28 feet. So if an person is 2 meters tall, they're also 6.56 feet tall.
    1 units of meter → 3.28 units of feet
    2 units of meter → 6.56 units of feet

So let's convert nose units into nap units. Now as an equation, this is just 2 * (nose) = 2 nap.

Our unit conversation factor of 2 can be represented, in general, as a variable W(nose->nap), which means how much we should weigh our nose value by to get our nap value.
    transform above eqn to vars. weight numbers should be in italics, to not get them confused

    another way to show this is nap/nose (first show w's subscript like this, then transform to nose->nap)

<<<<<<<<
4.3:

Again, to eventually explain the mysterious connection of neural networks to Latent Space, we'll have to see what this looks visually. We'll measure our nap units along a number line, and start with one input of nose.
    circle fades on bottom, and 1 (image too) goes into it, becoming a one vector.

    show equation on top the entire time

Now we need to weigh this nose vector by a factor of 2 to convert it into nap.
    the vector (oval reverts to circle) goes up a thick line next to a w, which transforms into 2*

So we end up with 2 nap units.
    finally, a thick vertical line connects this to the top, which is the nap number line. the output node shows '2'.

If we use an input of 2, we end up with 4 nap units.
    same visual as before. pause longer to let this sink in.

[In Summary]:
    a number line of nap. at bottom is a node called nose, where a number 1 is animted to be inputted (move in) to it. this expands the circle into an oval containing a vector of size 1 (use dots or thin vert lines). 
    the vector (oval reverts to circle) goes up a thick line next to a 3*, where like a roller coaster pinball machine, the 3* fades into the line (like a conveyer belt) and pushes it into a line of size 2.
    finally, a thick vertical line connects this to the top, which is the nap number line. the output node shows '2'.

<<<<<<<<
4.4:

So, what does this visual we've just seen have to do with a neural network? The answer has been hidden in front of our eyes this entire time. 
    piecewise weights snap to straigh line, showing nose to nap neuron. the w moves next to line. most importantly, shrink it down so conns all the same size to show length isn't impt (just topology).

    show equation W*nose = nap, too
    
The weight in our equation is a connection in a neural network. And this weight is a 1x1 matrix, multiplied by a 1x1 input vector, to get a 1x1 output vector, which is the nap neuron. This entire equation to calculate the nap neuron is just a neuron within a neural network. In other words, it is a neural network with one neuron.
    show matrices

Or, depending on your perspective, a neural network with two neurons and two equations, because the input can also be called a neuron- it is the nose neuron represented as the equation 1*x.

An equation is one of the first things you learn in math, and by itself, it's the simplest, most trivial neural network you can get. This means we have seen neural networks throughout our entire lives.

But what allows the powerful neural networks, composed of many neurons, to do complex tasks, such as writing stories, that a simple neural network can't do?

To answer this, we'll have to continue in our mathematical exploration of neural networks.

<<<<<<<<<<
4.5:

Now, let's say we can also predict how much a cat person enjoys naps by their ear length. That is, the longer a cat person's ears, the more they enjoy naps by a little. But relatively speaking, this relation of ears to naps is less important the relation of noses and naps for predicting nap enjoyment. For example, a cat person with ear length of 2 would only enjoy naps by 0.5 units more than a cat person with an length of 1.
    1 -> 0.5 nap
    2 -> 1 nap

So we get the equation: (0.5) * ear = nap
Which we can represent using variables.
And since it was also noticed that nose tip and ear length can build on top of each other to predict nap enjoyment, they are independent of each other, and can be added:
    w1*nose + (w2) * ear = nap

Let's see what this looks like visually, continuing where we left off.
    transform  w1*nose = nap to  w1*nose + (w2) * ear = nap
    same as before, but on top of prev output node is ear, which goes down to w2*, becoming 0.5 and making a half unit vector, which is added to create 2.5

And again, this has an analogy to a neural network, where w2 is another connection from the ear neuron to the nap neuron.

At the start, we asked how matrix multiplication works. 
w1*nose + (w2) * ear = nap

Notice how this equation resembles the following dot product between a weight vector and an input vector:
[nosecat earcat]⋅[1 2]=nosecat∗0.5+earcat∗2
    show dot product equation b/w w vec and x vec, each fading in as you say them

Not only that, but it resembles the first step of matrix multiplication:
[nosecat ? earcat ?][1 2]=[nosecat∗0.5+earcat∗2?]

Let's set nosecat as 2, and ignore the second term earcat for now.
    transform earcat into ?

for every nose tip of unit 1, there are 2 units of cat
    1 units of nose tip → 2 units of 'likely to be cat'

if we increase our input to [2, 2], then:



<<<<<<<<<<<<<<<<<<
We can also use negative weights. These mean that the higher a feature's value is, the less likely the cat is to be real. Having a longer nose is a penalty against being a real cat.
    higher x, lower cat

    NOTE: this weight negative and input negative are not the same!
    shorter nose tip (down) + longer ear = cap
    -> 1 * (nose tip) (down) + (2) ear = nap
    -> -1 nose tip + 2 * ear = cap

<<<<<<<<<<<<<<<<<<
SCENE 5: 

SCENE 3.2:

[ in openshot, gradually zoom in the manim video so it's y-axis is (-2, 3) and x-axis is (-2, 7)]

3.2.1:
We mentioned that there's some mysterious connection between a neural network and the geometry of a 'latent space'. To explain this connection, we'll have to understand how our data is visually represented.
    fade back in the King example

The data samples in this dataset can be represented as points in a coordinate space, using nose tip and ear length as axes. Note that each data point here corresponds to a data sample of a basic measuring unit.
    ear length on y-axis, nose tip on x-axis
    D:\Documents\_prog\prog_cust\manim\video 1 
        \ 3.2.py

    Face length and ear length vectors, then points, then fade in data samples w/ 1's in them

3.2.2:
We can add together the unit 1 nose tip and the unit 1 ear length to form a data sample with a unit 1 nose tip and a unit 1 ear length.
Notice that this is the same as adding the data points (1,0), and (0,1) to form (1,1). We can represent these data points as vectors, and show that adding these data sample images together is the same as vector addition. We'll represent our vectors using 1-dimensional matrices.
    Quickly move copies (or the original, so it doesn't take up too much space) of (1,0) and (0,1) into (1,1)
    Also move copies to top right so they're used as addition.??

    Addition at top right: add them, then after move them to overlap, fade in equals sign aligned w/ expression.
    Then put in data pts, move to (1,1), and eqns using vectors move out from img eqns to above.

    fade out prev eqns

3.2.3:
Each feature along the axis represents a different measurement.

It's easy to see how we can add together nose tips and ear lengths of different units. Let's take a nose tip of 0.5 units, and a ear length of 2 units, and add them together. To model this using vectors, we multiply 0.5 by [1, 0], and 2 by [0, 1], and then add them together to get [0.5, 2].
    Scale nose tip down, then ear length up, by moving data pt to become new pt, and move those two into (0.5, 2) such that they overlap. Same eqns. For vectors, start with 0.5 * [1,0], then move scalar inside and at same time, move 0.5 into img (layer behind) to shrink it.

[to prevent eqns from being seen as 'on grid', have a box around them w/ no grids and may have diff color inside]

These basic measuring units are labeled by the vectors [1, 0] and [0, 1], which are called basis vectors. In other words, these units are used as the basic building blocks to measure features, which in turn, measure data points. They're like an alphabet used to form words in a language.
    re-play the basis vector animations w/ labels
    write out words 'basis vectors' on screen

By labeling the basis vectors with nose tip and ear length, we have the basis vectors for our Input Space. Every point, or vector (to use the term informally), is a possible input.



<<<<<<<<<<<<<<<<<
Now, instead of using the basic measuring unit of 1 meter to measure a person, we are using the basic measuring unit of 1 foot to measure them. Before, 1 meter acted as a "basic vector", and now 1 foot acts as our basis vector. This unit conversion, or "Change of Units", is called a "Change of Basis".

One dot product step is analogous to 1D matrix multiplication; so two dot product steps would be analogous to 2D matrix multiplication. This answers the question we posed in the beginning of our video: the dot product is used to convert multiple units into a new unit.

Instead of just using one measurement like in our “meter to feet” example, the “nose and ear to cat” example uses two measurements to find the value of cat. Matrix multiplication allows for a change of multiple units, or multiple dimensions. The weight matrix W is analogous to the conversion factor.
    show words: meter -> feet

Now if we just use one row in our matrix, we get the equation to determine real cats. If we use two rows, we get the equation to also find fake cats.
    second row fades in:
    W=[nosecat earcat  
       noseRat earRat]

<<<<<<<<<<<<<<<<<<
SCENE 6: CoB visuals

With both the real cat equation and the fake cat equation, we have a system of equations. This matrix would measure the real cats on the x-axis, and the fake cats on the y-axis. 
    On right coordinate, real cats appears on x, etc

Let's say someone gives us the matrix values that allow us to accurately calculate "likely to be real cat or fake cat". We'll go over this matrix multiplication step by step soon in one of our next videos.

Since each coordinate space provides a different way to represent the data, let’s call each coordinate space a Model, namely Model 1 and Model 2. These two measurement methods are actually measuring the same data samples; the data samples in Model 1 are present in Model 2, but are now measured by different vectors.
    compare 2 coordinate spaces side by side, fading in each new image.

<<<<<<<<<<<<<<<<<<<<<<<<<
(put into video 3?):
Another thing to note here is that the concepts sent to the new basis vectors have an interpretation in the first Model. That is, the cat with [values] was found to be so often fake in our dataset, that it's sent to (0,1), which means samples with those values are almost always not ever a real cat, and almost always a fake cat.
    see vector values for cats.TXT

But although we can show the real cat and fake cat 'basis vectors' in Model 1 in this example, keep in mind that matrices are not always invertible. We'll discuss this more in a later video.

(also interpretation is not always req. a neg value may just mean 'so close to 0', and is just mean to throw samples a certain distance, not meant to be interpreted)

<<<<<<<<<<<<
SCENE 7: NNs

So what does Change of Basis have to do with Neural Networks? You might have noticed that changing from Model 1 to Model 2 demonstrates an idealized, simplified example of what a neural network does- making guesses about what an input data point is. In fact, one can think of this matrix as a single layer ‘neural network’ such that for the function that calculates the neuron activations:
O=σ(WX+b)
    instead of entire NN, show simple input and output. X on left, goes into equation, outputs O on right. show both I/O and eqn. I/O has circles has neurons, eqn is above their conn arrow. have pulsating animation where x lights up, then arrow lights up, then N (or A) lights up.

Which, for a data sample, outputs the values it guesses for the 2 classes {cat, rat}, it sets σ=I, the identity function, and b = 0:
O=WX
    substitute those terms in as transform, then make them fade out and have others shift to new pos

We can see how this matrix relates to weights in a neural network; each column corresponds to outgoing weights of a previous layer neuron, and each row corresponds to incoming weights of a next layer neuron:
    make a new NN using your matrix. two input nodes, two weights. add each node at a time and show the corresponding math rep (weight conn adds weight in matrix). transform weight variables into values; stronger connection make weights bolder, negatives make weights a diff color (red shade) vs blue or green

    'we can add a third feature to have 3 input neurons, which in turn, would create two more weights w3_1 and w3_2'. fades in then out.

    make using 3b1br anim template

As we see that each of the two neurons on the left act as basis vectors in the previous layer (Model 1), and the two neurons on the right act as basis vectors in the next layer (Model 2), such that the two neurons on the right are a linear combinations of the previous layer neurons and their weights, we come to a very important concept:
    show Model 1 and 2 w/ neurons arrowing out

Neurons are Basis Vectors in an Activation Space.
    write out on screen

Thus, every neuron in a neural network is a measurement on the data, coming up with an interpretation for it from its own perspective. (Think of this as taking a slice, or a shadow, of the collective network)
    neuron shadow slice of each output neuron w/ its weights to input only
    2D shadow from 3D NN, 3D neuron spheres? the slice on ground is an output neuron slice.

    wait here for a while to let viewer process

This Activation Space is commonly referred to as a Hidden Space, or a Latent Space, and analogies such as King - Man + Woman = Queen can be made by adding up vectors within Hidden Space. This is a high dimensional space, where each neuron acts as its dimension, and combinations of neurons can also act as their own dimension.
    latent space animation of adding two vectors to get another

    show point on 1D w/ neuron eqn
    the 2D planes show subnetwork on its surnose?

<<<<<<<<
magical or something like that
whatever

<<<<<
In our examples, for the purpose of gaining intuition, we defined our neurons, represented as basis vectors here, in terms of “human-understandable measurements”.

Given that studies suggest neurons may learn to act as "dog neurons" that measures how much of a dog something is like, it could be possible for a neuron to learn to measure cats, as in the examples shown above, and thus act as a “cat neuron”. 

But neurons do NOT always cleanly correspond to a Human-Defined Concept. A network might not even have a 'cat' neuron, even though the pattern of 'cat' is somehow recognized within the neural network. In fact, most of the millions of neurons in many neural networks may not correspond to a human-defined concept. One alternative possibility is that, instead, each neuron has a role in affecting the calculations of other neurons, similar to if-else branches in a decision tree.

(For example, one neuron’s role may be to act as a “signal” for another neuron- if neuron A is low, neuron B will disregard information from neuron C because C must pass through A to get to B, and vice versa. This relationship between neurons has led to research such as finding neuron circuits. The true “roles” of neurons in spreading information to other neurons remains mysterious, and is still a subject of research.)

<<<<<<<<<<<<
SCENE 8: conclusion

[repeat wind in the rushes a second time to end this]

There's still some questions we're going to answer. For example, what does it mean for a vector to 'become' another vector if both are just models of magnitude and direction? We'll see that what we're actually doing is moving data and the relative relations within it, not the vectors or coordinate points themselves. 

In our second video in this chapter, we'll use the Change of Basis to talk about how the data is different than the model coordinates used to label it, and to beware of False Friends in the Matrix.
    vector transform to another?
    fade in images of geschenk
    map != territory (put only in 2nd)

In our third video, we'll go over the visual, geometric interpretation of matrix multiplication step-by-step.

<<
cat person naps while comm dia is created, fading out until only square is left

use the commutative diagram logo as an 'end proof' square, which begins moving as everything else fades, and is the last thing staying.

bottomright, fade in then out: "As above, so below" like in cowboy bebop?