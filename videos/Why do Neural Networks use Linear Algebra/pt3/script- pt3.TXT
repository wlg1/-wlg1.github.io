
In our third video, we'll go over the visual, geometric interpretation of matrix multiplication step-by-step.


What are the Columns of a Matrix?

We now explained what the rows of W are. But what are its columns? And how does this tie into the geometric representation of matrix multiplication, as we see in the figure above? Let’s match terms in these equations to their geometric representation:

In Model 1, [10] labels . In Model 2, [1−1.5] labels .

Thus, [1−1.5] is where the data sample previously labeled by the basis vector [10] is sent.

Notice that [1−1.5] is the first column of W. And tailcat = 1 is the Cat coordinate of the vector [tailcat=1−1.5] in Model 2. Given the fact that the vertical axis of Model 2 denotes “likely to be rat”, it’s pretty clear that the second row of this vector is tailRat, which is how much the tail length is weighed by to calculate the value of the Rat coordinate.

Therefore, [tailcat=1tailRat=−1.5], the first column of W, and the Model 2 vector labeling , contains the conversion factors for each of the basis vectors {cat,Rat} of Model 2, in terms of only tail.4

Likewise, [bodycat=1.5bodyRat=−1], the second column of W, and the Model 2 vector labeling , contains the conversion factors for each of the basis vectors {cat,Rat} of Model 2, in terms of only body.

W=[tailcattailRatbodycatbodyRat]
If we multiply this by the basis vector in Model 1 that labels , we see it uses ONLY the conversion units for tail, and none of the conversion units for body. This is because this data sample has no body, so it would not use body in its calculation for cat and rat at all. The same goes for the other basis vector in Model 1.

[tailcattailRatbodycatbodyRat][10]=[1∗tailcat+0∗bodycat1∗tailRat+0∗bodyRat]=[tailcattailRat]
So we arrived at several conclusions:

1) Because Model 2 uses the old measurements of Model 1 to calculate its new measurements, the matrix W contains the weights needed to determine how important each old measurement is for each new measurement

2) Each row of the matrix contains weights to calculate one new measurement in O
3) Each column of the matrix contains weights for how one OLD measurement in X is used

4) The dot product is applied between every row of the matrix and the input vector because the same input vector uses different weights of the old measurements in X for every new measurement in O

<<<<<<<<<
Notice that if we animate the change of basis, it seems like there isn't much of a difference between the two Models. But there is. 
    [animation]

We can use a crude way to predict if the cat is real or fake based on if it has a greater value on the x-axis, real, or on the y-axis, fake. If we did not perform a change of basis, that is, not using the learned classification rules, we'd have to rely on unreliable methods, such as only using ear length to say bigger cats are fake (which is not always right). 
    only using ear length (show is bad classification)

Again, this is just a crude approach, as our output values aren't even probabilities; this can be remedied by passing them through a function to turn them into probabilities. Additionally, this is a very simple change of basis, used for teaching purposes; change of basis can get very complex, especially when composing multiple matrices together.