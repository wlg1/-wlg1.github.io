When studying new improvements to neural networks, people often encounter concepts that they're not familiar with, such as manifold, or isometry.

So what do these mathematical concepts mean, and why do they use them?

In this series, we'll be providing answers to these questions in an visual and intuitive way.

Let's start with a basic concept: using matrix multiplication to recognize features. Neural networks multiply weight matrices with inputs. 

But why does that specific algebraic procedure work? Why does its first step use the dot product of the weight matrix's first row and the input vector to get the first output? And what does the matrix have to do with how a neural network interprets a higher dimensional reality from its own lower dimensional perspective?

Knowing why this procedure is done is key to a visual intuition behind how neural networks compose together patterns into more complex, higher level patterns. For example, they can add concepts together to make analogies, composing together "King minus Man plus Woman to equal Queen" in a hidden, high dimensional world called Latent Space, or perform abstractions such as Style Editing. Throughout this series, we'll be explaining how all these work.

Aside from knowing multiplication, there's no prereqs to watching this. We'll explain a lot of concepts from scratch. If you're unfamiliar with some terms, it's recommended to watch these 6 3Blue1Brown videos, which are in the description. But just like two classes that teach the same material from different perspectives, we'll be reviewing many of those videos' concepts to explain new ones in our series.

Ok, let's begin.

