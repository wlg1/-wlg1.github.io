Scene 1:

When studying new improvements to neural networks, many people run into the following problem:

What do these mathematical concepts mean, and why do they use them?

Words such as eigendecomposition, manifold, and more are often encountered, but someone without a background in math may be wondering what's the intuition behind these terms. In this series, we'll be diving into the key insights behind how all these concepts work.

Let's start with the most basic concept: using matrix multiplication to recognize concepts. Many helpful explanations have made been about multiplying weight matrices with inputs. But why does that specific algebraic procedure work? Why does its first step use the dot product of the weight matrix's first row and the input vector? And what do dot products have to do with how a neural network recognizes concepts?

Knowing why this procedure is done is key to a visual intuition behind neural network interpretability, showing how neural networks compose together complex, higher level patterns from smaller patterns. 

You might have heard that neural networks are able to add concepts together to make analogies, such as composing together the concepts of "King minus Man plus Woman to equal Queen" in a high dimensional world called Latent Space. You might have also heard that Neural Networks can perform abstractions such as Style Editing. Throughout this series, we'll be explaining how all those work, and more.

(talk fast, ~30s):
Aside from knowing how equations work, there are no prereqs watching this video. We'll explain a lot of concepts from scratch, so you don't need to know neural network math to follow along, but if you're unfamiliar with some terms, it's recommended to watch 3Blue1Brownâ€™s Linear Algebra playlist, namely videos 1, 2, 3, 9 and 13, and also the first video in his Neural Network playlist; though just like two classes that teach the same material from different perspectives, we'll be reviewing many of those videos' concepts to explain new ones in our series.

[wind in rushes plays to silence, or fades out to silence for a moment]

Ok, let's begin.