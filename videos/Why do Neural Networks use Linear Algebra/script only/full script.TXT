When studying new improvements to neural networks, people often encounter concepts that they're not familiar with, such as manifold, or isometry.

So what do these mathematical concepts mean, and why do they use them?

In this series, we'll be providing answers to these questions in a visual and intuitive way.

Let's start with a basic concept: using matrix multiplication to recognize features. Neural networks multiply weight matrices with inputs. 

But why does that specific algebraic procedure work? Why does its first step use the dot product of the weight matrix's first row and the input vector to get the first output? And what does the matrix have to do with how a neural network interprets a higher dimensional reality from its own lower dimensional perspective?

Knowing why this procedure is done is key to a visual intuition behind how neural networks compose together patterns into more complex, higher level patterns. For example, they can add concepts together to make analogies, composing together "King minus Man plus Woman to equal Queen" in a hidden, high dimensional world called Latent Space, or perform abstractions such as Style Editing. Throughout this series, we'll be explaining how all these work.

Aside from knowing multiplication, there's no prereqs to watching this. We'll explain a lot of concepts from scratch. If you're unfamiliar with some terms, it's recommended to watch these 6 3Blue1Brown videos, which are in the description. But just like two classes that teach the same material from different perspectives, we'll be reviewing many of those videos' concepts to explain new ones in our series.

Ok, let's begin.

To show why neural networks use matrix multiplication, letâ€™s start with an example where in the future, cat people roam the world. Some evidence suggests that how much a cat person enjoys naps can be predicted by measuring how far away their nose tips are from the center of their face. We call this measurement, 'Nose tip'.

For instance, many cat people with long nose tips are said to enjoy naps. It's always the case that the more a cat person smiles when napping, the more they enjoy it. We measure how much they enjoy naps using a metric called 'Nap Smile', or just shortened to 'Naps'. 

But we don't know how nose tips exactly affects enjoying naps. How big of a Nap Smile does a cat person with a nose of size 3 have, compared to a cat person with a nose of size 2? A slightly bigger smile, or a much bigger smile? 

We're going to have to figure this out by using a neural network that takes in a cat person's nose tip as input, and predicts their nap enjoyment level.

<<<
Let's start with the input. We're only going to measure nose tip, capturing it into a Data Measurement.

These are measured using 1 unit of nose tip, which is a basic measuring unit, just like using 1 meter for distance, or using 1 second for time.

Let's measure along a number line which we'll call the Nose Space, or the Nose Dimension. We can make a nose tip of 1 unit twice as big to get a nose tip of 2 units, or make it three times as big to get 3 units.

Each point in Nose Space is associated with a different Data Measurement.

Now, we can also measure Nap Smile along Nap Space. Each positive unit measures how big their smile is.

Let's take an example where the longer a cat person's nose tip was, the more they liked naps. A cat person with a nose tip of 1 unit had 2 Nap Smile units, a cat person with 2 nose tip units had 4, and so forth.

We notice a pattern here that's somewhat like unit conversion. For instance, for every 1 meter, there are 3.28 feet. So if an person is 2 meters tall, they're also 6.56 feet tall.

So let's convert nose units into nap units. Turning this into an equation, we have 2 (nap over nose) * (1 nose) = 2 nap.

And visually speaking, this means that for every one unit of nose tip, there are two units of Nap Smile. 

Notice the point 1 in the Nose number line and the point 2 in the Nap number line are labeling the same Nose Tip Measurement. This is because we're saying that every nose tip of 1 ALWAYS has a nap smile of 2. 

Our equation isn't mapping a number, such as 1, to another number, such as 2, but it is mapping a Data Measurement from 1 in the nose space to 2 in the nap space. This is a Change of Units.

Likewise, for every two units of nose tip, there are four units of Nap Smile.

Our unit conversation factor of 2 can be represented, in general, as a variable W, which means how much we should weigh our nose value by to get our nap value. 

We can also represent out input with a variable x.

So, what does this visual we've just seen have to do with a neural network? The answer actually has been hidden in front of our eyes this entire time. 

Let's look at this equation below, which is used by a neural network. What does our simple unit conversion equation have to do with it? We're going to show that they're very similar.

First, we'll represent the output value as one variable, and not look at units.

Second, note that all the variables in the equation below are matrices. So we'll analogously map multiplication into matrix multiplication by putting our terms in brackets. 

The b is the bias, which we don't need to discuss now. We can show it in our equation above by adding 0 to the left side, noting that adding 0 doesn't change our equation.

The sigma is called an activation function, which we also don't need to discuss now. For our unit conversion equation, we'll use the Identity function I. We see that when x goes through the identify function I, we get x again. It's like multiplying an expression by 1. So again, this doesn't change our equation. 

Now, both equations look similar. But what is the equation we have below? It's a neuron equation, which is what a neural network uses to compute a neuron activation. So our unit conversion equation is just a very simple neuron. 

<<<
W contains weight connections that the X inputs are multiplied with, and A contains the neuron activations, which in our case, is the nap neuron. 

In this case, the matrix W actually just consists of a single weight variable lower case w, also called w one. Same for the X  and A matrices.

In fact, one can think of this nap neuron as just a single layer neural network with one neuron.

Or with two neurons, if you think of the input as a nose neuron represented by the equation 1*x.

Now, we see that our unit conversion visual has been hiding a neural network neuron all along.

<<<<
So a neural network can be viewed as a function that's composed of many smaller functions. Note that it can't approximate every function, but different types of neural networks can approximate a wide variety of functions that are useful for prediction.

Of course, a neural network is more than just this equation, so rather than saying this equation is a neural network, we can say it's approximate, or analogous, to a neural network.

But what allows large neural networks to do complex tasks that a simple neural network can't do? Let's see what happens when we add more connections to our network, starting by just adding one more.

When using our equation to predict nap enjoyment, we find that there's cases where using nose isn't enough. We found that Tom has a nose of 1 unit, and has a Nap Smile of 2.75, but Jane also has a nose of 1 unit, yet has a Nap Smile of 3.5. 

Since we also found that ear length has some correlation with nap enjoyment, let's use both nose tip and ear length to predict nap smile.

Just like with nose tip, we also have an ear length of 1 unit, 2 units, and 3 units

<<<
How can we represent a Data Measurement of a cat person in terms of both nose tip and ear length? We'll use a 2-dimensional coordinate space.

If we rotate our ear length number line by 90 degrees and put it next to our nose tip number line, we get a coordinate plane where we can represent each cat person using both nose tip and ear length as axes.

Now, how can we add together the Nose Tip and Ear Length Data Measurements to describe a Cat Person? 

Let's say we're taking measurements of a cat person named Tom. To describe him, we start at the origin point. We find that his nose tip is 1 unit, so we move from the origin to (1,0). Then we find his ears are 1 unit long, so we move up from (1,0) to (1,1).

<<<
These arrows that formed a path of step-by-step instructions are called Vectors, which are defined by a length, and a direction. You can place them between two coordinate points, from a tail point to a head point. 

We'll use vectors to tell us where we go from one coordinate point to another, and by how much. To add vectors together, we just move the tail of one vector to the head of another.

Because unlike the coordinate points, which are permanently fixed where they are, the vectors can be moved anywhere in coordinate space. 

This vector is the SAME one that's been moved here. But it is not the same as this vector, or this one.

Since the vectors can be moved, it seems like there's no fixed association between a vector and a Data Measurement. But for the ease of demonstration, we'll informally say there is to help us get a high-level understanding of how vectors add features together.

<<<
Looking back at our example with Tom, we'll represent our vectors using matrix brackets, whose values describe the head of a vector, with its tail on the origin.

We'll show that adding these two features together is the same as adding the two vectors pointing to these features.

When a vector's tail is on the origin, this vector points to the same Data Measurement as point (1,0). So far, we have this Data Measurement as a partial Data Measurement.

When you add it to the vector going up to (0, 1), (noting that this partial Data Measurement from [0,1] is not what's actually on point [1,1])

it's as if you're given an instruction to add the partial Data Measurement on [1, 0] with the partial Data Measurement on [0, 1].

Therefore, Tom is on vector [1,1]. You can get to Tom either by the path of these two added vectors, or by the vector pointing to [1,1].

Because this combination of nose tip and ear length describes our cat people input, we call this 2D coordinate space our Input Space.

So now we know how to describe a cat person using both nose tip and ear length. How do we use both of these features to calculate Nap Smile?

First, let's see how to predict Nap Smile using Ear Length. We notice that the longer a cat person's ears are, the more they enjoy naps. But relatively speaking, ear length doesn't have as much impact on nap enjoyment as nose tip does. For example, a cat person with an ear length of 2 would only enjoy naps by 0.75 units more than a cat person with an length of 1.

So we get the equation: (0.75 nap/ear) * 1 ear = (0.75) nap

The Data Measurement at point 1 in ear is sent to point 0.75 in Nap, just like how the Data Measurement at point 1 in nose was sent to point 2 in Nap.

And since studies on cat people suggest that nose tip and ear length can independently build on top of each other to predict nap enjoyment, we can add them together:

<<<
How do we add these two features to get nap smile? By using the exact same vector addition that we used in our Input Space.

Now when we add together the vectors in our 2D coordinate plane, we are also adding together their analogous vectors found in our 1D Nap Space.

We move the tail of the blue vector to the head of the red vector, and after adding, we get a purple vector pointing to the same data point in both Input and Nap Space, which is where our cat person Tom is at. 

Our Nap Space says Tom has a nap enjoyment level of 2.75.

<<<
Let's say a nose tip that faces down has a value of -1 units. The lower the tip points, the lower the value. 

In our Input Space, it's easy to see how we can add together nose tips and ear lengths of different units. First, convert from Input Space to Nap Space using weight multiplication.

Next, let's take a nose tip of -1 units, and a ear length of 2 units, and add them together. 

When we scale the Input Space nose vector on the left by -1, we're also scaling the Nap Space nose vector on the right by the same amount. Now both vectors point in the negative direction. 

After we scale the ear vectors in both spaces, we add them together. In Nap Space, this is done by first following the nose vector left, and then following the ear vector right.

Finally, we get a Nap Smile of -0.5, which means that cat person doesn't like naps that much.

<<<
Notice that we're just adding together scaled versions of [1,0] and [0,1]. We call these vectors that are mapped to basic measuring units of 1 'basis vectors', because they're basic buildings blocks we can scale and add together to get any other vector in our input space. They're like an alphabet used to form words in a language.

The basic measuring units labeled by the basis vectors in our Input Space are mapped to the vectors of length 3 and 2 in Nap Space. But these are no longer the basis vectors in Nap Space. The basis vector in Nap Space is a Nap Smile of Unit 1.

So even though these two spaces are both calculating Tom, they are describing Tom using different basis vectors, where the Input Space talks about Tom in terms of Nose units and Ear units, while the Nap Space expresses Tom in terms of Nap units. 

He simultaneously exists in both sets of dimensions at once, but he looks different in each one because they're just looking at him from different perspectives, like seeing his shadow at different angles.

