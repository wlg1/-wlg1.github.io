I"ð<head>
    <link rel="stylesheet" href="index.css" />
</head>

<div class="topnav">
  <a class="active" href="#home">Home</a>
  <a href="#contact">Contact</a>
  <a href="#about">About</a>
</div>

<p align="center"><h1><b>Making the Math of Neural Networks Intuitive</b></h1></p>

<!---
For localhost testing:

<a href="ch1.1.html">CHAPTER 1.1</a>

<a href="ch1.2.html">CHAPTER 1.2</a>

<a href="ch2.0.html">CHAPTER 2.0</a>

<a href="ch2.1.html">CHAPTER 2.1</a>

---

--->

<p><span><b>W</b></span>hen studying new improvements to neural networks, many people run into the following problem:</p>

<p align="center">
<b>How do these unfamiliar mathematical concepts relate to neural networks?</b></p>

<!---
fig Eg) What's orthogonal projection?
--->

<p>This blog solves these problems by explaining all the required prerequisites needed to understand the terminology from the bottom up.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. It also gets to the point: it only says what issues this concept solves for this neural network paper, nothing more.</p>

<p>But this blog takes an even further step: by generalizing this solution to solve similar problems.</p>

<!---
fig Eg) [give an example of issue- reasoning- soln - generalization, that concisely explains all after prereqs]
--->

<p>Not sure where to start?</p>

<p><strong><a href="ch1.1.md">Start Here</a></strong></p>

<hr />
<p><strong>CHAPTER 1: How Neural Networks Reveal Hidden Insights in Data (using Matrix Multiplication)</strong></p>

<p><strong><a href="ch1.1.md">1.1: How does matrix multiplication guess a cat from its face and body?</a></strong></p>

<p><strong><a href="ch1.2.md">1.2: Why use dot product in matrix multiplication?</a></strong></p>

<p>1.3: WHY use basis vectors? The relativity of data</p>

<p>1.4: The Duality of Neurons: As Objects, As Relations</p>

<p>1.5: The Analogy of the Matrix</p>

<p><strong>CHAPTER 2: How to Find the Important Features and Change Them (using Matrix Decomposition)?</strong></p>

<p>2.0: Cool Applications (Face filters, etc.)</p>

<p><strong><a href="ch2.1.md">2.1: How Do Neural Networks Make Faces Look Younger (using Orthogonal Projection)?</a></strong></p>

<p><strong><a href="ch2.2.md">2.2: How Neural Networks Find the Most Important Features in Data (using Eigenvectors)</a></strong></p>

<p>Â Â Â Â <strong><a href="ch2.2.1.md">2.2.1: Paper Explanation: GANspace</a></strong></p>

<p>Â Â Â Â <strong><a href="ch2.2.2.md">2.2.2: Covariance Matrix</a></strong></p>

<p>Â Â Â Â <strong><a href="ch2.2.3.md">2.2.3: Paper Explanation: Eigenfaces</a></strong></p>

<p>Â Â Â Â <strong><a href="">2.2.4: Paper Explanation: SeFA</a></strong></p>

<p>Â Â Â Â <strong><a href="ch2.2.5.md">2.2.5: SVD</a></strong></p>

<p>[applying to latest generative models: DALLE, stable diffusion, grokking]</p>

<hr />

<p>Appendices:</p>

<p><strong><a href="generative_models_review.md">Review: Generative Models </a></strong></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>assuming only that a reader has a basic understanding of X, or has watched these videos []. Links are provided to them.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET