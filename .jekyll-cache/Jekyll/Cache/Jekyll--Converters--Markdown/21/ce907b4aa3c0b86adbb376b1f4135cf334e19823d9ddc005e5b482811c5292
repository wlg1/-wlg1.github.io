I"RD<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>Ch1.2: Why use dot product in matrix multiplication?</strong>
<!---WHY THE ALGEBRAIC PROCEDURE OF MATRIX MULTIPLICATION WORKS---></p>

<p>Let’s re-visit the problem of finding the value of \(O\). In other words, how do we find the new label of <img src="/cob/cat.PNG" width="50" height="40" /> in Model 2?</p>

<p><img src="/cob/2mod_out.PNG" alt="2mod_vecs" /></p>

<p>In a linear algebra class, one learns that this can be done using matrix multiplication. But why does the algebraic procedure of matrix multiplication work? Why does the first step specifically use the dot product of the first row and the input vector? What even is a dot product?</p>

<p>Let’s figure this out by first understanding what we need to know to calculate the values of \(O\) that represents data sample <img src="/cob/cat.PNG" width="50" height="40" />. All data samples in Model 2 are represented by two measurements:</p>
<p align="center">
1) How likely it is to be a <span style="color:orange">Cat</span>
</p>
<p align="center">
2) How likely it is to be a <span style="color:green">Rat</span>

$$O = \def\a{\color{orange}{Cat}}
\def\b{\color{green}{Rat}}
\begin{bmatrix} \a \\ \b \end{bmatrix}$$  
</p>
<p>So if we want to find the value of \(O\), we need to calculate these two values. Recall in the previous section that we were able to calculate the value of “how likely to be cat” using the values of “face length” and “body size”:</p>

<p>[Shorter face + Bigger body = Likely a Cat</p>

<p>\(\color{red}{face_{cat}} * 0.5  + \color{blue}{body_{cat}} * 2\) = 0.5(face_cat)+2(body_cat) on cat axes ]</p>

<p>Notice how this resembles the following dot product:</p>

<p align="center">
$$
\def\a{\color{red}{face_{cat}}}
\def\b{\color{blue}{body_{cat}}}
\begin{bmatrix} \a &amp; \b \end{bmatrix}  
\cdot  \begin{bmatrix} 0.5 \\ 2 \end{bmatrix} 
= \color{red}{face_{cat}} * 0.5  + \color{blue}{body_{cat}} * 2$$
</p>

<p>Not only that, but it resembles the first step of matrix multiplication:</p>

<p align="center">
$$
\def\a{\color{red}{face_{cat}}}
\def\b{\color{blue}{body_{cat}}}
\begin{bmatrix} \a &amp; \b \\ ? &amp; ? \end{bmatrix}  
\begin{bmatrix} 0.5 \\ 2 \end{bmatrix} 
= \begin{bmatrix} \a * 0.5  + \b * 2 \\ ?\end{bmatrix} $$
</p>

<p>In fact, the matrix on the left is none other than \(W\) from the figure above, which is used in the equation \(O = WX\).</p>

\[\def\a{\color{red}{1}}
\def\b{\color{blue}{1.5}}
\def\c{\color{red}{-1.5}}
\def\d{\color{blue}{1}}
\begin{bmatrix} \a &amp; \b \\ \c &amp; \d \end{bmatrix}  

=
\def\a{\color{red}{face_{cat}}}
\def\b{\color{blue}{body_{cat}}}
\begin{bmatrix} \a &amp; \b \\ ? &amp; ? \end{bmatrix}\]

<p>We mentioned that \(face_{cat}\) and \(body_{cat}\) are weights that denote how important each feature is in the calculation. For example, if body size is more important, we’d set \(body_{cat} = 1.5 &gt; face_{cat} = 1\).</p>

<p>But what does this dot product <strong>mean</strong> in terms of our dataset? To make its meaning easier to understand, let \(face_{cat}=4\), and let’s ignore the second term by setting \(body_{cat} = ?\)</p>

\[\color{red}{W}\color{#CBC3E3}{X} = \color{orange}{O}\]

<p align="center">
$$
\def\a{\color{red}{(face_{cat}=4)}}
\def\b{\color{#CBC3E3}{0.5}}
\def\c{\color{red}{4}}
\def\d{\color{orange}{2}}
\begin{bmatrix} \a &amp; ? \\ ? &amp; ? \end{bmatrix}  
\begin{bmatrix} \b \\ ? \end{bmatrix} 
= \begin{bmatrix} \c * \b = \d  \\ ?\end{bmatrix} $$
</p>

<p align="center">
<b><span style="color:#CBC3E3">0.5 units of face length</span></b> <span style="font-size:20px">&#8594;</span> <span style="color:orange">2 units of 'likely to be cat'</span>
</p>

<p>In other words, <span style="color:red">“for every face length of unit 1, there are 4 units of cat”</span>. Thus, for <span style="color:#CBC3E3"><b>half a unit</b></span> of face length, we have half of the proportionate amount of cat, which is <span style="color:orange">2</span>. Doesn’t this sound familiar, like unit conversion?</p>

<p>[picture of meter to feet conversion]</p>

<p>For every 1 meter, there are 3.28 feet. So if an entity is 2 meters long:</p>

<p align="center">
$$
\def\a{\color{red}{(meter_{feet}=3.28)}}
\def\b{\color{#CBC3E3}{2}}
\def\c{\color{red}{3.28}}
\def\d{\color{orange}{6.56}}
\begin{bmatrix} \a \end{bmatrix}  
\begin{bmatrix} \b \end{bmatrix} 
= \begin{bmatrix} \c * \b = \d \end{bmatrix} $$
</p>

<p align="center">
<b><span style="color:#CBC3E3">2 units of meter</span></b> <span style="font-size:20px">&#8594;</span> <span style="color:orange">6.56 units of feet</span>
</p>

<p>Indeed, one dot product step is analogous to 1D matrix multiplication; so two dot product steps would be analogous to 2D matrix multiplication. At last, we realize that matrix multiplication, or “Change of Basis”, is none other than  unit conversion multiplication, or “Change of Units”. That is, \(O\) refers to the same quantity that \(X\) refers to, except the two vectors measure it using different units.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>But in contrast to the 1D “meter to feet” example, the “face and body to cat” example uses <strong>two</strong> measurements to find the value of cat. Matrix multiplication allows for a change of <strong>multiple</strong> units, or multiple dimensions. The weight matrix \(W\) is analogous to the conversion factor.</p>

<p>In summary:</p>

<p>1) A <strong>Dot Product</strong> is used to convert multiple units into a new unit</p>

<p>2) Multiple dot products are used in <strong>Matrix Multiplication</strong> to find multiple new units</p>

<p>Just like how each row of \(X\) measures the same entity but using a different unit (a basis vector of Model 1), each row of \(O\) uses a different unit (a basis vector of Model 2) to measure the same entity, and each is calculated using the known measurements from \(X\).</p>

<p>Since each new measurement is calculated using the <strong>same input</strong>, but with <span><i>different conversion factors</i></span>, each new value of \(O\) is calculated using the <strong>same input vector</strong> \(X\), but with <span><i>different rows of</i></span> \(W\). <sup id="fnref:multdim_X" role="doc-noteref"><a href="#fn:multdim_X" class="footnote" rel="footnote">2</a></sup></p>

<hr />

<p>We now explained what the rows of \(W\) are. But what are its columns? And how does this tie into the geometric representation of matrix multiplication, as we see in the figure above? Let’s match terms in these equations to their geometric representation:</p>

<p>In Model 1, \(\def\a{\color{#FA8072}{1}}
\def\c{\color{#FA8072}{0}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\) labels <img src="/cob/face1.PNG" width="50" height="40" />. In Model 2, \(\def\a{\color{red}{1}}
\def\c{\color{red}{-1.5}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\) labels <img src="/cob/face1.PNG" width="50" height="40" />.</p>

<p>Thus, \(\def\a{\color{red}{1}}
\def\c{\color{red}{-1.5}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\) is where the data sample previously labeled by the basis vector \(\def\a{\color{#FA8072}{1}}
\def\c{\color{#FA8072}{0}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\) is sent.</p>

<p>Notice that \(\def\a{\color{red}{1}}
\def\c{\color{red}{-1.5}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\) is the first column of \(W\). And \(\color{red}{face_{cat}}\) = <span style="color:red">1</span> is the <span style="color:orange">Cat</span> coordinate of the vector \(\def\a{\color{red}{face_{cat}=1}}
\def\c{\color{red}{-1.5}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\) in Model 2. Given the fact that the vertical axis of Model 2 denotes “likely to be rat”, it’s pretty clear that the second row of this vector is \(face_{Rat}\), which is how much the face length is weighed by to calculate the value of the <span style="color:green">Rat</span> coordinate.</p>

<p>Therefore, \(\def\a{\color{red}{face_{cat}=1}}
\def\c{\color{red}{face_{Rat}=-1.5}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\), the first column of \(W\), and the Model 2 vector labeling <img src="/cob/face1.PNG" width="50" height="40" />, contains the conversion factors for each of the basis vectors \(\{ \color{orange}{cat}, \color{green}{Rat} \}\) of Model 2, in terms of only \(\color{red}{face}\).<sup id="fnref:no_longer_basis" role="doc-noteref"><a href="#fn:no_longer_basis" class="footnote" rel="footnote">3</a></sup></p>

<p>Likewise, \(\def\a{\color{blue}{body_{cat}=1.5}}
\def\c{\color{blue}{body_{Rat}=-1}}
\begin{bmatrix} \a \\ \c \end{bmatrix}\), the second column of \(W\), and the Model 2 vector labeling <img src="/cob/body1.PNG" width="50" height="40" />, contains the conversion factors for each of the basis vectors \(\{ \color{orange}{cat}, \color{green}{Rat} \}\) of Model 2, in terms of only \(\color{blue}{body}\).</p>

\[\def\a{\color{red}{face_{cat}}}
\def\b{\color{blue}{body_{cat}}}
\def\c{\color{red}{face_{Rat}}}
\def\d{\color{blue}{body_{Rat}}}
\color{purple}{W} = \begin{bmatrix} \a &amp; \b \\ \c &amp; \d \end{bmatrix}\]

<p>If we multiply this by the basis vector in Model 1 that labels <img src="/cob/face1.PNG" width="50" height="40" />, we see it uses ONLY the conversion units for \(face\), and none of the conversion units for \(body\). This is because this data sample has no body, so it would not use \(body\) in its calculation for \(cat\) and \(rat\) at all. The same goes for the other basis vector in Model 1.</p>

\[\def\a{\color{red}{face_{cat}}}
\def\b{\color{blue}{body_{cat}}}
\def\c{\color{red}{face_{Rat}}}
\def\d{\color{blue}{body_{Rat}}}
\begin{bmatrix} \a &amp; \b \\ \c &amp; \d \end{bmatrix} 
\begin{bmatrix} 1 \\ 0 \end{bmatrix} 
= \begin{bmatrix} 1 * \a + 0* \b  \\ 1 * \c + 0* \d \end{bmatrix} = \begin{bmatrix} \a \\ \c \end{bmatrix}\]

<p>So we arrived at several conclusions:</p>

<p>1) Because Model 2 uses the old measurements of Model 1 to calculate its new measurements, the matrix \(W\) contains the weights needed to determine how important each old measurement is for each new measurement</p>

<p>2) Each row of the matrix contains weights to calculate one <strong>new</strong> measurement in \(O\)</p>

<p>3) Each column of the matrix contains weights for how one <strong>OLD</strong> measurement in \(X\) is used</p>

<p>4) The dot product is applied between every row of the matrix and the input vector because the same input vector uses different weights of the old measurements in \(X\) for every new measurement in \(O\)</p>

<hr />

<p>[Discuss how prev right rotation is bad, and how new matrix is better]
-2, 2.5
2.5, 2</p>

<p>“A face length of unit 1 denotes that it’s -2 units ‘likely to be cat”</p>

<p>Or in other words, “A face length of unit 1 denotes that it’s 2 units NOT ‘likely to be cat”</p>

<p>If there are 2 meters, we multiply 2 meters.
[picture of 2 meters, or 2 * 3.28 feet]</p>

<p>If the face length is 0.5, that means it’s 2 units not likely to be a cat.
[top is face length, bottom is -+ line denoting cat chance]</p>

<hr />

<!---
Remember how the values of $$X$$ were calculated using the basis vectors. The first value, the face length of $$X$$, was calculated using:

<p align="center">
$$
\def\a{\color{#FA8072}{1}}
\def\b{\color{#ADD8E6}{0}}
\begin{bmatrix} \a & \b \end{bmatrix}  
\cdot  \begin{bmatrix} 0.5 \\ 2 \end{bmatrix} 
= \color{#FA8072}{1} * 0.5  + \color{#ADD8E6}{0} * 2 = \color{#CBC3E3}{0.5}$$
</p>

This is just like the dot product we saw to find the <span style="color:orange">Cat</span> value in Model 2.

<p align="center">
$$
\def\a{\color{red}{face_{cat}}}
\def\b{\color{blue}{body_{cat}}}
\begin{bmatrix} \a & \b \end{bmatrix}  
\cdot  \begin{bmatrix} 0.5 \\ 2 \end{bmatrix} 
= \color{red}{1} * 0.5  + \color{blue}{1.5} * 2$$
</p>

We were focused on calculating the values of $$O$$, but now we see that it's very similar to calculating the values of $$X$$! 
--->

<p>We have been thinking of \(W\) as a conversion matrix, such that its columns
are the basis vectors of Model 2. But how are the basis vectors in Model 1
represented in these equations? Let’s try to put them in columns to see what we get.</p>

\[\def\a{\color{red}{1}}
\def\b{\color{blue}{0}}
\def\c{\color{red}{0}}
\def\d{\color{blue}{1}}
\begin{bmatrix} \a &amp; \b \\ \c &amp; \d \end{bmatrix}\]

<p>The basis vectors in Model 1 form \(I\), the identity matrix, whose multiplication on any vector leaves that vector unchanged!</p>

\[X = IX\]

<p>We see that \(IX\) in Model 1 is analogous to \(WX\) in Model 2:</p>

\[IX = 
\def\a{\color{red}{1}}
\def\b{\color{blue}{0}}
\def\c{\color{red}{0}}
\def\d{\color{blue}{1}}
\begin{bmatrix} \a &amp; \b \\ \c &amp; \d \end{bmatrix} 
\begin{bmatrix} 0.5 \\ 2 \end{bmatrix} 
= X\]

<!---
= \begin{bmatrix} 1 * 0.5  + 0 * 2 \\ 0 * 0.5 + 1 * 2\end{bmatrix} $$
--->

\[WX = 
\def\a{\color{red}{-2}}
\def\b{\color{blue}{2.5}}
\def\c{\color{red}{2.5}}
\def\d{\color{blue}{-2}}
\begin{bmatrix} \a &amp; \b \\ \c &amp; \d \end{bmatrix} 
\begin{bmatrix} 0.5 \\ 2 \end{bmatrix} 
= O\]

<!---
= \begin{bmatrix} -2 * 0.5  + 2.5 * 2 \\ 2.5 * 0.5 + -2 * 2\end{bmatrix} $$
--->

<p>Note that the basis vectors of Model 1 in \(I\) are NOT the rows of I, but the columns. This can be easy to mix up because I is a symmetric matrix, so the \(i^{th}\) row equals the \(i^{th}\) column.</p>

<p>The first column in each of the matrices labels <img src="/cob/face1.PNG" width="50" height="40" />, and the second column labels <img src="/cob/body1.PNG" width="50" height="40" />. Each value in X is a quantity specifying the values of <img src="/cob/face1.PNG" width="50" height="40" /> and <img src="/cob/body1.PNG" width="50" height="40" />; each value can also be thought of as an instruction on how many units of that basis vector to use. Let’s go through the multiplications of \(IX\) and \(WX\) side by side to see how different they are when they use different basis vectors.</p>

<hr />

<p>First, we break down the steps of the first dot product, corresponding to the <span style="color:orange">Cat</span> coordinate, involving the first row of the matrices.</p>

<p>STEP 1: Get the first row of each matrix</p>

<p>\(I\): \(\def\a{\color{red}{1}}
\def\b{\color{blue}{0}}
\begin{bmatrix} \a &amp; \b \end{bmatrix}\)</p>

<p>\(W\): \(\def\a{\color{red}{-2}}
\def\b{\color{blue}{2.5}}
\begin{bmatrix} \a &amp; \b \end{bmatrix}\)</p>

<p><img src="/cob/1.2/step1.png" alt="step1" /></p>

<p>STEP 2: Scale by \(X\)</p>

<p>\(I\): \(\def\a{\color{red}{1}}
\def\b{\color{blue}{0}}
\begin{bmatrix} \a &amp; \b \end{bmatrix}
\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}
--&gt; \color{red}{1} * 0.5\qquad \color{blue}{0} * 2\)</p>

<p>\(W\): \(\def\a{\color{red}{-2}}
\def\b{\color{blue}{2.5}}
\begin{bmatrix} \a &amp; \b \end{bmatrix}
\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}
--&gt; \color{red}{-2} * 0.5\qquad \color{blue}{2.5} * 2\)</p>

<p><img src="/cob/1.2/step2.png" alt="step2" /></p>

<p>STEP 3: Add</p>

<p><img src="/cob/1.2/step3.png" alt="step3" /></p>

<!---[Explain side-by-side of dot product on Sys 1 on left, and on Sys 2 on right. Show same instructions from I are done on Sys 2, but require W since now the basis vectors that I targeted look different]--->

<p>Row 2 is the same; you can work it out yourself (but show animation / final result). Thus, the procedure to do 2D matrix multiplication is a sequence of 1D vector additions! (repeat the steps to calculate each member of the matrix)</p>

<p>This is why we use 2 dot products: each component is calculated separately…</p>

<p>We also see this is how dot product projects onto…</p>

<hr />
<p>[inverse: going down 2 rat, 1 cat gets you to body size 1]</p>

<hr />

<p>[^]: The dot product between two vectors scales the numbers in the first vector, then adds them all together. The second vector contains the weights used to scale the first vector’s elements. Since the dot product is commutative, it is interchangable which vector is the “first” or “second”.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The entity is neither 2 (meters) nor 6.56 (feet); those are just measurements labeling the data sample from two different perspectives. Remember, the dimensions are merely labels measuring the entity, but are not part of the entity itself. They are a way for an outside observer to describe the entity. So the vectors \(X\) and \(O\) are just different ways to measure the same data sample, but they are not the data sample itself. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:multdim_X" role="doc-endnote">
      <p>It is not hard to imagine that if we wanted to find the new values measuring MULTIPLE inputs, then instead of vector X, we use a matrix X, such that each column is a single input vector labeling a data sample. <a href="#fnref:multdim_X" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:no_longer_basis" role="doc-endnote">
      <p>Remember that <span style="color:red">[1, 1.5]</span> is NOT a basis vector, so <img src="/cob/face1.PNG" width="50" height="40" /> is no longer labeled by a basis vector. <a href="#fnref:no_longer_basis" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET