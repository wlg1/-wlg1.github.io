I"~<p><strong>WORKING TITLE</strong></p>

<p><strong><a href="ch1.1.md">ch1.1</a></strong></p>

<p><a href="ch1.1.html">CHAPTER 1.1</a></p>

<p><strong><a href="ch1.2.md">ch1.2</a></strong></p>

<p><a href="ch1.2.html">CHAPTER 1.2</a></p>

<!---
For localhost testing:

<a href="ch1.1.html">CHAPTER 1</a>
-->

<p>When studying new improvements to neural networks, many people run into the following problem:</p>

<p>1) Unfamiliar mathematical concepts and even more unfamiliar explanations</p>

<p>2) How do these mathematical concepts relate to neural networks?</p>

<p>Eg) Whatâ€™s eigenvector decomposition? How can I learn this as fast as possible?
Donâ€™t use example from actual paper</p>

<p>It explains all the required prerequisites needed to understand the terminology from the bottom up.</p>

<p>The website gets to the point: it cuts out all the extra information thatâ€™s not needed to understand how this term is used for this neural network paper. 
assuming only that a reader has a basic understanding of X, or has watched these videos []. Links are provided to them.</p>

<p>[give an example of issue- reasoning- soln - generalization, that concisely explains all after prereqs]</p>

<p>All explanations get to the point. They always start off by stating the issue that this concept will solve, and how that issue will improve a neural network. Then, they go further by saying how generalizing this solution can solve similar problems, thus showing the long term usefullness of the concept.</p>

<!---
When studying new improvements to neural networks, many people run into the following problem:

The terminology, such as the mathematical concepts, is unfamiliar. When one looks up the terms, the articles describing them use even more unfamiliar terms, and because they're not efficiently tailored towards the paper one is reading, there's so much extra stuff in them that it's very time consuming to learn. Concepts are introduced such that one doesn't know why they're important. What's their purpose? How is it connected to improving a neural network?

Eg) What's eigenvector decomposition? How can I learn this as fast as possible?
Don't use example from actual paper

[image caption: All of a sudden we're spending an hour talking about X. ]

This website solves this problem:
It explains all the required prerequisites needed to understand the terminology from the bottom up, so one does not have to go through a time consuming scavenger hunt to understand all of them; they're all neatly collected in one place. The website cuts out all the extra information that's not needed to understand how this term is used for this neural network paper. 
assuming only that a reader has a basic understanding of X, or has watched these videos []. Links are provided to them.

[give an example of issue- reasoning- soln - generalization, that concisely explains all after prereqs]

All explanations get to the point. They always start off by stating the issue that this concept will solve, and how that issue will improve a neural network. Then, they go further by saying how generalizing this solution can solve similar problems, thus showing the long term usefullness of the concept.
--->

<hr />
<p>CHAPTER 1: How Neural Networks Reveal Hidden Insights in Data (using Matrix Multiplication)</p>

<p>1.1: How does matrix multiplication guess a cat from its face and body?</p>

<p>1.2: Why use dot product in matrix multiplication?</p>

<p>1.3: WHY use basis vectors? The relativity of data</p>

<p>1.4: The Duality of Neurons: As Objects, As Relations</p>

<p>1.5: The Analogy of the Matrix</p>

<p>CHAPTER 2:  How Neural Networks Make Faces Look Younger (using Orthogonal Projection)</p>

<p>2.1: Dot product for direction, Lengths, Normal</p>

<p>2.2: Orthogonal Projections</p>

<p>CHAPTER 3: How Neural Networks Find the Most Important Features in Data (using Eigenvectors)
Eigenvectors, Diagonal, Decomposition, matrix factorization &amp; PCA</p>
:ET