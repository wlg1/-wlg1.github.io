I"2
<head>
    <link rel="stylesheet" href="index.css" />
</head>

<div class="topnav">
  <a class="active" href="#home">Home</a>
  <a href="#contact">Contact</a>
  <a href="#about">About</a>
</div>

<p align="center"><b>WORKING TITLE</b></p>

<!---
    -->
<p>For localhost testing:</p>

<p><a href="ch1.1.html">CHAPTER 1.1</a></p>

<p><a href="ch1.2.html">CHAPTER 1.2</a></p>

<p><a href="ch2.0.html">CHAPTER 2.0</a></p>

<p><a href="ch2.1.html">CHAPTER 2.1</a></p>

<hr />

<font size="+2">W</font>
<p>When studying new improvements to neural networks, many people run into the following problem:</p>

<p align="center">
<b>How do these unfamiliar mathematical concepts relate to neural networks?</b></p>

<p>fig Eg) What’s orthogonal projection?</p>

<p>This blog solves these problems by explaining all the required prerequisites needed to understand the terminology from the bottom up.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. It also gets to the point: it only says what issues this concept solves for this neural network paper, nothing more.</p>

<p>But this blog takes an even further step: by generalizing this solution to solve similar problems.</p>

<p>fig Eg) [give an example of issue- reasoning- soln - generalization, that concisely explains all after prereqs]</p>

<p>Not sure where to start?</p>

<p><strong><a href="ch1.1.md">Start Here</a></strong></p>

<hr />
<p>CHAPTER 1: How Neural Networks Reveal Hidden Insights in Data (using Matrix Multiplication)</p>

<p><strong><a href="ch1.1.md">1.1: How does matrix multiplication guess a cat from its face and body?</a></strong></p>

<p><strong><a href="ch1.2.md">1.2: Why use dot product in matrix multiplication?</a></strong></p>

<p>1.3: WHY use basis vectors? The relativity of data</p>

<p>1.4: The Duality of Neurons: As Objects, As Relations</p>

<p>1.5: The Analogy of the Matrix</p>

<p>CHAPTER 2:  How Neural Networks Make Faces Look Younger (using Orthogonal Projection)</p>

<p>2.1: Dot product for direction, Lengths, Normal</p>

<p>2.2: Orthogonal Projections</p>

<p>CHAPTER 3: How Neural Networks Find the Most Important Features in Data (using Eigenvectors)</p>

<p>Eigenvectors, Diagonal, Decomposition, matrix factorization &amp; PCA</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>assuming only that a reader has a basic understanding of X, or has watched these videos []. Links are provided to them. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET