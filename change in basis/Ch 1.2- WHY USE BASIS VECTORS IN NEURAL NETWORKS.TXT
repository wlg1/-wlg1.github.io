To gain stronger intuition about how linear algebra is used to find hidden relationships in data, we'll begin with a thought experiment. We will illustrate the difference between what we call "Reality", the concept space in Figure X, and a Model, a coordinate system. 

[pic 1, pic 2, etc] are all entities called Concepts within this Reality
We call a still picture of this Reality animation a Viewpoint. We observe that there is no one true viewpoint of Reality. All these viewpoints of Reality are valid ways to see it from a certain perspective, and they all are equivalent to each other. [take stills of the animation]
(Analogous because they preserve relations)

Note that there are no names for these Concepts in Reality. How can anyone refer to any of them, or describe them, or compare them to one another? So in this thought experiment, the goal of a neural network is to label the concepts within Reality using a coordinate system. Each point, or vector, in the coordinate system is a label. This system of numbers is also how the neural network can measure how far away each concept is from one another. For example, [pic 1] can be labeled with [-1 0], while [pic 2] can be labeled with [-2 0]. Then the neural network knows [pic 2] is 1 unit to the left of [pic 1].
[label this on figure]

But a neural network cannot place a coordinate system on top of this moving animation of Reality; it must choose one of the viewpoints. If it tried to place it on top of all the viewpoints, the label [-1 0] would mean [picture 1] but also [picture 2].
[try placing coordinate system on top of moving animation. label some numbers]
[place the same coord sys on two views that contradict each other]

To lock into a viewpoint, the neural network needs at least one reference point, or a "reference vector". That is, it must fix a label onto one concept. Like an anchor, this reference point locks the animation onto this seemingly arbitrary label. Using the reference point, the neural network can describe all other concepts based on 1) how far away they are from the reference point, and 2) whether the concept is behind, in front of, to the left of, and so forth, from the reference point. Figure X shows that the only difference between each viewpoint is the choice of reference points.

If only one reference point is used, the neural network stops Reality from moving up and down, and the same label can describe two different concepts. But Reality can still move left to right. However, if the neural network uses two reference points, it stops the animation from moving completely. 

Another word for a reference point is a basis vector. Thus, to label the concepts within Reality, the neural network must choose a set of basis vectors. The set of basis vectors creates a coordinate system, which we'll call a Model.

Let's consider two coordinate systems. (an example taken from 4:30-5m in video)

[Fig: Left is Sys 1, Right is sys 1 on Sys 2]
. first show c-d Sys 1 and c-d on top of Sys 2, with one vector b/c [-1 2]

fig: show that Identity * [-1 2] in Sys 1 is 'analogous' to 'W matrix' * [-1 2]
[-1 2] is the ONLY non-basis vector in both pics. [-1 2] never existed in Sys 2; it was always [-4 1]

Now each viewpoint has its own possible advantages and disadvantages relative to a goal. If we wanted to separate [poison pic] and [gift pic], we could use a basis vector to label them with numbers, then a second basis vector to hold them in place so they can be labeled with unique numbers. It appears that these two basis vectors are working together to classify our Reality, which is something that should be very familiar to those working with neural networks.
[figure of splitting]

But what if we wanted to classify circles vs not circles? The viewpoint above can't allow us to do that. Another viewpoint, using new basis vectors, can:
[figure of 2nd viewpoint]

Now what are the basis vectors in a neural network? As we shall soon see, they are the neurons themselves.

[explain this]

So to reveal hidden relationships in Reality, the neural network wants to consider multiple points of view. If it combines information from two viewpoints, such that there is information from one viewpoint that another viewpoint lacks, it gains greater insight into Reality than using one point of view. Since each viewpoint uses a different coordinate system, each view is a different Model of Reality. 

Now let's take a step back from this thought experiment, and connect it back to our world. A data set looks at a limited perception of our world, and labels it by describing it with a collection of features. These features can be individual pixels, the length of a petal, the number of windows in a house, and so forth. As many know from machine learning tutorials, these features act as axes in the coordinate system of an Input Space, where the vectors are data points. 
[show picture]

Using the terminology of the thought experiment, the Input Space's coordinate system is a Model of our world; the data set is not reality. In fact, it is the first Model that the neural network "perceives". 


<<<<<<<<
[EXPLAIN THIS LATER ONCE ESTABLISH NEURONS CREATING LATENT SPACE:]
Let's look at an example where the input space uses pixels as feature axes. At first the neural network just sees an image as a vector based on the value of its individual pixels. But this may not be enough for it to recognize objects that defined are as pixels arranged in a certain way. So using only this first Model, the neural network has to relate it to another Model that describes not just the pixels themselves, but the relationship between pixels, making sure to preserve the relationship between concepts captured in the first Model. 
a = WX + b [show picture such that each latent space is a Model]

