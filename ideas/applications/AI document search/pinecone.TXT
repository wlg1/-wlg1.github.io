https://docs.pinecone.io/docs/quickstart

-------------------------------
Q: What is p1 pod type in pinecone?

A type of deployment that is designed for low-latency and high-throughput serving of machine learning models.

production use cases where real-time predictions are required. 

Pinecone's p1 pods are designed to handle large volumes of incoming requests, and they utilize advanced load-balancing techniques to distribute requests evenly across multiple instances. They also provide built-in monitoring and logging features, which allow users to track the performance of their models and quickly identify any issues that may arise.
    Load-balancing is the process of distributing workloads across multiple computing resources to optimize resource utilization and improve performance.

<<<
The pod is a unit of computing resources in Kubernetes, which is commonly used to deploy and manage containerized applications.

-------------------------------
talks not tutorials:

https://www.youtube.com/watch?v=RL-ZnpE9hwM&t=20s&ab_channel=Pinecone
Build overpowered AI apps with the OP stack (OpenAI + Pinecone)

https://www.youtube.com/watch?v=nMniwlGyX-c&t=13s&ab_channel=Pinecone
Building the Future with LLMs, LangChain, & Pinecone

-------------------------------
https://docs.pinecone.io/docs/examples

https://docs.pinecone.io/docs/semantic-text-search

There are three components to every Pinecone vector embedding:
a vector ID
a sequence of floats of a user-defined, fixed dimension
vector metadata (a key-value store)

compare one vector to tens or hundreds of millions or more vectors, to do so with low latency (less than 50ms) and a high throughput. 

doesn't load pdf; loads articles stored in pandas df to pinecone

<<<<<<<<<<<<<<<<<<<<<<
TO DO TASKS:

- Embed pdfs and upload to pinecone pod
- Query and compare w/ experiments you do now
    Eg) Compare old expm w/ the new one. 
    Eg) Reproducible expm; Is it done correctly?
    Eg) Compare diff opinions from multiple pdfs