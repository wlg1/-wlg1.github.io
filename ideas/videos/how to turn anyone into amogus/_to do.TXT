V 1. create amogus images on SD
V 2. VHS filter in davinci
V 3. create amogus shape in manim
    V 3.1 enlarge amogus head (taller)
V 4. non-euclid latent space in manim
V 5. transform non-euclid surface into amogus
    noneuclid_to_sus.py
V 6. arrow animations b/w latent and semantic
    latent_to_semantic.py
V 7. semantic space as surface
    what h-space looks like? how linear? by expms
    how does latent map to it? [b/c unclear, leave as unasnwered question. just say Asyrp finds h-space, but ignore mapping]
V 8. neural network into latent space 
    NN_to_latent.py
V 9. check script
V 10. input text, output image; fades
V 11. separate out parts of black and white amogus then combine
V 12. write out 'non-euclidean'
    right before non-euclid to sphere
V 13. fade in tangent on sphere (zoom in)
    person svg icon on flat tangent (do it in davinci)
V 14. flat patches around sphere
    (these safe, cozy patches are called tangent)
V 15. the amogus direction (an arrow on a number line)
    MANUALLy place it onto sphere. this should be away from tangent arrow. make black transparent.
V 16. write out 'tangent', write out 'metric'
V 17. pullback vs f: transport person on a canoe.
V 18. static
19. sph_to_noneuc_to_sus.py
V 20. sus sound effect - modify in audacity
V 21. logo screen
V 22. img2img - more
    empty canvas filled with amogus, then you can prompt anything

    entire room of amogus (tv, fridge, rug, window, chair, microphone)

    use less strong vhs effects for math part (just use end of filters clip)
V 23. finish video edit
V 24. test vol. use vol 3 for nar

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
amogus- liminal, pepe inspiration. funny objs
    no need to be comprehensive; also, can make more vids

<<<
put dreambooth model into webui

<<<<
V 1. dreambooth transfer learning on SD to generate amogus mixed with someone by prompt

2a. img2img: turn someone into a giant amogus. forcefully mold their shape; gets rid of arms, detects legs
- stable diffusion AND model that doesn't use art images (may use transfer learning or not)

3. img2img (Inpainting OR auto-locate like yolo): turn PARTS of a person/image into amogus (eg. their eyes, a trashcan)
    control what counts as an amogus using training data, etc. turn only 'one' thing into amogus, or multiple.

4. gradient ascent, deepdream (it has been ignored)
    make it less nauseating

5. cut out, bounding box
    like yolo, outline the amogus, then focus only there

6. search for amogus in a database of images and then find bounding box or convert (at even the smallest places)

<<<<<<<<<<<<<<<<<<<<<<<<<<
- input amogus itself into img2img