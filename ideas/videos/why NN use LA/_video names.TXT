* Why do Neural Networks use Linear Algebra? - A Visual Intuition of Why Matrix Multiplication is like Unit Conversion
(too long)

100 char limit; 70 char truncated

Why do Neural Networks use Linear Algebra? | Pt 1: Cat Furry Mathematics
70 chars

Why do Neural Networks use Linear Algebra? | Pt 1: Furry Mathematics

It's okay to not put it into full title; put it in thumbnail
FURRIES + MATH ?!?!?
Cat making soyjak face with pointing hand to a neural network and linear algebra background of vectors
the circle and arrow red can be vectors

put furries in tags to make it searchable

<<<
Why do Neural Networks use Linear Algebra? | Pt 1: The Visual Intuition of Cat Mathematics
    ~ 15 mins

Why do Neural Networks use Linear Algebra? | Pt 2: Beware Of False Friends in The Matrix
    ~ 10 mins
    continues the NN explanation from before in brief recap
    Also discuss more esoteric concepts such as relative origin

Why do Neural Networks use Linear Algebra? | Pt 3: Matrix Multiplication is like Unit Conversion
    ~ 10-15 mins
    mention 'why dot product' and put it in tags and video file name
    Links to "make faces look younger" at end

    or mysteries of the hidden space (concept)
    The dimensions of AI imagination

In description and intro, put: - A Visual Intuition of Why Matrix Multiplication is like Unit Conversion

<<<
or just 3 videos:

1.1. furry math
1.2. beware false friends
1.3. to make you look younger

<<<
How Neural Networks Make Faces Look Younger | Pt 1: Using Vector Addition

How Neural Networks Make Faces Look Younger | Pt 2: Using Orthogonal Projection

How Neural Networks Make Faces Look Younger | Pt 3: Using Hyperplanes

<<<
Going over the blog itself:
ch1.1 - 1.2: 30 minutes
ch 1.3: 15 minutes
ch2: 20 minutes
